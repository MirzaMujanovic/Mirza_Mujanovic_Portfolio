[["index.html", "Marketing in R About me", " Marketing in R About me My name is Mirza Mujanovi, I am an ever-curious, eager to learn and ready to adapt student in his final semester of Marketing master program at the Vienna University of Economics and Business. Originally, I come from Tuzla, Bosnia &amp; Herzegovina, where I finished high school and first two semesters at the Faculty of Economics. In 2015, I moved to Vienna to study, and in 2019 completed a bachelors program in business administration at the Vienna University of Economics and Business (WU) in 2019. Thinking of my future plans I see myself working in the field of data analytics or data science for marketing as I am an R enthusiast passionate about marketing. Random facts about me: Big basketball fan. Nowadays, fan of sports analytics. Interested in psychology-related topics. Used to collect old money together with my older sister. Favorite food: Bulgogi and pudding (but not together, of course!). You can download my CV here "],["emotions-in-online-customer-reviews.html", "1 Emotions In Online Customer Reviews What to expect in this article? 1.1 Dictionaries for NLP 1.2 Data set 1.3 Corpus cleaning 1.4 Visualisations of terms frequency 1.5 Sentiment analysis", " 1 Emotions In Online Customer Reviews Figure 1.1: Foto von Andrea Piacquadio von Pexels Consumers usually seek quality and relevant information when buying new products. With the expansion and availability of the Internet, online consumer reviews have become a valuable resource to look at. Several studies tried to demystify relationship between product sales and online customer reviews. On the one hand, some of them, such as Senecal and Nantel (2004), suggest that participants who consulted product recommendations selected these products twice as often as those who did not consult recommendations. On the other hand, Zhang and Dellarocas (2006) find that online reviews and do not influence sales and serve solely as prediction. Between these two opinion fronts, one thing is certain: both sides aim to find out how consumers perceive and process word-of-mouth in a digital environment. In the academic paper The Role of Emotions for the Perceived Usefulness in Online Customer Reviews authors suggests that emotions impact the helpfulness ratings, i.e., the quality of online reviews as perceived by other customers. They found that, on average, the most prominent emotion dimensions that influence helpfulness ratings are trust, joy, and anticipation. Inspired by these findings, I decided to apply natural language processing techniques to analyze online customer reviews of a bestselling product on Amazon and try to detect those emotions using available lexicons. Final insights will show us whether trust, joy and anticipation can be identified in the reviews, thus improve helpfulness of reviews for potential customers. What to expect in this article? First, I will extract text via web-scrapping and form a corpus. Next, the text in the corpus will be pre-processed. Subsequently, from the pre-processed text will be stored in form of document-term-matrices or term-document matrices. Finally, an exploratory text analysis will be conducted and corresponding marketing implications pointed out. 1.1 Dictionaries for NLP For this exercise I will use 3 different lexicons available for R. One of them is AFINN, a lexicon of words rated for valence between minus five (indicating negative valence) and plus five (indicating positive valence). Next, I will use NRC Emotion Lexicon, which consists of English words and their labels for eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). # Dictionaries ---- afinn &lt;- get_sentiments(&quot;afinn&quot;) bing &lt;- get_sentiments(&quot;bing&quot;) loughran &lt;- get_sentiments(&quot;loughran&quot;) nrc &lt;- get_sentiment_dictionary(&#39;nrc&#39;, language = &quot;english&quot;) 1.2 Data set For our analysis, we will use text of 200 online customer reviews from Apple MacBook Pro (16-inch, 16GB RAM, 512GB Storage, 2.6GHz Intel Core i7) obtained in unpre-processed form: ## [1] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Great product. Replacement for my other macbook that lasted 7 years and was still going strong besides the battery. Great upgrade\\n\\n \\n&quot; ## [2] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Top notch until you don&#39;t use external monitor as well. Its common issue\\n\\n \\n&quot; ## [3] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n This is my first Mac and I love it. Apple is the BEST!\\n\\n \\n&quot; ## [4] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Basically, the laptop is a work of art. You know exactly what youre buying with an Apple product. I love that it comes with 16 GB of memory as default. The video card is very powerful. The touchpad is huge and works flawlessly. It is expensive, but worth every penny.\\n\\n \\n&quot; ## [5] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n A los dos dias de usarlo la pantalla se puso a rayas\\n\\n \\n \\n&quot; ## [6] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Product too heavy. Returned and have not received refund.\\n\\n \\n&quot; ## [7] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n I love this laptop - the screen resolution is beautiful, the light up touch bar is super nice and an innovative touch. I have recently converted form a PC user to MAC after all the years that I have enjoyed other apple products such as Ipad, and I doubt I&#39;d switch back.\\n\\n \\n&quot; ## [8] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n It is well made, Liked it had two positions when opened rather than just one. However, it would have been better if it had three reading positions when opened.\\n\\n \\n&quot; ## [9] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Great laptop! I have been using it for months now and havent had any issues. Highly recommended\\n\\n \\n&quot; ## [10] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n I opened the box and was like, Hello Lover It&#39;s gorgeous.\\n\\n \\n&quot; 1.3 Corpus cleaning From the results above we could see that text contains unnecessary characters. Therefore, I will use some usual procedure to clean up the reviews text and make it more understandable. For the purpose of this exercise and for efficiency reasons, we will use the volatile corpus, that stores the collection of documents in RAM memory. To create a volatile corpus, I need to pass reviews text in such a form that each review text is interpretated as a document. # Creation of volatile corpus review.corpus &lt;- VCorpus(VectorSource(review$review_text)) We see that the volatile corpus contains as many documents as many online reviews we collected. To undertake a custom transformation, I will use tm package and content_transformer() function. It takes a custom function as input, which defines what transformation needs to be done: review.toSpace&lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;/&quot;) # remove &quot;/&quot; review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;@&quot;) # remove &quot;@&quot; review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;\\\\|&quot;) # remove &quot;\\\\|&quot; review.corpus &lt;- tm_map(review.corpus, content_transformer(tolower)) # convert all capital letters to small review.corpus &lt;- tm_map(review.corpus, removeNumbers) # convert all capital letters to small review.corpus &lt;- tm_map(review.corpus, removeWords, stopwords(&quot;english&quot;)) # remove stop-words review.corpus &lt;- tm_map(review.corpus, removePunctuation) # remove punctuation review.corpus &lt;- tm_map(review.corpus, stripWhitespace) # strip extra whitespace from a document After cleaning the corpus, we can use document-term-matrix to store our cleaned corpus: review.dtm &lt;- DocumentTermMatrix(review.corpus) However, document-term-matrix is not the most suitable to work with, because it stores review texts in rows and terms frequencies in columns. We will transform it with tidy function: # Tidy up the document-term-matrix review.tidy &lt;- tidy(review.dtm) review.tidy$count &lt;-as.numeric(review.tidy$count) # Ensure correct class colnames(review.tidy)[2]&lt;- &#39;word&#39; # change name of the column from &quot;term&quot; to &quot;word&quot; review.tidy$document &lt;- as.numeric(review.tidy$document) # Ensure correct class Our tidy format has dimensions 6907 (the total number of terms) x 3 (document, term and count of the term in corresponding document): dim(review.tidy) # Dimensions ## [1] 6907 3 head(review.tidy)# Display first 6 rows ## # A tibble: 6 x 3 ## document word count ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 battery 1 ## 2 1 besides 1 ## 3 1 going 1 ## 4 1 great 2 ## 5 1 lasted 1 ## 6 1 macbook 1 1.4 Visualisations of terms frequency 1.4.1 Bar charts with the most frequent terms We would be interested in the most frequent words used in customer reviews. Sometimes just a glimpse of the most frequent words is sufficient to get some insights. Here we see that word love and great appears among most frequent terms. # Most frequent terms ---- review.tdm &lt;- TermDocumentMatrix(review.corpus) review.m &lt;- as.data.frame.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) word.names&lt;-names(review.v) df.review.v&lt;-data.frame(review.v,word.names) colnames(df.review.v)&lt;-c(&quot;n&quot;,&quot;word&quot;) p&lt;-ggplot(data=df.review.v[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;steelblue&quot;) + coord_flip() + ggtitle(&quot;20 most frequent words in customer reviews - MacBook Pro&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_test() ggplotly(p) 1.4.2 Wordcloud with the most frequent terms Similarly to the bar chart with the most frequent words, we could use wordcloud as well. It displays words from the corpus and signalizes their frequency by displaying more frequent words bigger relative to those that appear less frequently in the corpus. In the wordcloud below you can see 200 most frequent words, where the minimum frequency was set to 1. # Wordcloud review.tdm &lt;- TermDocumentMatrix(review.corpus) review.m &lt;- as.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) review.d &lt;- data.frame(word = names(review.v),freq=review.v) set.seed(1234) wordcloud(words = review.d$word, freq = review.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) 1.4.3 The most frequent terms indicating emotions When it comes to anticipation, words such as good,time,happy or powerful indicates that this emotion can be identified among customer reviews. On the other hand, there are some words that could be a signal both for good and bad experience: finally,money or wait. # Anticipation words---- nrc.anticipation &lt;- subset(nrc, nrc$sentiment==&quot;anticipation&quot;) review.anticipation.words &lt;- inner_join(review.tidy, nrc.anticipation) review.anticipation.words &lt;- count(review.anticipation.words, word) review.anticipation.words &lt;- review.anticipation.words[order(review.anticipation.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.anticipation.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;orange&quot;) + coord_flip() + ggtitle(&quot;20 most frequent anticipation words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) Similarly to anticipation, now we observe a list of top 20 words that indicate trust. It reveals new quite frequent term in the corpus: recommend. # Trust words---- nrc.trust &lt;- subset(nrc, nrc$sentiment==&quot;trust&quot;) review.trust.words &lt;- inner_join(review.tidy, nrc.trust) review.trust.words &lt;- count(review.trust.words, word) review.trust.words &lt;- review.trust.words[order(review.trust.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.trust.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;royalblue1&quot;) + coord_flip() + ggtitle(&quot;20 most frequent trust words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) Although at the bottom of the list, The top 20 list of joy words displays some additional words that we did not observe previously such as beautiful,gorgeous,wonderful,improvement,excellent. # Joy words ---- nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) review.joy.words &lt;- inner_join(review.tidy, nrc.joy) review.joy.words &lt;- count(review.joy.words, word) review.joy.words &lt;- review.joy.words[order(review.joy.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.joy.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;darkorange1&quot;) + coord_flip() + ggtitle(&quot;20 most frequent trust words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) 1.5 Sentiment analysis 1.5.1 Polarity timeline One usual way to compare and quantify emotions in text is via polarity. We simply count number of unique words in each document (=review) labelled as negative and deduct from the count of unique positive words. For instance, the first review contains 2 unique positive words (great and strong) and none negative unique words. Therefore, its polarity score is 2. This polarity timeline suggests very important implication: the reviews sentiment is moving above the 0, bearly going even below +2, giving an indication that this product continuously meet customers expectations. That is a good signal to believe that customers are rather satisfied with the product. # Polarity timeline ---- review.sentiment &lt;- inner_join(review.tidy, bing) review.sentiment &lt;- count(review.sentiment, sentiment, index=document) review.sentiment &lt;- spread(review.sentiment, sentiment, n, fill=0) review.sentiment$polarity &lt;- review.sentiment$positive - review.sentiment$negative review.sentiment$pos &lt;- ifelse(review.sentiment$polarity &gt;=0, &quot;Positive&quot;, &quot;Negative&quot;) p&lt;-ggplot(review.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1)+theme_gdocs()+ggtitle(label=&quot;Polarity timeline&quot;) ggplotly(p) # Smooth curve review.smooth &lt;- ggplot(review.sentiment, aes(index, polarity)) p&lt;-review.smooth + stat_smooth() + theme_gdocs() + ggtitle(&quot;Polarity timeline - smooth&quot;) ggplotly(p) In the polarity graph at index 81 we identify a review with sentiment score of even 34! This seems to be a thrilled customer every brand loves! Let us take closer look: review.sentiment &lt;- inner_join(review.tidy, bing) doc_81&lt;-filter(review.sentiment, document==&quot;81&quot;) head(doc_81[order(doc_81$count,decreasing = T),]) ## # A tibble: 6 x 4 ## document word count sentiment ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 81 best 3 positive ## 2 81 better 3 positive ## 3 81 improved 3 positive ## 4 81 amazing 2 positive ## 5 81 breeze 2 positive ## 6 81 good 2 positive Finally, it certainly pays off to check the actual review: # Outlier in polarity score review$review_text[81] ## [1] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n If you&#39;ve been waiting for Apple to wake up and address the concerns raised by the Apple community -- your wait is now over. This is the MacBook Pro we&#39;ve all been wanting for years. This review is for the higher end model, stock.The good.- Keyboard. It&#39;s fantastic. The key travel has been extended to 1mm, which is about half of the original keyboard found on the 2015 and prior model years. It feels just as good to type on because apple improved the tactile feedback. The keys sort of spring back. The keyboard is quiet and very comfortable to type on. The keys are slightly smaller but do not make typing on them any more difficult. The directional arrows are now properly setup and the physical Esc key is back, making it a breeze to flip through open applications.- Screen. The difference in size is subtle but noticeable. It&#39;s technically even more dense, but just barely. You won&#39;t notice much difference from recent MacBook Pros but true to Apple, the display is absolutely gorgeous. The 0.6 inch difference in size retains the same logical resolution, so everything should look just slightly bigger which I welcome.- Processor. The base model has the same chipset as the 2019 15.4\\&quot; model. The performance, however, is about 10% better due to significantly improved airflow and larger heatsinks. The issue of heat related throttling has been largely addressed. On the upper model, the machine now comes with 8 cores and serious performance bump. There is literally nothing you can throw at this MacBook that it won&#39;t handle with breeze.- Graphics. The traditional setup is still here. You have the onboard Intel chipset, which operates when the demand for visual performance is low. You also have a discrete AMD chip which delivers significant improvement over the previous generation. In fact, the base model delivers performance in excess of the upper spec Vega chipset from last year. The leap is extraordinary. As before, the system will automatically choose which graphics card to use depending on demand.- Sound. In one world, amazing. Imagine Apple took a HomePod and flattened it to fit it in the housing of the MacBook Pro. That is essentially the experience. Bass is pronounced and crisp and treble is sharp. The sound is rich and room-filling. There are six speakers instead of two in the last generation.-Microphone. Major improvements with three mics instead of one in the previous generation. I use the MacBook to make calls using an iPhone and the sound on the other end is clear and free from background noice. I&#39;ve been told it sounds a lot better than before, but that is of course subjective. On paper, you&#39;re getting better noise reduction and improved sound fidelity.- Touch Bar. Controversial in the past, I think it may now be the \\&quot;happy medium\\&quot; between physical keys and the useful Touch Bar which adapts to the content on the screen. The Esc key is back and on the right hand side you&#39;ll find Touch ID and power button.- SSD. You&#39;ll love the fact that now base starting size of the SSD has been doubled on both the entry model and the upper model with 512GB and 1TB respectively.- Gaming. This has to be mentioned. The graphics card offers incredible leap in performance. Modern games that would get 14-16FPS on high setting now perform at 35-40FPS with ease. Same settings. Same games. Huge improvement. It&#39;s now possible to play AAA games on the MacBook Pro with reasonable performance and high visual settings.- Productivity. This machine is a beast. I use the full Affinity suite and do some limited video editing. In addition, I have multiple productivity programs open, over a dozen Safari tabs, two email clients, and dozens of other apps, such as CRM, task managers, notes, etc. Everything runs smoothly.- Value. Yes, value. This expensive laptop brings the best value in the lineup of the 15.\\&quot;4 (now 16\\&quot;) offering to date. If you carefully look at the costs of truly compatible Windows offerings, you&#39;ll find the MacBook Pro to be competitively priced.The Bad- Weight. The machine is slightly heavier but I welcome the added bulky. Finally Apple went with functionality over its obsession with thiner and lighter hardware, giving us a machine with proper thermal management, proper keyboard and more. But if you plan on taking it with you places, you&#39;ll feel the extra bulk.- BTO Updates. The cost of BTO options is still quite high, with the noted exception of the 8GB option on the GPU. You&#39;re still paying a significant premium for each incremental upgrade over the base model.To sum up -- This is the best MacBook Pro in many years. It&#39;s a well-rounded, powerful machine that brings about incredible performance and value. I would highly recommend it to any Pro.If you&#39;re upgrading from the 13\\&quot; model and wonder how much more real estate you can expect, see attached side-by-side comparison photo.Update 3/15/20 -- Absolutely a beast of a machine. I love it. It has been pretty much perfect and its performance continues to surprise me. I have the top-speced model and it smokes pretty much everything I&#39;ve used to date. In fact, it will perform on par with the new Mac Pro base configuration. Don&#39;t believe me? Google it. This is by far the best MacBook Pro to date and an amazing value. Well worth the price of admission if you need it.\\n\\n \\n&quot; It seems that our assumption was correct! The customer was definitely thrilled! This is a nice example how you can identify and take closer look at reviews that stand out based on its polarity score. 1.5.2 Analysis on sentence-level Text analysis provides freedom to choose level of observation. So far, we explored words and their frequencies, we explored customer reviews and quantified their sentiment in two dimensions (positive and negative). Next, we will approach the task of identifying the most negative and positive reviews by organizing text by sentences. By doing so, we will directly access those sentences whose average sentiment stand out. # Calculating the average sentiment review.highlighted&lt;-review$review_text%&gt;% get_sentences() %&gt;% sentiment_by() head(review.highlighted) ## element_id word_count sd ave_sentiment ## 1: 1 2 NA 0.35355339 ## 2: 2 16 NA 0.12500000 ## 3: 3 2 NA 0.35355339 ## 4: 4 10 NA 0.06324555 ## 5: 5 3 NA -0.57735027 ## 6: 6 9 NA 0.25000000 # Preparing data review.score &lt;- subset(review.highlighted, select = c(&quot;ave_sentiment&quot;,&quot;element_id&quot;)) review.worst &lt;- review.score[order(review.score$ave_sentiment,decreasing = FALSE),] review.worst&lt;-review.worst$element_id[1:10] review.best &lt;- review.score[order(review.score$ave_sentiment, decreasing = TRUE),] review.best &lt;- review.best$element_id[1:10] sentences&lt;-review$review_text %&gt;% get_sentences() sentences&lt;-as.matrix(sentences) And here we have the worst 10 sentences from customer reviews; # 10 worst sentences sentences[review.worst] ## [1] &quot;The screen is nice but has ridiculous ghosting.&quot; ## [2] &quot;Very disappointed.&quot; ## [3] &quot;Force cancelling woofers keep the annoying vibration at louder playback at bay.&quot; ## [4] &quot;I am very disappointed!&quot; ## [5] &quot;Having ports would have been nice (especially seeing how much space would be available for ports) but unfortunately, that is how Apple operates.&quot; ## [6] &quot;This is a Finder issue, as apps launch but don&#39;t leap to the top.&quot; ## [7] &quot;Im glad I didnt have important files to transfer or I would have been upset.&quot; ## [8] &quot;I guess I got a lemon, which is disappointing considering the high cost.&quot; ## [9] &quot;Small issue: fan comes up too often ( I do application development with it).&quot; ## [10] &quot;Very suspicious batch.&quot; Despite the fact that positive sentiment prevails, we see that there are certain problems associated with MacBook laptop. Issues with screen, problems with woofers, disappointment that there are no ports, unsatisfying value-price ratio. # 10 most positive sentences sentences[review.best] ## [1] &quot;Yes its considerably big and heavy compared to others, but Honestly that just feels like better quality to me&quot; ## [2] &quot;Quality is very good.&quot; ## [3] &quot;Extremely pleased with my NEW MacBook Pro!&quot; ## [4] &quot;This MacBook Pro 16\\&quot; is outstanding and much much better than the previous generation.&quot; ## [5] &quot;Overall, this new MacBook is rather pricey, but there&#39;s a reason it is called \\&quot;Pro\\&quot; because it is designed for professional.&quot; ## [6] &quot;Very fast!&quot; ## [7] &quot;Very fast, great screen, fantastic keyboard and wonderful speakers.&quot; ## [8] &quot;If you&#39;re upgrading from the 13\\&quot; model and wonder how much more real estate you can expect, see attached side-by-side comparison photo.&quot; ## [9] &quot;Well made computerBest specsNo much innovation from the 15 MacBook Pro that I Have.&quot; ## [10] &quot;I absolutely love this new Mac!&quot; If we take a look at 10 most positive sentences from customer reviews, we would find a similar evidence as we obtained with polarity score. However, by reading those sentences a reader can have better feeling what the reviewer is actually satisfied or unsatisfied with. Here we see that some people admire the speed for instance. 1.5.3 What are the most emotional reviews? Package sentimentr provides nice function emotion() which uses a dictionary to find emotion words and then compute the rate per sentence. The final emotion score ranges between 0 (no emotion used) and 1 (all words used were emotional). # Extract emotions terms reviews.emotion &lt;- review$review_text %&gt;% get_sentences() %&gt;% emotion() # Top 50 sentences with the highest emotion score top_emotional_sentences &lt;- unique(reviews.emotion[order(reviews.emotion$emotion,decreasing = TRUE),]$element_id[1:50]) # The most emotional reviews sentences[top_emotional_sentences,] ## [1] &quot;Frustrated!&quot; &quot;PERFECT!!!!&quot; &quot;Finally!&quot; ## [4] &quot;Hot damn.&quot; &quot;Hot.&quot; &quot;Damn.&quot; ## [7] &quot;LOVE LOVE LOVE it!&quot; &quot;The Bad- Weight.&quot; &quot;Highly recommend.&quot; ## [10] &quot;Highly recommend.&quot; &quot;Outstanding laptop.&quot; &quot;Excellent computer&quot; ## [13] &quot;Loving it!&quot; &quot;Good computer.&quot; &quot;good product&quot; ## [16] &quot;Huge improvement.&quot; &quot;So disappointed.....&quot; &quot;Very disappointed.&quot; ## [19] &quot;so disappointed.&quot; &quot;Good keyboard.&quot; &quot;Ridiculously good.&quot; We can see that identified sentences very clearly reflect emotions that customers expressed. It seems that intensity of emotions is high in both positive and negative direction. Finally, we can plot detected emotions in order to get a bit more clear insight in emotional structure detected in the reviews: # Plot of emotion p&lt;-plot(reviews.emotion, transformation.function = syuzhet::get_dct_transform, drop.unused.emotions = TRUE, facet = TRUE) ggplotly(p) This plot depicts emotional inclination of the reviews. Curves indicating emotional propensity of trust, joy and anticipation suggest strong inclination towards mentioned emotions. In line with the academic paper The Role of Emotions for the Perceived Usefulness in Online Customer Reviews, we have found an evidence that online reviews analyzed encode emotions contributing to the higher helpfulness rating, i.e. quality of reviews. In the end, it is important to mention that the role and influence of emotions defined by Plutchik on the quality of reviews differ across different product categories. "],["text-analysis-of-novels.html", "2 Text Analysis of Novels 2.1 Data 2.2 Text cleaning 2.3 Text preprocessing 2.4 Word analysis 2.5 Commonality cloud and Comparison cloud 2.6 Sentiment timeline 2.7 Topic Modelling: LDA 2.8 Main points of interpretation", " 2 Text Analysis of Novels Figure 2.1: Novels used in analysis Although I like reading books, unfortunately I dont find much time to read. However, I found that NLP is giving an opportunity to learn more about novels without reading it. My idea here is to try to learn as much as possible about two novels, The Aspern Papers and Awakening. 2.1 Data Downloading Novels The Awakening and The Aspern Papers awakening &lt;- scan(&quot;http://www.gutenberg.org/files/160/160-0.txt&quot;, what=&quot;character&quot;, blank.lines.skip = TRUE, sep=&quot;\\n&quot;) aspern &lt;- scan(&quot;http://www.gutenberg.org/files/211/211-0.txt&quot;, what=&quot;character&quot;, blank.lines.skip = TRUE, sep=&quot;\\n&quot;) awakening&lt;- iconv(awakening, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) aspern&lt;- iconv(aspern, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) 2.2 Text cleaning Cleaning headers of the two novels. # Awakening: cleaning headers awakening.begin &lt;- which(awakening==&quot;THE AWAKENING&quot;)[2] awakening.end &lt;- which(awakening==&quot;*****&quot;) - 1 awakening.v&lt;- awakening[awakening.begin:awakening.end] # Aspern: cleaning headers aspern.begin &lt;- which(aspern==&quot;Macmillan and Co., 1888.&quot;)+1 aspern.end &lt;- which(aspern==&quot;End of the Project Gutenberg EBook of The Aspern Papers, by Henry James&quot;) - 1 aspern.v &lt;- aspern[aspern.begin:aspern.end] head(aspern.v) ## [1] &quot;I&quot; ## [2] &quot;I had taken Mrs. Prest into my confidence; in truth without her I&quot; ## [3] &quot;should have made but little advance, for the fruitful idea in the whole&quot; ## [4] &quot;business dropped from her friendly lips. It was she who invented the&quot; ## [5] &quot;short cut, who severed the Gordian knot. It is not supposed to be the&quot; ## [6] &quot;nature of women to rise as a general thing to the largest and most&quot; 2.3 Text preprocessing Cutting in chapters and corpus creation: # Cutting the Awakening in chapters awakening.v &lt;- gsub(&quot;^I*(X|V)*I*$&quot;, &quot;@@@&quot;, awakening.v) awakening.string &lt;- paste(awakening.v, collapse = &quot; &quot;) awakening.chapters &lt;- strsplit(awakening.string, &quot;@@@ &quot;) # Cutting the Aspern in chapters aspern.v &lt;- gsub(&quot;^I*(X|V)*I*$&quot;, &quot;@@@&quot;, aspern.v) aspern.string &lt;- paste(aspern.v, collapse = &quot; &quot;) aspern.chapters &lt;- strsplit(aspern.string, &quot;@@@ &quot;) # Awakening corpus creation awakening.df &lt;- as.data.frame(awakening.chapters, stringsAsFactors = FALSE) awakening.df &lt;-awakening.df[2:38,1] awakening.df &lt;- as.data.frame(awakening.df) colnames(awakening.df) &lt;- &quot;chapters&quot; awakening.docs &lt;- Corpus(VectorSource(awakening.df$chapters)) # Aspern corpus creation aspern.df &lt;- as.data.frame(aspern.chapters, stringsAsFactors = FALSE) aspern.df &lt;-aspern.df[2:38,1] aspern.df &lt;- as.data.frame(aspern.df) colnames(aspern.df) &lt;- &quot;chapters&quot; aspern.docs &lt;- Corpus(VectorSource(aspern.df$chapters)) Text pre-processing: removing unnecessary simbols and signs, converting letters to lower case, removing numbers, punctuation and white space. # Awakening: Text pre-processing toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;/&quot;) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;@&quot;) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;\\\\|&quot;) awakening.docs &lt;- tm_map(awakening.docs, content_transformer(tolower)) awakening.docs &lt;- tm_map(awakening.docs, removeNumbers) awakening.docs &lt;- tm_map(awakening.docs, removeWords, stopwords(&quot;english&quot;)) awakening.docs &lt;- tm_map(awakening.docs, removePunctuation) awakening.docs &lt;- tm_map(awakening.docs, stripWhitespace) # Aspern: Text pre-processing toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;/&quot;) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;@&quot;) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;\\\\|&quot;) aspern.docs &lt;- tm_map(aspern.docs, content_transformer(tolower)) aspern.docs &lt;- tm_map(aspern.docs, removeNumbers) aspern.docs &lt;- tm_map(aspern.docs, removeWords, stopwords(&quot;english&quot;)) aspern.docs &lt;- tm_map(aspern.docs, removePunctuation) aspern.docs &lt;- tm_map(aspern.docs, stripWhitespace) 2.4 Word analysis Creation of a WordCloud: # Awakening: Wordcloud awakening.dtm &lt;- DocumentTermMatrix(awakening.docs, control=list(weighting=weightTf)) awakening.m &lt;- as.matrix(t(awakening.dtm)) awakening_v &lt;- sort(rowSums(awakening.m),decreasing=TRUE) awakening.d &lt;- data.frame(word = names(awakening_v),freq=awakening_v) set.seed(1234) wordcloud(words = awakening.d$word, freq = awakening.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) # Aspern: Wordcloud aspern.dtm &lt;- DocumentTermMatrix(aspern.docs, control=list(weighting=weightTf)) aspern.m &lt;- as.matrix(t(aspern.dtm)) aspern_v &lt;- sort(rowSums(aspern.m),decreasing=TRUE) aspern.d &lt;- data.frame(word = names(aspern_v),freq=aspern_v) set.seed(1234) par(mar = rep(0, 4)) wordcloud(words = aspern.d$word, freq = aspern.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) 2.5 Commonality cloud and Comparison cloud Creation of a corpus out of the both texts: cc.docs &lt;- Corpus(VectorSource(c(awakening.string,aspern.string))) Text pre-processing of the new corpus: toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;/&quot;) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;@&quot;) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;\\\\|&quot;) cc.docs &lt;- tm_map(cc.docs, content_transformer(tolower)) cc.docs &lt;- tm_map(cc.docs, removeNumbers) cc.docs &lt;- tm_map(cc.docs, removeWords, stopwords(&quot;english&quot;)) cc.docs &lt;- tm_map(cc.docs, removePunctuation) cc.docs &lt;- tm_map(cc.docs, stripWhitespace) Comparison and Commonality cloud: cc.dtm &lt;- DocumentTermMatrix(cc.docs) cc.m &lt;- as.matrix(t(cc.dtm)) colnames(cc.m)&lt;- c(&quot;Awakening&quot;,&quot;Aspern&quot;) #Comparison cloud par(mar = rep(0, 4)) comparison.cloud(cc.m,max.words = 100,min.frrandom.order=FALSE) # Commonality cloud par(mar = rep(0, 4)) commonality.cloud(cc.m,max.words = 100,colors = &quot;steelblue1&quot;,min.frrandom.order=FALSE) 2.6 Sentiment timeline Loading in relevant lexicons: library(tidytext) library(dplyr) library(tidyr) library(plotly) library(ggthemes) library(sentimentr) library(syuzhet) data(&quot;sentiments&quot;) afinn &lt;- get_sentiments(&quot;afinn&quot;) bing &lt;- get_sentiments(&quot;bing&quot;) loughran &lt;- get_sentiments(&quot;loughran&quot;) nrc &lt;- get_sentiment_dictionary(&#39;nrc&#39;, language = &quot;english&quot;) Polarity timeline of Awakening: sent.awakening &lt;- readLines(&quot;http://www.gutenberg.org/files/160/160-0.txt&quot;) sent.awakening &lt;-iconv(sent.awakening, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) awakening.corpus &lt;- VCorpus(VectorSource(sent.awakening)) awakening.corpus &lt;- tm_map(awakening.corpus, content_transformer(tolower)) awakening.corpus &lt;- tm_map(awakening.corpus, removeNumbers) awakening.corpus &lt;- tm_map(awakening.corpus, removeWords, stopwords(&quot;english&quot;)) awakening.corpus &lt;- tm_map(awakening.corpus, removePunctuation) awakening.corpus &lt;- tm_map(awakening.corpus, stripWhitespace) awakening.dtm &lt;- DocumentTermMatrix(awakening.corpus) awakening.tidy &lt;- tidy(awakening.dtm) awakening.tidy$count &lt;-as.numeric(awakening.tidy$count) colnames(awakening.tidy)[2]&lt;- &#39;word&#39; awakening.tidy$document &lt;- as.numeric(awakening.tidy$document) nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) joy.words &lt;- inner_join(awakening.tidy, nrc.joy) joy.words &lt;- count(joy.words, word) bing &lt;- get_sentiments(&quot;bing&quot;) awakening.sentiment &lt;- inner_join(awakening.tidy, bing) awakening.sentiment &lt;- count(awakening.sentiment, sentiment, index=document) awakening.sentiment &lt;- spread(awakening.sentiment, sentiment, n, fill=0) awakening.sentiment$polarity &lt;- awakening.sentiment$positive - awakening.sentiment$negative awakening.sentiment$pos &lt;- ifelse(awakening.sentiment$polarity &gt;=0, &quot;pos&quot;, &quot;neg&quot;) ggplot(awakening.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1) awakening.smooth &lt;- ggplot(awakening.sentiment, aes(index, polarity)) (p2&lt;-awakening.smooth + stat_smooth()) Polarity timeline of Aspern: sent.aspern &lt;- readLines(&quot;http://www.gutenberg.org/files/211/211-0.txt&quot;) aspern.corpus &lt;- VCorpus(VectorSource(sent.aspern)) aspern.corpus &lt;- tm_map(aspern.corpus, content_transformer(tolower)) aspern.corpus &lt;- tm_map(aspern.corpus, removeNumbers) aspern.corpus &lt;- tm_map(aspern.corpus, removeWords, stopwords(&quot;english&quot;)) aspern.corpus &lt;- tm_map(aspern.corpus, removePunctuation) aspern.corpus &lt;- tm_map(aspern.corpus, stripWhitespace) sent.aspern &lt;-iconv(sent.aspern, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) aspern.dtm &lt;- DocumentTermMatrix(aspern.corpus) aspern.tidy &lt;- tidy(aspern.dtm) aspern.tidy$count &lt;-as.numeric(aspern.tidy$count) colnames(aspern.tidy)[2]&lt;- &#39;word&#39; aspern.tidy$document &lt;- as.numeric(aspern.tidy$document) nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) joy.words &lt;- inner_join(aspern.tidy, nrc.joy) joy.words &lt;- count(joy.words, word) #bing &lt;- subset(sentiments, sentiments$lexicon==&#39;bing&#39;)[,-4] aspern.sentiment &lt;- inner_join(aspern.tidy, bing) aspern.sentiment &lt;- count(aspern.sentiment, sentiment, index=document) aspern.sentiment &lt;- spread(aspern.sentiment, sentiment, n, fill=0) aspern.sentiment$polarity &lt;- aspern.sentiment$positive - aspern.sentiment$negative aspern.sentiment$pos &lt;- ifelse(aspern.sentiment$polarity &gt;=0, &quot;pos&quot;, &quot;neg&quot;) ggplot(aspern.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1) aspern.smooth &lt;- ggplot(aspern.sentiment, aes(index, polarity)) (p3&lt;-aspern.smooth + stat_smooth()) g2 &lt;- ggplotGrob(p2) g3 &lt;- ggplotGrob(p3) g &lt;- rbind(g2, g3, size = &quot;first&quot;) g$widths &lt;- unit.pmax(g2$widths, g3$widths) grid.newpage() grid.draw(g) 2.7 Topic Modelling: LDA Pick up 5 topics and try to make sense of the topics giving a label to them. # Awakening top.mod.awakening.dtm &lt;- DocumentTermMatrix(awakening.corpus, control = list(weighting=weightTf)) burnin = 1000 iter = 1000 keep = 50 set.seed(510) top.mod.awakening.n &lt;- nrow(top.mod.awakening.dtm) top.mod.awakening.dtm &lt;- top.mod.awakening.dtm[row_sums(top.mod.awakening.dtm &gt; 0) &gt; 1,] top.mod.awakening.lda_basic.model&lt;- LDA(top.mod.awakening.dtm, k = 5L, method = &quot;Gibbs&quot;, control = list(burnin = burnin, iter = iter, keep = keep, alpha = 1) ) top.mod.awakening.lda.topics &lt;- as.matrix(topics(top.mod.awakening.lda_basic.model)) #top.mod.awakening.lda.topics top.mod.awakening.lda.terms &lt;- as.matrix(terms(top.mod.awakening.lda_basic.model, 10)) top.mod.awakening.lda.terms&lt;- iconv(top.mod.awakening.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #top.mod.awakening.lda.terms awakening.top10termsPerTopic &lt;- terms(top.mod.awakening.lda_basic.model, 10) awakening.top10termsPerTopic &lt;- iconv(top.mod.awakening.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #awakening.top10termsPerTopic colnames(awakening.top10termsPerTopic)&lt;-c(&quot;Edna in her house/room&quot;,&quot; Work, life and love - Leaving the papers after he/she died&quot;,&quot;Madame Edna - Mademoiselle Ratignolle - Robert&quot;,&quot;Emotions - Description of a face &quot;,&quot;Mrs.Pontellier - Edna - Robert&quot;) awakening.top10termsPerTopic ## Edna in her house/room ## [1,] &quot;one&quot; ## [2,] &quot;like&quot; ## [3,] &quot;never&quot; ## [4,] &quot;thought&quot; ## [5,] &quot;day&quot; ## [6,] &quot;might&quot; ## [7,] &quot;seemed&quot; ## [8,] &quot;children&quot; ## [9,] &quot;two&quot; ## [10,] &quot;felt&quot; ## Work, life and love - Leaving the papers after he/she died ## [1,] &quot;project&quot; ## [2,] &quot;work&quot; ## [3,] &quot;must&quot; ## [4,] &quot;gutenbergtm&quot; ## [5,] &quot;without&quot; ## [6,] &quot;new&quot; ## [7,] &quot;found&quot; ## [8,] &quot;many&quot; ## [9,] &quot;feeling&quot; ## [10,] &quot;full&quot; ## Madame Edna - Mademoiselle Ratignolle - Robert ## [1,] &quot;upon&quot; ## [2,] &quot;eyes&quot; ## [3,] &quot;face&quot; ## [4,] &quot;hand&quot; ## [5,] &quot;looked&quot; ## [6,] &quot;sat&quot; ## [7,] &quot;white&quot; ## [8,] &quot;night&quot; ## [9,] &quot;table&quot; ## [10,] &quot;took&quot; ## Emotions - Description of a face Mrs.Pontellier - Edna - Robert ## [1,] &quot;little&quot; &quot;pontellier&quot; ## [2,] &quot;madame&quot; &quot;mrs&quot; ## [3,] &quot;went&quot; &quot;said&quot; ## [4,] &quot;away&quot; &quot;edna&quot; ## [5,] &quot;old&quot; &quot;will&quot; ## [6,] &quot;edna&quot; &quot;know&quot; ## [7,] &quot;back&quot; &quot;time&quot; ## [8,] &quot;house&quot; &quot;come&quot; ## [9,] &quot;ratignolle&quot; &quot;good&quot; ## [10,] &quot;robert&quot; &quot;mademoiselle&quot; awakening.topicNames &lt;- apply(awakening.top10termsPerTopic, 2, paste, collapse=&quot; &quot;) #awakening.topicNames # Aspern top.mod.aspern.dtm &lt;- DocumentTermMatrix(aspern.corpus, control = list(weighting=weightTf)) burnin = 1000 iter = 1000 keep = 50 set.seed(510) top.mod.aspern.n &lt;- nrow(top.mod.aspern.dtm) top.mod.aspern.dtm &lt;- top.mod.aspern.dtm[row_sums(top.mod.aspern.dtm &gt; 0) &gt; 1,] top.mod.aspern.lda_basic.model&lt;- LDA(top.mod.aspern.dtm, k = 5L, method = &quot;Gibbs&quot;, control = list(burnin = burnin, iter = iter, keep = keep, alpha = 1) ) top.mod.aspern.lda.topics &lt;- as.matrix(topics(top.mod.aspern.lda_basic.model)) #top.mod.aspern.lda.topics top.mod.aspern.lda.terms &lt;- as.matrix(terms(top.mod.aspern.lda_basic.model, 10)) top.mod.aspern.lda.terms&lt;- iconv(top.mod.aspern.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #top.mod.aspern.lda.terms aspern.top10termsPerTopic &lt;- terms(top.mod.aspern.lda_basic.model, 10) aspern.top10termsPerTopic &lt;- iconv(top.mod.aspern.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #aspern.top10termsPerTopic colnames(aspern.top10termsPerTopic)&lt;-c(&quot;Home - Seeing somebody - description of eyes - leaving&quot;,&quot;Jeffrey&quot;,&quot;Tita&quot;,&quot;Aunt&quot;,&quot;Time - Woman - Venice&quot;) aspern.top10termsPerTopic ## Home - Seeing somebody - description of eyes - leaving Jeffrey ## [1,] &quot;went&quot; &quot;project&quot; ## [2,] &quot;came&quot; &quot;gutenbergtm&quot; ## [3,] &quot;house&quot; &quot;work&quot; ## [4,] &quot;back&quot; &quot;papers&quot; ## [5,] &quot;face&quot; &quot;may&quot; ## [6,] &quot;looked&quot; &quot;almost&quot; ## [7,] &quot;took&quot; &quot;works&quot; ## [8,] &quot;away&quot; &quot;electronic&quot; ## [9,] &quot;look&quot; &quot;jeffrey&quot; ## [10,] &quot;put&quot; &quot;aspern&quot; ## Tita Aunt Time - Woman - Venice ## [1,] &quot;&quot; &quot;might&quot; &quot;old&quot; ## [2,] &quot;miss&quot; &quot;come&quot; &quot;one&quot; ## [3,] &quot;&quot; &quot;little&quot; &quot;made&quot; ## [4,] &quot;tita&quot; &quot;way&quot; &quot;never&quot; ## [5,] &quot;said&quot; &quot;aunt&quot; &quot;time&quot; ## [6,] &quot;know&quot; &quot;even&quot; &quot;long&quot; ## [7,] &quot;dont&quot; &quot;take&quot; &quot;great&quot; ## [8,] &quot;think&quot; &quot;make&quot; &quot;still&quot; ## [9,] &quot;oh&quot; &quot;idea&quot; &quot;woman&quot; ## [10,] &quot;bordereau&quot; &quot;told&quot; &quot;upon&quot; #aspern.topicNames &lt;- apply(aspern.top10termsPerTopic, 2, paste, collapse=&quot; &quot;) #aspern.topicNames 2.8 Main points of interpretation The most common words are names of characters such as Edna, Tita and Mrs.Pontellier. According to the sentiment analysis, Awakening novel is slightly from the beginning to the end. On the other hand, Aspern has a slight drop in the negative sentiment, but the end is positive. Awakening is as twice as longer text than Aspern. Due to lemmatization could not address the time of speaking (wheater it is told in past or future) Words said,little,think,house,good,like,one,come,back,thought\" are the most common. "],["twitter-analysis-for-co2mustgo-initiative.html", "3 Twitter Analysis for CO2mustGo initiative What is CO2mustGO? 3.1 Sankey diagram: Energy production capacity Twitter data analysis 3.2 Text cleaning and preprocessing 3.3 Tweets distribution 3.4 Word Frequency in Tweets 3.5 Word Network in Tweets 3.6 Wordcloud 3.7 Word associations 3.8 LDA 3.9 Sentiment analysis 3.10 Visualize the emotions 3.11 Top retweeted Tweets 3.12 Network of retweets", " 3 Twitter Analysis for CO2mustGo initiative What is CO2mustGO? Figure 3.1: Foto von Andrea Piacquadio von Pexels At the beginning of 2020 I heared that my working colleagues from another institute at the WU have launched an initiative called CO2mustGO initiative. The initiative aims to gather multinational group of students, researchers and teachers from the Vienna University of Economics and Business and other universities around Europe, to unite around the single issue of carbon price. This project started as a university course, with the vision of uniting students and scientists in an international movement supporting every serious carbon price initiative globally. Since my hometown, Tuzla, heavily suffers from the air pollution, I felt that I should give my contribution to this initiative and joined them in April 2020. I decided to support it with my R programming skills. Consequently, I ended up analyzing Twitter data, as it is one of the hot-spots for this topic. My role was to spark interest of stakeholders regarding this issue by using data available. Therefore, my tasks were related to using R to create understandable visualizations and to make use of Twitter data available. 3.1 Sankey diagram: Energy production capacity # Creation of source table$typ_11 &lt;- factor(table$typ_11,levels = c(&quot;c1&quot;,&quot;c2&quot;,&quot;g1&quot;,&quot;g2&quot;,&quot;g3&quot;,&quot;g4&quot;,&quot;n&quot;,&quot;o1&quot;,&quot;o2&quot;,&quot;o3&quot;,&quot;o4&quot;,&quot;r1&quot;,&quot;res&quot;),labels = c(&quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;,&quot;11&quot;,&quot;12&quot;)) source1 &lt;- as.vector.factor(table$typ_11) source1 &lt;- as.numeric(source1) #Country as mediator table2 &lt;- table table2$country&lt;-as.character(table2$country) table2$country[table$country==&quot;AT&quot;] &lt;- 14 table2$country[table$country==&quot;BE&quot;] &lt;- 15 table2$country[table$country==&quot;BG&quot;] &lt;- 16 table2$country[table$country==&quot;CH&quot;] &lt;- 17 table2$country[table$country==&quot;CY&quot;] &lt;- 18 table2$country[table$country==&quot;CZ&quot;] &lt;- 19 table2$country[table$country==&quot;DE&quot;] &lt;- 20 table2$country[table$country==&quot;DK&quot;] &lt;- 21 table2$country[table$country==&quot;EE&quot;] &lt;- 22 table2$country[table$country==&quot;ES&quot;] &lt;- 23 table2$country[table$country==&quot;FI&quot;] &lt;- 24 table2$country[table$country==&quot;FR&quot;] &lt;- 25 table2$country[table$country==&quot;GR&quot;] &lt;- 26 table2$country[table$country==&quot;HR&quot;] &lt;- 27 table2$country[table$country==&quot;HU&quot;] &lt;- 28 table2$country[table$country==&quot;IE&quot;] &lt;- 29 table2$country[table$country==&quot;IT&quot;] &lt;- 30 table2$country[table$country==&quot;LT&quot;] &lt;- 31 table2$country[table$country==&quot;LU&quot;] &lt;- 32 table2$country[table$country==&quot;LV&quot;] &lt;- 33 table2$country[table$country==&quot;NL&quot;] &lt;- 34 table2$country[table$country==&quot;NO&quot;] &lt;- 35 table2$country[table$country==&quot;PL&quot;] &lt;- 36 table2$country[table$country==&quot;PT&quot;] &lt;- 37 table2$country[table$country==&quot;RO&quot;] &lt;- 38 table2$country[table$country==&quot;SE&quot;] &lt;- 39 table2$country[table$country==&quot;SI&quot;] &lt;- 40 table2$country[table$country==&quot;SK&quot;] &lt;- 41 table2$country[table$country==&quot;UK&quot;] &lt;- 42 target1&lt;- table2$country #Creation of mediator(country),source and value value &lt;- table2 %&gt;% group_by(country,typ_11) %&gt;% count(mw_2018) #1st part of Target target1 &lt;- value$country target1&lt;-as.numeric(target1) #1st part of Value value1&lt;-value$mw_2018 #Creation of the final target value.df &lt;-table2 %&gt;% group_by(country)%&gt;% summarise(sum(mw_2018)) value.df$country&lt;-as.numeric(value.df$country) #2nd part of Source(from countries to total capacity) target1.2 &lt;-value.df$country #2nd part of Values (total of countries to the grand total) value1.2&lt;-value.df$`sum(mw_2018)` #Final target ft&lt;-rep(13,29) final_target &lt;- c(target1,ft) #final source final_soruce &lt;-c(source1,target1.2) #final value final_value &lt;-c(value1,value1.2) fig &lt;- plot_ly( type = &quot;sankey&quot;, orientation = &quot;h&quot;, valuesuffix = &quot;MW&quot;, arrangement = &quot;snap&quot;, node = list( label = c( &quot;Coal-lignite&quot;, # Node 0 &quot;Coal-hard&quot;, # Node 1 &quot;G1-gas&quot;, # Node 2 &quot;G2-gas&quot;, # Node 3 &quot;G3-gas&quot;, # Node 4 &quot;G4-gas&quot;, # Node 5 &quot;Nunclear&quot;, # Node 6 &quot;O1-oil&quot;, # Node 7 &quot;O2-oil&quot;, # Node 8 &quot;O3-oil&quot;, # Node 9 &quot;O4-oil&quot;, # Node 10 &quot;Renewables&quot;,# Node 11 &quot;Hydro&quot;, # Node 12 &quot;Total capacity&quot;, # Node 13 &quot;AT&quot;,#Node 14 &quot;BE&quot;,#Node 15 &quot;BG&quot;,#Node 16 &quot;CH&quot;,#Node 17 &quot;CY&quot;,#Node 18 &quot;CZ&quot;,#Node 19 &quot;DE&quot;,#Node 20 &quot;DK&quot;,#Node 21 &quot;EE&quot;,#Node 22 &quot;ES&quot;,#Node 23 &quot;FI&quot;,#Node 24 &quot;FR&quot;,#Node 25 &quot;GR&quot;,#Node 26 &quot;HR&quot;,#Node 27 &quot;HU&quot;,#Node 28 &quot;IE&quot;,#Node 29 &quot;IT&quot;,#Node 30 &quot;LT&quot;,#Node 31 &quot;LU&quot;,#Node 32 &quot;LV&quot;,#Node 33 &quot;NL&quot;,#Node 34 &quot;NO&quot;,#Node 35 &quot;PL&quot;,#Node 36 &quot;PT&quot;,#Node 37 &quot;RO&quot;,#Node 38 &quot;SE&quot;,#Node 39 &quot;SI&quot;,#Node 40 &quot;SK&quot;,#Node 41 &quot;UK&quot;)),#Node 42 link = list( source = final_soruce, target = final_target, value = final_value)) total.capacity &lt;- fig %&gt;% layout(title = &quot;Energy Production Capacity in the EU&quot;) total.capacity Figure 3.2: Energy production capacities in the EU In order to depict the nature of energy production facilities across EU countries, I created this sankey diagram. On the left side, you can see types of energy sources across the EU countries (data is not updated). In the middle, you can see respective countries. Finally, on the right you see the total of energy production. Especially nice feature of the visualization is the interactive component, so that, for instance, using Box select option, you can merge certain number of components of the same type (e.g., select 3 sources of energy) and the visualization changes accordingly. Twitter data analysis Twitter is nowadays among platforms that host strong communities. Carbon footprint and its repercussions belongs to prominent climate issue. Consequently, I decided to analyze Tweets downloaded via Twitter API on on 10.06.2020. The aim was simply to explore data about tweets with hashtags #carbonfootprint or #co2 and try to draw conclusions on how to approach social media presence on Twitter. 3.1.1 Data Our data was stored in CSV format.rtweet package provides nice function to read in data conveniently. #library(rtweet) # Read in Tweets data carbon_tweets &lt;- read_twitter_csv(&quot;data/#carbonfootprintOR#greenhouse-tweets.csv&quot;, unflatten = T) # Delete empty columns carbon_tweets &lt;- carbon_tweets[, colSums(is.na(carbon_tweets)) != nrow(carbon_tweets)] # Head of the data set dim(carbon_tweets) ## [1] 1858 86 Our data set contains 1858 Tweets and 86 features. 3.2 Text cleaning and preprocessing Tweets text in our data set requires some preprocessing and cleaning as it contains elements that are not helpful for our analysis. # Text cleaning carbon_tweets$stripped_text &lt;- gsub(&quot;https\\\\S*&quot;,&quot;&quot;, carbon_tweets$text) carbon_tweets$stripped_text &lt;- gsub(&quot;@\\\\S*&quot;,&quot;&quot;, carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;amp&quot;,&quot;&quot;,carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;[\\r\\n]&quot;,&quot;&quot;,carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;[[:punct:]]&quot;, &quot;&quot;,carbon_tweets$stripped_text) # Text to lowercase, punctuation removed, frequency of the each word added carbon_tweets_clean &lt;- carbon_tweets %&gt;% dplyr::select(stripped_text) %&gt;% unnest_tokens(word, stripped_text) # Remove stop words data(&quot;stop_words&quot;) carbon_tweets_words &lt;- carbon_tweets_clean %&gt;% anti_join(stop_words) 3.3 Tweets distribution Since our data set contains information about the time Tweets are posted, I was interested to see the distribution of Tweets in the given period. # Distribution of tweets considered in the data. search_term &lt;- &#39;#carbonfootprint OR #co2&#39; by &lt;- &#39;hour&#39; p &lt;- ts_plot(carbon_tweets, by = by, trim = 2) + geom_point(col = &quot;#00acee&quot;) + theme_minimal() + labs(title = paste0(&quot;Tweets with &quot;,search_term,&quot; by &quot;,by),x = &#39;Date&#39;, y = &#39;Count&#39;) ggplotly(p) The period captured in our data set is from 15:00 on 2nd of June to 11:00 A.M. on 10th of June. Interestingly enough, there was certain occasion on 6th of June important for our topic. We see from the number of Tweets with hashtags #carbonfootprint or #co2, which stood at 43! This happening has to be more closely analyzed. 3.4 Word Frequency in Tweets Next, I wanted to inspect the word frequency in the Tweets from the data set. p &lt;- carbon_tweets_words %&gt;% dplyr::count(word, sort=T) %&gt;% top_n(10) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x = word, y = n)) + geom_col(fill=&quot;deepskyblue&quot;) + theme_minimal()+ xlab(NULL) + coord_flip() + labs(x = &quot;Count&quot;, y = &quot;Unique words&quot;, title = &quot;Count of unique words found in tweets&quot;) ggplotly(p) Generally, no major surprises regarding the most frequent words. It can be noted that term worldenvironmentday appears frequently as the World Environment Day is on 5th of June. 3.5 Word Network in Tweets Knowing that Tweets are short messages, I decided to inspect word network in order to possibly observe some unusual word combinations. The word network is made based on bi-grams. Basically, based on the number of times two words shows up together. # Remove punctuation, convert to lowercase, add id for each tweet! carbon_tweets_paired_words &lt;- carbon_tweets %&gt;% dplyr::select(stripped_text) %&gt;% unnest_tokens(paired_words, stripped_text, token = &quot;ngrams&quot;, n = 2) #library(tidyr) carbon_tweets_separated_words &lt;- carbon_tweets_paired_words %&gt;% separate(paired_words, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) carbon_tweets_filtered &lt;- carbon_tweets_separated_words %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) carbon_words_counts &lt;- carbon_tweets_filtered %&gt;% dplyr::count(word1, word2, sort = TRUE) #library(igraph) #library(ggraph) # Plot carbon change word network p&lt;- carbon_words_counts %&gt;% filter(n &gt;=30) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = 0.5, edge_width = n)) + geom_node_point(color = &quot;darkslategray4&quot;, size = 3) + geom_node_text(aes(label = name), vjust = 1.8, size = 3) + labs(title = &quot;Word Network: Tweets using the hashtag #carbonfootprint or #co2&quot;, subtitle = &quot;Text mining twitter data &quot;, x = &quot;&quot;, y = &quot;&quot;) p An interesting observation is that the word network shows the word justiceforvinayaki appering together with climatecrisis. More specifically, justiceforvinayaki is actually a hashtag related to the story behind the pregnant elephants killing in Keralas Palakkad. More you can read here. 3.6 Wordcloud In order to get a bigger picture of frequent terms in Tweets, I created a wordcloud. # Text preparation carbon_tweets$stripped_text &lt;- iconv(carbon_tweets$stripped_text, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) review.docs &lt;- Corpus(VectorSource(carbon_tweets$stripped_text)) review.toSpace&lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;/&quot;) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;@&quot;) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;\\\\|&quot;) review.docs &lt;- tm_map(review.docs, content_transformer(tolower)) review.docs &lt;- tm_map(review.docs, removeNumbers) review.docs &lt;- tm_map(review.docs, removeWords, stopwords(&quot;english&quot;)) my_list &lt;- c(&quot;carbonfootprint&quot;,&quot;greenhouse&quot;,&quot;can&quot;,&quot;will&quot;,&quot;wont&quot;) review.docs &lt;- tm_map(review.docs, removeWords,my_list) review.docs &lt;- tm_map(review.docs, content_transformer(tolower)) review.docs &lt;- tm_map(review.docs, removePunctuation) review.docs &lt;- tm_map(review.docs, stripWhitespace) review.tdm &lt;- TermDocumentMatrix(review.docs) review.m &lt;- as.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) review.d &lt;- data.frame(word = names(review.v),freq=review.v) set.seed(1234) wordcloud(words = review.d$word, freq = review.d$freq, max.words = 200, min.freq = 10, random.order=FALSE, rot.per=0.15, colors=brewer.pal(8, &quot;Dark2&quot;), scale=c(8,.3), vfont=c(&quot;sans serif&quot;,&quot;plain&quot;)) Minimum word frequency is set to 10, and the wordcloud depicts 200 most frequent terms. As you probably already know, Twitter is a place where discussions are going on frequently. Therefore, it made sense to try to reorganize the wordcloud to indicate positive vs negative terms. library(wordcloud) library(reshape2) par(mar = rep(0, 4)) set.seed(1234) p&lt;-carbon_tweets_words%&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment,sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;darkred&quot;,&quot;darkgreen&quot;), max.words = 400, min.freq= 10, scale = c(4.0,0.25)) p 3.7 Word associations With the function findAssocs() we are able to inspect correlation between given term and every other word in our term-document-matrix. Strong associations are indicated with numbers closer to 1, while weaker associations are closer to 0. In addition, the function enables setting a cut-off threshold, so that, for instance, it extracts just correlations higher or equal than 0.6. library(cowplot) library(ggplot2) library(ggthemes) tdm &lt;- TermDocumentMatrix(review.docs, control=list(weighting=weightTf)) # Association with the term &quot;globalwarming&quot; associations &lt;- findAssocs(tdm,terms=&quot;globalwarming&quot;, corlimit=0.6) associations &lt;- as.data.frame(associations) associations$terms &lt;- row.names(associations) associations$terms &lt;- factor(associations$terms, levels=associations$terms) assoc_globalwarming &lt;- ggplot(associations, aes(y=terms)) + geom_point (aes(x=globalwarming), data=associations, size=0.01)+ theme_gdocs()+ geom_text(aes(x=globalwarming, label=globalwarming), colour=&quot;red&quot;, hjust=-.5, size=3)+ labs(title = &quot;Terms correlated with the term &#39;globalwarming&#39; (corr &gt; 0.6)&quot;)+ theme(text=element_text(size=8), axis.title.y=element_blank()) ggplotly(assoc_globalwarming) # Association with the term &quot;lose&quot; associations &lt;- findAssocs(tdm,terms=&quot;lose&quot;, corlimit=0.6) associations &lt;- as.data.frame(associations) associations$terms &lt;- row.names(associations) associations$terms &lt;- factor(associations$terms, levels=associations$terms) assoc_lose &lt;- ggplot(associations, aes(y=terms)) + geom_point (aes(x=lose), data=associations, size=0.01)+ theme_gdocs()+ geom_text(aes(x=lose, label=lose), colour=&quot;red&quot;, hjust=-.5, size=3)+ labs(title = &quot;Terms correlated with the term &#39;lose&#39; (corr &gt; 0.6)&quot;)+ theme(text=element_text(size=8), axis.title.y=element_blank()) ggplotly(assoc_lose) 3.8 LDA We could try to break collection of tweets into several topics, so that we can independently understand what topics tweets are associated with. Topic modeling is a tool for unsupervised classification of certain documents that finds natural groups of topics even though we are not sure what we are searching for. # Libraries library(sjmisc) library(topicmodels) library(ldatuning) # Document Term Matrix review.dtm &lt;- DocumentTermMatrix(review.docs) burnin = 1000 iter = 10000 keep = 50 set.seed(510) raw.sum=apply(review.dtm,1,FUN=sum) #sum by raw each raw of the table review.dtm = review.dtm[raw.sum!=0,] lda_basic.model&lt;- LDA(review.dtm, k = 4L, method = &quot;Gibbs&quot;, control = list(burnin = burnin, iter = iter, keep = keep, alpha = 0.01)) lda.terms &lt;- as.matrix(terms(lda_basic.model, 10)) lda.terms&lt;- iconv(lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) top10termsPerTopic &lt;- terms(lda_basic.model, 10) top10termsPerTopic &lt;- iconv(lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) colnames(top10termsPerTopic)&lt;-c(&quot;Renewable sources of energy&quot;,&quot;Carbonfootprint emissions&quot;,&quot;Carbonfootprint by businesses&quot;,&quot;Global warming&quot;) top10termsPerTopic ## Renewable sources of energy Carbonfootprint emissions ## [1,] &quot;energy&quot; &quot;carbon&quot; ## [2,] &quot;source&quot; &quot;reduce&quot; ## [3,] &quot;gardening&quot; &quot;worldenvironmentday&quot; ## [4,] &quot;garden&quot; &quot;sustainability&quot; ## [5,] &quot;sustainable&quot; &quot;footprint&quot; ## [6,] &quot;doesnt&quot; &quot;climatechange&quot; ## [7,] &quot;reduce&quot; &quot;emissions&quot; ## [8,] &quot;electricity&quot; &quot;growing&quot; ## [9,] &quot;solarenergy&quot; &quot;gardening&quot; ## [10,] &quot;renewable&quot; &quot;help&quot; ## Carbonfootprint by businesses Global warming ## [1,] &quot;carbon&quot; &quot;environment&quot; ## [2,] &quot;business&quot; &quot;worldenvironmentday&quot; ## [3,] &quot;reduce&quot; &quot;climatechange&quot; ## [4,] &quot;emissions&quot; &quot;world&quot; ## [5,] &quot;energy&quot; &quot;globalwarming&quot; ## [6,] &quot;buy&quot; &quot;just&quot; ## [7,] &quot;best&quot; &quot;climatecrisis&quot; ## [8,] &quot;see&quot; &quot;happy&quot; ## [9,] &quot;footprint&quot; &quot;hope&quot; ## [10,] &quot;sustainability&quot; &quot;climatechangeisreal&quot; We could identify 4 topics such as: Renewable resources Carbonfootprint emissions Carbonfootprint by and sustainability in business Climate and environment crisis caused by global warming 3.9 Sentiment analysis Knowing the nature of the carbon footprint relatFed topics and based on the previous assumption about vivid discussions on Twitter, an analysis of emotions in Tweets would help us in opinion mining. # Sentiment analysis sentiment &lt;- carbon_tweets[,3:5] %&gt;% unnest_tokens(output = &#39;word&#39;, input = &#39;text&#39;) #Add sentiment dataset sentiment_dataset &lt;- get_sentiments(&quot;afinn&quot;) sentiment_dataset &lt;- arrange(sentiment_dataset, -value) #Merge sentiment &lt;- merge(sentiment, sentiment_dataset, by = &#39;word&#39;) #Clean sentiment$word &lt;- NULL sentiment$screen_name &lt;- NULL #Time sentiment$hour &lt;- format(base::round.POSIXt(sentiment$created_at, units=&quot;hours&quot;), format=&quot;%H:%M&quot;) #Pivot pivot &lt;- sentiment %&gt;% group_by(hour) %&gt;% summarise(sentiment = mean(value)) #Plot p &lt;- ggplot(pivot[-1,], aes(x = hour, y = sentiment)) + geom_line(group = 1, color=&quot;deepskyblue&quot;) + geom_point() + theme_minimal() + labs(title = paste0(&#39;Average sentiment of tweetings mentioning &quot;&#39;,search_term,&#39;&quot;&#39;),x = &#39;Date&#39;, y = &#39;Sentiment&#39;, caption = &#39;Source: Twitter API&#39;) ggplotly(p) The visualisation above depicts the average sentiment score of Tweets during a day. It seems that at 10:00 and 15:00 Tweets tend to be less positive than at 17:00 for instance. This information helps for scheduling Tweets so that they dont get caught in the bad moment. 3.10 Visualize the emotions In order to decide what sort of discussion was going on the period of observation, we can visualise the count of words indicating emotions. # Get sentiments using the four different lexicons syuzhet &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;syuzhet&quot;) bing &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;bing&quot;) afinn &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;afinn&quot;) nrc &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;nrc&quot;) sentiments &lt;- data.frame(syuzhet, bing, afinn, nrc) # get the emotions using the NRC dictionary nrc.sentiment &lt;- get_nrc_sentiment(carbon_tweets$stripped_text) emo_bar = colSums(nrc.sentiment) emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar)) emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)]) # Visualize the emotions from NRC sentiments plot_ly(emo_sum, x=~emotion, y=~count, type=&quot;bar&quot;, color=~emotion) %&gt;% layout(xaxis=list(title=&quot;&quot;), showlegend=FALSE, title=&quot;Distribution of emotion categories&quot;) It seems that positivity, trust, anticipation and joy are far more present in Tweets than emotions usually associated with something negative. Again, we can organize wordcloud so that it does not show just positive or negative words, but rather words associated with corresponding emotion. # Comparison word cloud all = c( paste(carbon_tweets$stripped_text[nrc.sentiment$anger &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$anticipation &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$disgust &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$fear &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$joy &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$sadness &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$surprise &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$trust &gt; 0], collapse=&quot; &quot;) ) all &lt;- removeWords(all, stopwords(&quot;english&quot;)) # create corpus corpus = Corpus(VectorSource(all)) # # create term-document matrix tdm = TermDocumentMatrix(corpus) # # convert as matrix tdm = as.matrix(tdm) tdm1 &lt;- tdm[nchar(rownames(tdm)) &lt; 11,] # # add column names colnames(tdm) = c(&#39;anger&#39;, &#39;anticipation&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;sadness&#39;, &#39;surprise&#39;, &#39;trust&#39;) colnames(tdm1) &lt;- colnames(tdm) comparison.cloud(tdm1, random.order=FALSE, colors = c(&quot;#00B2FF&quot;, &quot;red&quot;, &quot;#FF0099&quot;, &quot;#6600CC&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;blue&quot;, &quot;brown&quot;), title.size=1, max.words=250, scale=c(2.5, 0.4),rot.per=0.4) 3.11 Top retweeted Tweets Top retweets (with equal or more than 60 mentions) Next task was to identify Tweets that stand out. The graph below helped me to identify days on which certain Tweets were re-tweeted substaintially more than usually. # Select top retweeted tweets selected &lt;- which(carbon_tweets$retweet_count &gt;= 60) # Plot dates &lt;-as.POSIXct(strptime(carbon_tweets$created_at, format=&quot;%Y-%m-%d&quot;)) plot(x=dates, y=carbon_tweets$retweet_count, type=&quot;l&quot;, col=&quot;grey&quot;, xlab=&quot;Date&quot;, ylab=&quot;Times retweeted&quot;) colors &lt;- rainbow(10)[1:length(selected)] points(dates[selected], carbon_tweets$retweet_count[selected], pch=19, col=colors) Interactive graph with retweets text In this interactive graph I manage to identify Tweets with their text that were re-tweeted frequently. If you hover over big sky-blue points you will see the actual text of each Tweet. # Plotly carbon_tweets$created_at &lt;-as.POSIXct(strptime(carbon_tweets$created_at, format=&quot;%Y-%m-%d&quot;)) p&lt;-ggplot(carbon_tweets, aes(x=created_at, y=retweet_count, col=retweet_count, size=retweet_count, retweet_text=retweet_text, created_at=created_at, retweet_name=retweet_name))+geom_point() +xlab(label=&quot;Date&quot;)+ylab(label=&quot;Retweet count&quot;)+ggtitle(label=&quot;Top retweeted tweets&quot;) ggplotly(p,tooltip = c(&quot;retweet_text&quot;,&quot;retweet_name&quot;)) The most re-tweeted Tweet comes from Forest Products Resolute, which is a global leader in the forest products industry with a diverse range of products, including market pulp, tissue, wood products and papers, which are marketed in close to 70 countries. They tweeted that they are going to take decisive actions towards lowering carbon footprint. 3.12 Network of retweets Finally, I decided to explore the network of the Tweets in order to identify some communities on Twitter that the initiative could potentially join. # Create data frame for the network rt_df &lt;- carbon_tweets[, c(&quot;screen_name&quot; , &quot;retweet_screen_name&quot; )] # Remove rows with missing values rt_df_new &lt;- rt_df[complete.cases(rt_df), ] # Convert to matrix matrx &lt;- as.matrix(rt_df_new) # Create the retweet network nw_rtweet &lt;- graph_from_edgelist(el = matrx, directed = TRUE) # View the retweet network print.igraph(nw_rtweet) ## IGRAPH 337b589 DN-- 1100 936 -- ## + attr: name (v/c) ## + edges from 337b589 (vertex names): ## [1] Amrapali_c -&gt;cathrinejahnsen hanopcan -&gt;GreenTech_SWest ## [3] MehmetO33440789-&gt;UKHaulier m_carmody -&gt;crowdfarmingco ## [5] amrendrakumar02-&gt;SUPERGASind abhishekk85 -&gt;theswitchfix ## [7] MarkCNorwich -&gt;52WeeksForEarth RPiUptime -&gt;TrafficlyApp ## [9] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [11] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [13] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [15] RPiUptime -&gt;TrafficlyApp madebyhyphae -&gt;GreenTech_SWest ## + ... omitted several edges ## [1] 828 2 3.12.1 Where do people tweet the most? The interactive world-map plot below allows you to zoom-in certain regions to have a better look what countries are hot-spots when it comes to carbon footprint and co2-related topics. # Extract geolocation data and append new columns library(rtweet) library(sf) library(rnaturalearth) library(rnaturalearthdata) library(rgeos) pol_coord &lt;- lat_lng(carbon_tweets) pol_geo &lt;- na.omit(pol_coord[, c(&quot;lat&quot;, &quot;lng&quot;,&quot;location&quot;,&quot;retweet_count&quot;)]) world &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) p&lt;-ggplot(data = world) + geom_sf() + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + geom_point(data= pol_geo,aes(x=lng, y=lat,loc=location,retweet_count=retweet_count),col = &quot;#00acee&quot;)+ theme(panel.grid.major = element_line(color = gray(.25), linetype =&quot;dashed&quot;, size = 0.15),panel.background = element_rect(fill = &quot;aliceblue&quot;))+ ggtitle(&quot;World map with tweets location and retweet count&quot;, subtitle = paste0(&quot;(&quot;, length(unique(pol_geo$location)), &quot; countries)&quot;)) ggplotly(p,tooltip = c(&quot;location&quot;,&quot;retweet_count&quot;)) 3.12.2 Who are the users who retweet the most? # Calculate the out-degree scores out_degree &lt;- degree(nw_rtweet, mode = c(&quot;out&quot;)) # Sort the users in descending order of out-degree scores out_degree_sort &lt;- sort(out_degree, decreasing = TRUE) head(out_degree_sort,10) ## imagine_garden RPiUptime taxa_monocot Eco1stArt greenhousedave ## 9 8 7 5 5 ## pepparsteve LazarovMartin7 PetrovichBilly greentechdon researchmrx ## 4 4 4 4 3 # INTERPRETATION: Users who retweeted the most. #Hubs: Tweeter accounts with a lot of outgoing edges. hs &lt;- hub_score(nw_rtweet, weights=NA)$vector sort(hs, decreasing = TRUE)[1:20] ## SaachinPatel virendrathor007 ModheraAjay YuvraajsO mahisawOfficial ## 1 1 1 1 1 ## MayankS08111059 Maveric94280289 ujjain_live PayaswiniShett1 MaheshK70846514 ## 1 1 1 1 1 ## er_gaurav_singh mohanbhadri rushessensedood ShivanshikaF GMahindroo ## 1 1 1 1 1 ## avng47 berojgaradami SandeepKumawat_ _AdityaRaje kirtischandel ## 1 1 1 1 1 Ex-kurs: An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices.They are likely to retweet. 3.12.3 Who are the most retweeted users? # Calculate the in-degree scores in_degree &lt;- degree(nw_rtweet, mode = c(&quot;in&quot;)) # Sort the users in descending order of in-degree scores in_degree_sort &lt;- sort(in_degree, decreasing = TRUE) head(in_degree_sort,10) ## ReSanskrit PeacockSolar rockerblonde StellaYeahilike WaterlooEnergy ## 81 41 25 19 19 ## gulf_intel DanAlluf BPCLimited ByronTweetsData TrafficlyApp ## 17 17 15 14 13 Lets identify who are authorities, i.e., Twitter accounts with a lot of incoming edges. as &lt;- authority_score(nw_rtweet, weights=NA)$vector sort(as, decreasing = TRUE)[1:20] ## ReSanskrit TrafficlyApp PeacockSolar OrchidOfTheDay rockerblonde ## 1.000000e+00 1.677825e-14 3.459879e-15 1.982355e-15 1.238491e-15 ## WaterlooEnergy DanAlluf BPCLimited ByronTweetsData IntlPeaceBureau ## 8.468956e-16 8.319809e-16 8.051224e-16 7.793320e-16 7.423804e-16 ## starindia L_FudgerGalvez ESA_EO gulf_intel StellaYeahilike ## 7.423804e-16 6.518274e-16 5.949006e-16 5.739893e-16 5.672778e-16 ## crowdfarmingco katyalston gikiearth CrownPlatform AyiccZim ## 5.267684e-16 5.267684e-16 5.197328e-16 4.809261e-16 4.755414e-16 An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices.They are likely to be retweeted. 3.12.4 Who are users important for connecting with others in network? Here I aimed to identify users with important role in allowing information to pass through network. Usually, users with higher betweenness has more control over the network. # Calculate the betweenness scores of the network betwn_nw &lt;- betweenness(nw_rtweet, directed = TRUE) # Sort the users in descending order of betweenness scores betwn_nw_sort &lt;- betwn_nw %&gt;% sort(decreasing = TRUE) %&gt;% round() %&gt;% head(10) betwn_nw_sort ## gikiearth Privatecarfree ChinyeRumby DioxideMat LifexSoles ## 6 6 4 2 1 ## esuohneerg swift_iron NGO_NISWARTH BPCLBareilly Va3tsal ## 1 1 1 1 1 3.12.5 Clustering largest_cliques(nw_rtweet) #list only 20 vertices in that cluster ## [[1]] ## + 3/1100 vertices, named, from 337b589: ## [1] BpclStateLPGUP BPCLimited BPCLBareilly ## ## [[2]] ## + 3/1100 vertices, named, from 337b589: ## [1] iona_nyandoro LifexSoles LxS_Build ## ## [[3]] ## + 3/1100 vertices, named, from 337b589: ## [1] MKY110987 BPCLimited BPCLBareilly ## ## [[4]] ## + 3/1100 vertices, named, from 337b589: ## [1] DarlacGardening WaterWorxUK swift_iron 3.12.6 Community detection Finally, I tried to identify communities, i.e., groups of account that might be interested in engaging in conversation about carbon footprint and co2 emissions. #Community detection based on edge betweenness (Newman-Girvan) comm &lt;- cluster_edge_betweenness(nw_rtweet) sort(sizes(comm), decreasing = T)[1:20] ## Community sizes ## 60 44 16 74 98 31 108 233 205 58 42 114 91 110 21 57 4 37 48 99 ## 82 42 31 25 22 20 20 18 15 14 13 13 12 12 11 11 10 10 10 10 comm_1 &lt;- communities(comm) # Tweet accounts in the Community 60 (the biggest community) comm_1$`60` ## [1] &quot;SaachinPatel&quot; &quot;ReSanskrit&quot; &quot;virendrathor007&quot; &quot;ModheraAjay&quot; ## [5] &quot;YuvraajsO&quot; &quot;mahisawOfficial&quot; &quot;MayankS08111059&quot; &quot;Maveric94280289&quot; ## [9] &quot;ujjain_live&quot; &quot;PayaswiniShett1&quot; &quot;MaheshK70846514&quot; &quot;er_gaurav_singh&quot; ## [13] &quot;mohanbhadri&quot; &quot;rushessensedood&quot; &quot;ShivanshikaF&quot; &quot;GMahindroo&quot; ## [17] &quot;avng47&quot; &quot;berojgaradami&quot; &quot;SandeepKumawat_&quot; &quot;_AdityaRaje&quot; ## [21] &quot;kirtischandel&quot; &quot;Sai_3196&quot; &quot;Abhi_Gosavi17&quot; &quot;g_one01&quot; ## [25] &quot;amritanshu20712&quot; &quot;Shrirang_4u&quot; &quot;ChandanSahaDas1&quot; &quot;HGhumnar&quot; ## [29] &quot;im_zala&quot; &quot;the_bhaveshp&quot; &quot;SolankiDhavalJ&quot; &quot;AshokDh10070811&quot; ## [33] &quot;VGilankar&quot; &quot;free_sridhar&quot; &quot;looookeeee&quot; &quot;_dactar_babu&quot; ## [37] &quot;QnalH&quot; &quot;Bhuhan_Raut12&quot; &quot;thakurmanish25&quot; &quot;Dvipalgoswami&quot; ## [41] &quot;SSSPrashantS&quot; &quot;pahariparul&quot; &quot;shwetarathi0301&quot; &quot;Pournimakothap1&quot; ## [45] &quot;SKMaisuriya&quot; &quot;gandhecha_deep&quot; &quot;ravi_patel21&quot; &quot;SarveshdKumar&quot; ## [49] &quot;jenilsaurabh&quot; &quot;Byomkesh5&quot; &quot;vishaltechrexx&quot; &quot;Shubham82408211&quot; ## [53] &quot;Damnshashi&quot; &quot;ms_ayushi&quot; &quot;SankalpKr&quot; &quot;rchandra12&quot; ## [57] &quot;Vishalcr7999&quot; &quot;ub1112&quot; &quot;VishalRajShodhi&quot; &quot;prats_ag&quot; ## [61] &quot;Priyank64671386&quot; &quot;RudreshScharma&quot; &quot;naagaputra&quot; &quot;ipradyu&quot; ## [65] &quot;Prakash13119341&quot; &quot;anitajha2802&quot; &quot;Ankit_yadaavv&quot; &quot;sanatan_&quot; ## [69] &quot;Raj_Sawant96&quot; &quot;sohil_oza&quot; &quot;NoAbsurdity&quot; &quot;PujariRaksha&quot; ## [73] &quot;ddhamaal_bai&quot; &quot;AyushmanDubey18&quot; &quot;NSK_Kochhar&quot; &quot;HadaniSuresh&quot; ## [77] &quot;binayamishra16&quot; &quot;realMeetu&quot; &quot;Sanmon96952110&quot; &quot;ItsRajatRai&quot; ## [81] &quot;ramkotipalli&quot; &quot;Omtripathi95&quot; # Tweet accounts in the Community 44 (the second biggest community) comm_1$`44` ## [1] &quot;TushitGarg&quot; &quot;PeacockSolar&quot; &quot;ianikk18&quot; &quot;rupeshwar_rao&quot; ## [5] &quot;imjyotidwivedi&quot; &quot;NandiniRajendr4&quot; &quot;5a13fec9874b479&quot; &quot;Moanish58105963&quot; ## [9] &quot;sonukau18053840&quot; &quot;snipervenom21&quot; &quot;RobinSingh1825&quot; &quot;Shalmalichakra3&quot; ## [13] &quot;RubalSh04483180&quot; &quot;arun_thevan03&quot; &quot;AshishK54595782&quot; &quot;BasantKKothari&quot; ## [17] &quot;SriAtul2&quot; &quot;roshan_munda&quot; &quot;KillerSrinivas2&quot; &quot;RitanshuChugh&quot; ## [21] &quot;KunalSh62765789&quot; &quot;Rampras17015630&quot; &quot;manvpandit&quot; &quot;27shikhar&quot; ## [25] &quot;anshulajin&quot; &quot;KumariMitya&quot; &quot;EmescoFdn&quot; &quot;AtishKu82064443&quot; ## [29] &quot;ReetuShukla12&quot; &quot;VIJAYAS36039459&quot; &quot;Shashwa38747031&quot; &quot;ManishV59713691&quot; ## [33] &quot;Sanjeev16620270&quot; &quot;Aashish04884619&quot; &quot;ArchitG36451766&quot; &quot;Vikrant64760265&quot; ## [37] &quot;bunny3298&quot; &quot;Joydeep85178197&quot; &quot;KishoreH14&quot; &quot;HarshTi80357066&quot; ## [41] &quot;Sivas_07&quot; &quot;AbhijitOjha7&quot; # Tweet accounts in the Community 16 (the second biggest community) comm_1$`16` ## [1] &quot;gardener_the&quot; &quot;HoratiosGarden&quot; &quot;BrownBurden&quot; &quot;VivienLloyd&quot; ## [5] &quot;LazarovMartin7&quot; &quot;FreeDealSteals&quot; &quot;theOGryankirk&quot; &quot;StellaYeahilike&quot; ## [9] &quot;vhhydroponics&quot; &quot;agritecture&quot; &quot;PlantGetEnough1&quot; &quot;GladerPhilip&quot; ## [13] &quot;Woroud&quot; &quot;tepanchinceva&quot; &quot;Smart_Reads&quot; &quot;birdwriter7&quot; ## [17] &quot;hiro_thoyou3864&quot; &quot;Watrasri&quot; &quot;AgtechOtori&quot; &quot;sthanleyc&quot; ## [21] &quot;richardelio_nyc&quot; &quot;Yeahilike&quot; &quot;azkamel1&quot; &quot;edohpa&quot; ## [25] &quot;romi_hime_black&quot; &quot;banan_thompson&quot; &quot;StellaSanLF&quot; &quot;PeLopez1&quot; ## [29] &quot;RTusuzuro_&quot; &quot;umada_ushijiro&quot; &quot;Sun_Flower119&quot; "],["job-interview-task-social-media-data.html", "4 Job-interview task: Social media data 4.1 Data 4.2 How many authors have interacted in the database? 4.3 Which one is the most used media? 4.4 What is the percentage of positive, negative and neutral comments? 4.5 What is the average sentiment in Twitter? 4.6 Visalisation task", " 4 Job-interview task: Social media data Figure 4.1: Foto from Google This tasks is a part of a case study given as a job-exercise. Here I aim to demonstrate how one can solve analytics task with R very efficiently. The exact task reads as follow: At the company we create clarity, out of the chaos of digital noise. Our big data analytics platform and services combine technology and human expertise to help organizations around the world achieve clear and actionable insights every day. In our team of data scientists, you will become part of the human layer that develops specialized expertise for organizations, we explore hypotheses and dig deeper into big data assets and uncover actionable insights. This assignment is designed to give you a glimpse of some of the challenges you will be facing in this role. Please be aware there are no perfect solutions - for us, its more important to see how you find solutions, process your ideas, structure your thoughts and how you make your decision paths. Be creative but realistic about whats possible. We are thrilled to get to know a bit more about the way you solve tasks. 4.1 Data library(XML) library(tibble) library(tidyverse) library(readr) library(kableExtra) library(ggplot2) library(plotly) As usual, we first load in the data by using readxl_xlsx() function and take a glimpse at it: data&lt;-readxl::read_xlsx(&quot;data/Alto-Case_Study_Dataset (1).xlsx&quot;) glimpse(data) ## Rows: 4,287 ## Columns: 12 ## $ ID &lt;dbl&gt; 109537653, 109556421, 109537642, 109543173, 1095... ## $ Autors &lt;chr&gt; &quot;josiehoulston&quot;, &quot;O2&quot;, &quot;MuccaMadness&quot;, &quot;bhups&quot;, ... ## $ TITLE &lt;chr&gt; &quot;@O2 no signal again. Third day of interruption&quot;... ## $ LINK &lt;chr&gt; &quot;http://twitter.com/josiehoulston/statuses/60394... ## $ BODY &lt;chr&gt; &quot;@O2 no signal again. Third day of interruption&quot;... ## $ PUBDATE &lt;dttm&gt; 2015-05-28 17:29:47, 2015-05-28 17:29:30, 2015-... ## $ `PERSONAL-WEBSITE` &lt;chr&gt; &quot;http://twitter.com/josiehoulston&quot;, &quot;http://www.... ## $ COUNTRY &lt;chr&gt; &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, ... ## $ `PUBLISHER-ID` &lt;dbl&gt; 11, 11, 11, 140, 11, 11, 11, 11, 10, 11, 11, 11,... ## $ `PUBLISHER-NAME` &lt;chr&gt; &quot;Twitter&quot;, &quot;Twitter&quot;, &quot;Twitter&quot;, &quot;GiffGaff&quot;, &quot;Tw... ## $ `ORIG-ID` &lt;chr&gt; &quot;603946276951031808&quot;, &quot;603946202216955904&quot;, &quot;603... ## $ SENTIMENT &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,... The data has 12 features and 4287 observations. Let us inspect missing values: apply(is.na(data),2,sum) ## ID Autors TITLE LINK ## 0 0 0 0 ## BODY PUBDATE PERSONAL-WEBSITE COUNTRY ## 5 0 15 0 ## PUBLISHER-ID PUBLISHER-NAME ORIG-ID SENTIMENT ## 0 0 0 0 There are 5 missing entries in BODY column and 15 in PERSONAL-WEBISTE. Consequently, we will remove these entries from the data set. data&lt;-(data[complete.cases(data), ]) data ## # A tibble: 4,267 x 12 ## ID Autors TITLE LINK BODY PUBDATE `PERSONAL-WEBSI~ COUNTRY ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.10e8 josie~ @O2 ~ http~ @O2 ~ 2015-05-28 17:29:47 http://twitter.~ gb ## 2 1.10e8 O2 @luk~ http~ @luk~ 2015-05-28 17:29:30 http://www.o2.c~ gb ## 3 1.10e8 Mucca~ @Sar~ http~ @Sar~ 2015-05-28 17:29:15 http://twitter.~ gb ## 4 1.10e8 bhups Re: ~ http~ Chec~ 2015-05-28 17:29:00 https://communi~ gb ## 5 1.10e8 helen~ @Ash~ http~ @Ash~ 2015-05-28 17:28:54 http://twitter.~ gb ## 6 1.10e8 21Ayu~ RT @~ http~ RT @~ 2015-05-28 17:28:36 http://twitter.~ gb ## 7 1.10e8 O2tou~ RT @~ http~ RT @~ 2015-05-28 17:28:14 http://O2Touch.~ gb ## 8 1.18e8 O2Aca~ RT @~ http~ RT @~ 2015-05-28 17:28:08 http://www.o2ac~ gb ## 9 1.49e8 Dave ~ Re: ~ http~ cant~ 2015-05-28 17:28:04 https://www.fac~ gb ## 10 1.10e8 Emmie~ @The~ http~ @The~ 2015-05-28 17:27:52 http://twitter.~ gb ## # ... with 4,257 more rows, and 4 more variables: `PUBLISHER-ID` &lt;dbl&gt;, ## # `PUBLISHER-NAME` &lt;chr&gt;, `ORIG-ID` &lt;chr&gt;, SENTIMENT &lt;dbl&gt; Finally, our data has the following structure: glimpse(data) ## Rows: 4,267 ## Columns: 12 ## $ ID &lt;dbl&gt; 109537653, 109556421, 109537642, 109543173, 1095... ## $ Autors &lt;chr&gt; &quot;josiehoulston&quot;, &quot;O2&quot;, &quot;MuccaMadness&quot;, &quot;bhups&quot;, ... ## $ TITLE &lt;chr&gt; &quot;@O2 no signal again. Third day of interruption&quot;... ## $ LINK &lt;chr&gt; &quot;http://twitter.com/josiehoulston/statuses/60394... ## $ BODY &lt;chr&gt; &quot;@O2 no signal again. Third day of interruption&quot;... ## $ PUBDATE &lt;dttm&gt; 2015-05-28 17:29:47, 2015-05-28 17:29:30, 2015-... ## $ `PERSONAL-WEBSITE` &lt;chr&gt; &quot;http://twitter.com/josiehoulston&quot;, &quot;http://www.... ## $ COUNTRY &lt;chr&gt; &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, &quot;gb&quot;, ... ## $ `PUBLISHER-ID` &lt;dbl&gt; 11, 11, 11, 140, 11, 11, 11, 11, 10, 11, 11, 11,... ## $ `PUBLISHER-NAME` &lt;chr&gt; &quot;Twitter&quot;, &quot;Twitter&quot;, &quot;Twitter&quot;, &quot;GiffGaff&quot;, &quot;Tw... ## $ `ORIG-ID` &lt;chr&gt; &quot;603946276951031808&quot;, &quot;603946202216955904&quot;, &quot;603... ## $ SENTIMENT &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,... 4.2 How many authors have interacted in the database? My first task is to identify the number of authors who had interaction with the database. In order to do so, we will use column Autors. This column has a character class. Let us inspect number of authors asked: # Total number of interactions interacted length(data$Autors) ## [1] 4267 # Number of authors interacted with the data base length(unique(data$Autors)) ## [1] 2672 Out of total 4267 interactions, 2672 are unique. Thus, we can say that the number of unique interactions is 2672. # Authors (task1&lt;-as.data.frame(table(data$Autors)) %&gt;% rename(Authors=Var1, Count=Freq) %&gt;% arrange(desc(Count))) %&gt;% head(10)%&gt;% kableExtra::kable(escape = T) %&gt;% kable_paper(c(&quot;hover&quot;), full_width = F) Authors Count O2 258 SoldoutGigs 33 O2JobsFeed 25 yobigdawg 14 mathew40 12 Cleoriff 10 O2AcademyOxford 10 PocutDaraa 10 MI5 9 JimGilroy 8 Let us make a visualisation of the table: library(ggplot2) task1 %&gt;% arrange(desc(Count)) %&gt;% head(10) %&gt;% ggplot(aes(x=reorder(Authors,Count),y=Count,fill=Authors)) + geom_bar(stat=&quot;identity&quot;)+ labs(x=&quot;Authors&quot;,y=&quot;Count&quot;,title = &quot;Top 10 Most Active Authors on Twitter&quot;, subtitle = &quot;O2 is significantly ahead in comparison to the rest of authors.&quot;)+ coord_flip()+ theme_bw()+ theme(legend.position = &quot;none&quot;) The most active authors on Twitter are: O2 - as the most active by far, SoldoutGigs and O2JobsFeed - follows after O2. 4.3 Which one is the most used media? media &lt;- as.data.frame(table(data$`PUBLISHER-NAME`)) colnames(media) &lt;- c(&quot;Media&quot;,&quot;Freq&quot;) # Top 10 media used media &lt;- head(media[order(media$Freq,decreasing = T),],10) media%&gt;% kableExtra::kable(escape = T) %&gt;% kableExtra::kable_paper(c(&quot;hover&quot;), full_width = F) Media Freq 7 Twitter 3798 3 GiffGaff 204 1 Facebook 177 6 O2 UK 59 9 YouTube 20 5 Instagram 4 8 Vodafone UK 3 2 Flickr 1 4 Google+ 1 # Plot ggplot(media,aes(fill=Media)) + geom_bar(stat = &quot;identity&quot;,aes(reorder(Media,Freq),Freq)) + coord_flip() + theme(legend.position = &quot;none&quot;)+ scale_y_log10()+ labs(x=&quot;Media&quot;, y=&quot;Count&quot;,title = &quot;Top 10 Most Used Media&quot;)+ theme_bw() The most used media is Twitter, followed by GiffGaff and Facebook. media_perc&lt;-as.data.frame(prop.table(table(data$`PUBLISHER-NAME`))) media_perc$Freq &lt;- round(media_perc$Freq*100,4) head(media_perc[order(media_perc$Freq,decreasing = T),],10) %&gt;% rename(Author=Var1,Percentage=Freq)%&gt;% kable(escape = T) %&gt;% kable_paper(c(&quot;hover&quot;), full_width = F) Author Percentage 7 Twitter 89.0087 3 GiffGaff 4.7809 1 Facebook 4.1481 6 O2 UK 1.3827 9 YouTube 0.4687 5 Instagram 0.0937 8 Vodafone UK 0.0703 2 Flickr 0.0234 4 Google+ 0.0234 Twitter was used in almost 90% of cases, while GiffGaff and Facebook in a bit below 5% each. 4.4 What is the percentage of positive, negative and neutral comments? as.data.frame(prop.table(table(data$SENTIMENT))*100) %&gt;% rename(Sentiment=Var1,Percentage=Freq) %&gt;% arrange(desc(Percentage))%&gt;% kable(escape = T) %&gt;% kable_paper(c(&quot;hover&quot;), full_width = F) Sentiment Percentage 0 76.7518163 -1 11.5537849 1 10.7804078 2 0.5390204 -2 0.3749707 Based on the analysis, around 77% of comments are neutral, 12% slightly negative and 11% slightly positive. Percentage of extremely positive or extremely negative comments is in total around 0.8%. 4.5 What is the average sentiment in Twitter? mean(data$SENTIMENT) ## [1] -0.004452777 4.6 Visalisation task Make a scatter plot of the database using 3 variables. Two of them are provided here: Media: Twitter, Facebook and Instagram. Visibility: total of comments and average sentiment. By combining information about publishers (Twitter, Facebook and Instagram), date of publishing, sentiment and average sentiment we are able to create a multiple line plot to explain sentiment in each publisher in the given period of a day. First we filtered data to retain publishers such as Twitter, Facebook, Instagram, GiffGaff and O2 UK. Subsequently, we pivot the table so that the final sheet look like this (only first 6 rows): plot&lt;-subset(data,`PUBLISHER-NAME`==&quot;Twitter&quot; | `PUBLISHER-NAME`==&quot;Facebook&quot; | `PUBLISHER-NAME`==&quot;Instagram&quot; | `PUBLISHER-NAME`==&quot;GiffGaff&quot; | `PUBLISHER-NAME`==&quot;O2 UK&quot;) %&gt;% rename(Publisher=`PUBLISHER-NAME`,Sentiment=SENTIMENT) %&gt;% group_by(Publisher,PUBDATE) %&gt;% mutate(Date=PUBDATE, Publisher=as.factor(Publisher))%&gt;% summarise(Sentiment=mean(Sentiment)) plot%&gt;% head()%&gt;% kable(escape = T) %&gt;% kable_paper(c(&quot;hover&quot;), full_width = F) Publisher PUBDATE Sentiment Facebook 2015-05-27 17:30:00 0 Facebook 2015-05-27 17:31:11 1 Facebook 2015-05-27 17:31:43 0 Facebook 2015-05-27 17:38:51 -1 Facebook 2015-05-27 17:45:00 -1 Facebook 2015-05-27 17:45:48 -1 In the first column are publishers we retained. The second column is the exact date and time of publishing the comment. Finally, the last column denotes the sentiment score associated with each comment. We are in a position to vizualise sentiment scores across platforms in the given observation time. plot %&gt;% filter(Publisher==&quot;Facebook&quot;|Publisher==&quot;Instagram&quot;|Publisher==&quot;Twitter&quot;)%&gt;% mutate(avg_sentiment=mean(Sentiment))%&gt;% ggplot(aes(x = PUBDATE, y = Sentiment)) + geom_line(aes(color = Publisher), size = 0.5) + geom_hline(yintercept = plot$avg_sentiment)+ labs(title = &quot;How is sentiment across platform?&quot;,x=&quot;&quot;,subtitle =&quot;Number of comments: FB=177; IG=4; TW=3798&quot;)+ facet_grid(Publisher~.)+ theme_bw() We could see that Twitter is the most balanced publisher, as the sentiment scores are constant in the time observed. Moreover, Twitter has the biggest count of comments, 3798. Some good news were published on May 27 after 18 PM as the sentiment scores for Twitter, Facebook in this period were extremely positive. Instagram doesnt seem to be a channel for the company as there are bearly any comments. "],["customer-churn-prediction.html", "5 Customer churn prediction 5.1 Why predicting customer churn? 5.2 Data 5.3 Data Pre-processing 5.4 Data partition 5.5 Random Forest model 5.6 Evaluating Model 5.7 References", " 5 Customer churn prediction Figure 5.1: Foto from technologyadvice.com In this project I decided to use Telco dataset, a dataset from the IBM Watson Analytics community, and apply Random Forest learning method to make prediction whether a customer will churn or not based on features and information about customers who churned within the last month or are still customers. 5.1 Why predicting customer churn? Consumer churn prediction models are developed to determine which consumers are likely to churn and to encourage an effective segmentation of the customer base to enable companies to approach customers at risk of leaving with a retention strategy. Such a prediction models help small marketing budgets to be wisely utilized to minimize turnover, i.e. to maximize the return on marketing expenditure (ROMI). Customer retention has usually been found to be extremely profitable for companies because it costs five to six times more to acquire new customer than to retain an existing customer. Additionally, long-term customers are more profitable, appear to be less susceptible to aggressive marketing activities, appear to be less costly to service, and can create new referrals by positive word-of - mouth. Consequently, even a small improvement in customer retention may yield significant returns. Note: for references take a look at the Reference section options(stringsAsFactor=TRUE) library(curl) library(readr) library(tidyr) library(dplyr) library(randomForest) library(caret) 5.2 Data The data set includes information about: Customers who left within the last month  the column is called Churn. Services that each customer has signed up for  phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies. Customer account information  how long theyve been a customer, contract, payment method, paperless billing, monthly charges, and total charges. Demographic info about customers  gender, age range, and if they have partners and dependents. Variable Topic Gender Whether the customer is a male or a female SeniorCitizen Whether the customer is a senior citizen or not (1, 0) Partner Whether the customer has a partner or not (Yes, No) Dependents Whether the customer has dependents or not (Yes, No) Tenure Number of months the customer has stayed with the company PhoneService Whether the customer has a phone service or not (Yes, No) MultipleLines Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService Customers internet service provider (DSL, Fiber optic, No) OnlineSecurity Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection Whether the customer has device protection or not (Yes, No, No internet service) TechSupport Whether the customer has tech support or not (Yes, No, No internet service) StreamingTV Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovie Whether the customer has streaming movies or not (Yes, No, No internet service) Contract The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling Whether the customer has paperless billing or not (Yes, No) PaymentMethod The customers payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges The amount charged to the customer monthly TotalCharges The total amount charged to the customer Churn Whether the customer churned or not (Yes or No) Note: Information collected from https://www.kaggle.com/blastchar/telco-customer-churn Here are dimensions of our data set and first 6 rows: ## customerID gender SeniorCitizen Partner Dependents tenure PhoneService ## 1 7590-VHVEG Female 0 Yes No 1 No ## 2 5575-GNVDE Male 0 No No 34 Yes ## 3 3668-QPYBK Male 0 No No 2 Yes ## 4 7795-CFOCW Male 0 No No 45 No ## 5 9237-HQITU Female 0 No No 2 Yes ## 6 9305-CDSKC Female 0 No No 8 Yes ## MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection ## 1 No phone service DSL No Yes No ## 2 No DSL Yes No Yes ## 3 No DSL Yes Yes No ## 4 No phone service DSL Yes No Yes ## 5 No Fiber optic No No No ## 6 Yes Fiber optic No No Yes ## TechSupport StreamingTV StreamingMovies Contract PaperlessBilling ## 1 No No No Month-to-month Yes ## 2 No No No One year No ## 3 No No No Month-to-month Yes ## 4 Yes No No One year No ## 5 No No No Month-to-month Yes ## 6 No Yes Yes Month-to-month Yes ## PaymentMethod MonthlyCharges TotalCharges Churn ## 1 Electronic check 29.85 29.85 No ## 2 Mailed check 56.95 1889.50 No ## 3 Mailed check 53.85 108.15 Yes ## 4 Bank transfer (automatic) 42.30 1840.75 No ## 5 Electronic check 70.70 151.65 Yes ## 6 Electronic check 99.65 820.50 Yes 5.3 Data Pre-processing First, we need to remove rows which have at least one NA as a value: # Drop Nas dim(x) ## [1] 7043 21 x &lt;- x %&gt;% drop_na() dim(x) ## [1] 7032 21 11 are removed from the data set as they held at least one NA as a value. Next, we need to prepare our data set in terms of data types. In order to use Random Forest for churn prediction, we need to make sure that our categorical are represented in numeric manner. Therefore, all categorical variables will be converted to factors. # Categorical variables to factors x[c(&quot;Partner&quot;,&quot;Dependents&quot;,&quot;PhoneService&quot;,&quot;gender&quot;,&quot;MultipleLines&quot;,&quot;InternetService&quot;,&quot;OnlineSecurity&quot;,&quot;OnlineBackup&quot;,&quot;DeviceProtection&quot;,&quot;TechSupport&quot;,&quot;StreamingTV&quot;,&quot;Contract&quot;,&quot;StreamingMovies&quot;,&quot;PaperlessBilling&quot;,&quot;PaymentMethod&quot;,&quot;Churn&quot;)]&lt;-lapply(x[c(&quot;Partner&quot;,&quot;Dependents&quot;,&quot;PhoneService&quot;,&quot;gender&quot;,&quot;MultipleLines&quot;,&quot;InternetService&quot;,&quot;OnlineSecurity&quot;,&quot;OnlineBackup&quot;,&quot;DeviceProtection&quot;,&quot;TechSupport&quot;,&quot;StreamingTV&quot;,&quot;Contract&quot;,&quot;StreamingMovies&quot;,&quot;PaperlessBilling&quot;,&quot;PaymentMethod&quot;,&quot;Churn&quot;)], as.factor) str(x) ## &#39;data.frame&#39;: 7032 obs. of 21 variables: ## $ customerID : chr &quot;7590-VHVEG&quot; &quot;5575-GNVDE&quot; &quot;3668-QPYBK&quot; &quot;7795-CFOCW&quot; ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 1 1 2 1 1 2 ... ## $ SeniorCitizen : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Partner : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 1 1 1 1 1 2 1 ... ## $ Dependents : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 2 1 1 2 ... ## $ tenure : int 1 34 2 45 2 8 22 10 28 62 ... ## $ PhoneService : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 2 1 2 2 2 1 2 2 ... ## $ MultipleLines : Factor w/ 3 levels &quot;No&quot;,&quot;No phone service&quot;,..: 2 1 1 2 1 3 3 2 3 1 ... ## $ InternetService : Factor w/ 3 levels &quot;DSL&quot;,&quot;Fiber optic&quot;,..: 1 1 1 1 2 2 2 1 2 1 ... ## $ OnlineSecurity : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 3 3 3 1 1 1 3 1 3 ... ## $ OnlineBackup : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 3 1 3 1 1 1 3 1 1 3 ... ## $ DeviceProtection: Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 3 1 3 1 3 1 1 3 1 ... ## $ TechSupport : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 3 1 1 1 1 3 1 ... ## $ StreamingTV : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 1 1 3 3 1 3 1 ... ## $ StreamingMovies : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 1 1 3 1 1 3 1 ... ## $ Contract : Factor w/ 3 levels &quot;Month-to-month&quot;,..: 1 2 1 2 1 1 1 1 1 2 ... ## $ PaperlessBilling: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 2 1 2 2 2 1 2 1 ... ## $ PaymentMethod : Factor w/ 4 levels &quot;Bank transfer (automatic)&quot;,..: 3 4 4 1 3 3 2 4 3 1 ... ## $ MonthlyCharges : num 29.9 57 53.9 42.3 70.7 ... ## $ TotalCharges : num 29.9 1889.5 108.2 1840.8 151.7 ... ## $ Churn : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 2 1 2 2 1 1 2 1 ... We aim to convert our data set into matrix with only 0s and 1s. Thus, all binomial variables can be turned to 0s and 1s in the following way: # Binomial categorical variables to 0 and 1 x$PhoneService &lt;- as.numeric(x$PhoneService)-1 x$Partner &lt;-as.numeric(x$Partner)-1 x$Dependents &lt;- as.numeric(x$Dependents)-1 x$Churn &lt;- as.numeric(x$Churn)-1 x$gender &lt;- as.numeric(x$gender)-1 Now it is left to convert multi-class variables 0s and 1s. Before we do it, lets inspect our data a bit: ## Categorical variables with more than 2 categories par(mfrow=c(5,2)) # Multiple lines ggplot(x, aes(MultipleLines,fill=MultipleLines)) + geom_bar() + labs(title=&quot;Multiple lines&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # Internet services ggplot(x, aes(InternetService,fill=InternetService)) + geom_bar() + labs(title=&quot;Internet service&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # OnlineSecurity ggplot(x, aes(OnlineSecurity,fill=OnlineSecurity)) + geom_bar() + labs(title=&quot;OnlineSecurity&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # OnlineBackup ggplot(x, aes(OnlineBackup,fill=OnlineBackup)) + geom_bar() + labs(title=&quot;OnlineBackup&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # DeviceProtection ggplot(x, aes(DeviceProtection,fill=DeviceProtection)) + geom_bar() + labs(title=&quot;DeviceProtection&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # TechSupport ggplot(x, aes(TechSupport,fill=TechSupport)) + geom_bar() + labs(title=&quot;TechSupport&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # StreamingTV ggplot(x, aes(StreamingTV,fill=StreamingTV)) + geom_bar() + labs(title=&quot;StreamingTV&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # StreamingMovies ggplot(x, aes(StreamingMovies,fill=StreamingMovies)) + geom_bar() + labs(title=&quot;StreamingMovies&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # Contract ggplot(x, aes(Contract,fill=Contract)) + geom_bar() + labs(title=&quot;Contract&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # PaymentMethod ggplot(x, aes(PaymentMethod,fill=PaymentMethod)) + geom_bar() + labs(title=&quot;PaymentMethod&quot;,x=&quot;&quot;,y=&quot;Count&quot;) We can apply one hot encoding to our data set by using Rs base function model.matrix. In the code below, ~.+0 leads to encoding of all categorical variables without producing an intercept. # One-hot encoding x.mat&lt;- model.matrix(~MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+Contract+StreamingMovies+PaperlessBilling+PaymentMethod+0,data = x) Now is our data set pre-processed: # Creation of the final data frame with 0s and 1s final.df&lt;-as.data.frame(x.mat) final.df&lt;- cbind(x.mat,x$tenure,x$TotalCharges,x$MonthlyCharges,x$PhoneService,x$Partner,x$Dependents,x$gender,x$Churn) colnames(final.df)[24:31]&lt;-c(&quot;tenure&quot;,&quot;TotalCharges&quot;,&quot;MonthlyCharges&quot;,&quot;PhoneService&quot;,&quot;Partner&quot;,&quot;Dependents&quot;,&quot;gender&quot;,&quot;Churn&quot;) 5.4 Data partition Now we need to split our data into test and train data. The proportion is 70:30. # Creation of training and test data sets index &lt;- caret::createDataPartition(x$Churn, p = 0.7, list = F) train &lt;- final.df[index,] test &lt;- final.df[-index,] # Partitioning test data x_test &lt;- as.matrix(test[,-31]) y_test &lt;- as.matrix(test[,31]) # Partitioning train data x_train &lt;- as.matrix(train[,-31]) y_train &lt;- as.matrix(train[,31]) To train our random forest model we use function randomForest(). At this point we will not aim to fine tune our model, so we will define just two parameters,ntree and maxnodes: ntree defines the number of trees to build in the forest. maxnodes defines the maximum number of terminal nodes each tree in the forest can have. 5.5 Random Forest model # Random Forest # Training library(randomForest) rfModel &lt;- randomForest(x=x_train, y=factor(y_train), ntree=500, maxnodes=24) Since predictions are made based on features our model was trained on, it is possible to observe importance of each feature. The more important a feature, the greater influence it exerts on predictions: importance_features &lt;- randomForest::importance(rfModel) importance_features &lt;- as.data.frame(importance_features) importance_features$features &lt;- row.names(importance_features) importance_features &lt;- importance_features[order(importance_features$MeanDecreaseGini ,decreasing = TRUE),] library(plotly) p&lt;-ggplot(importance_features) + geom_point(aes(reorder(features,MeanDecreaseGini),MeanDecreaseGini),stat = &quot;identity&quot;)+ theme_minimal()+ coord_flip()+ labs(title=&quot;Important features&quot;,x=&quot;Features&quot;) ggplotly(p) As we can see from this output, the tenure feature seems to be the most important factor in making the final prediction. Factors such as InternetServiceFiber optic,TotalCharges and ContractTwo year come subsequently. 5.6 Evaluating Model In order to evaluate our model we will take a look at accuracy, precision and recall. # Evaluating Models prediction_insample &lt;- as.double(predict(rfModel, x_train)) - 1 prediction_outsample &lt;- as.double(predict(rfModel, x_test)) - 1 Accuracy is the percentage of correct predictions out of all predictions. # Accuracy accu_insample &lt;- mean(y_train == prediction_insample) accu_outsample &lt;- mean(y_test == prediction_outsample) print(sprintf(&#39;In-Sample Accuracy: %0.4f&#39;, accu_insample)) ## [1] &quot;In-Sample Accuracy: 0.8005&quot; print(sprintf(&#39;Out-Sample Accuracy: %0.4f&#39;, accu_outsample)) ## [1] &quot;Out-Sample Accuracy: 0.7956&quot; We managed to achieve pretty good out-sample accuracy even without thorough fine-tuning our parameters. Precision is the number of true positives divided by the total number of true positives and false positives. # Precision prec_insample &lt;- sum(prediction_insample &amp; y_train) / sum(prediction_insample) prec_outsample &lt;- sum(prediction_outsample &amp; y_test) / sum(prediction_outsample) print(sprintf(&#39;In-Sample Precision: %0.4f&#39;, prec_insample)) ## [1] &quot;In-Sample Precision: 0.7059&quot; print(sprintf(&#39;Out-Sample Precision: %0.4f&#39;, prec_outsample)) ## [1] &quot;Out-Sample Precision: 0.6452&quot; Recall is defined as the number of true positives divided by number of true positives plus false negatives. # Recall recall_insample &lt;- sum(prediction_insample &amp; y_train) / sum(y_train) recall_outsample &lt;- sum(prediction_outsample &amp; y_test) / sum(y_test) print(sprintf(&#39;In-Sample Recall: %0.4f&#39;, recall_insample)) ## [1] &quot;In-Sample Recall: 0.4571&quot; print(sprintf(&#39;Out-Sample Recall: %0.4f&#39;, recall_outsample)) ## [1] &quot;Out-Sample Recall: 0.4151&quot; Finally, in order to estimate how good our model is in comparison to the random prediction, we will inspect ROC curve and the Area Under the Curve. library(ROCR) pred_prob_insample &lt;- as.double(predict(rfModel, x_train, type=&#39;prob&#39;)[,2]) pred_prob_outsample &lt;- as.double(predict(rfModel, x_test, type=&#39;prob&#39;)[,2]) pred &lt;- prediction(pred_prob_outsample, y_test) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure=&#39;auc&#39;)@y.values[[1]] {plot(perf,main=sprintf(&#39;Random Forest (AUC: %0.2f)&#39;, auc),col=&#39;darkblue&#39;,lwd=2) + grid() abline(a = 0, b = 1, col=&#39;darkgray&#39;, lty=4, lwd=2)} 5.7 References Hwang, Y. H. (2019). Hands-on data science for marketing: Improve your marketing strategies with machine learning using Python and R. Birmingham: Packt Publishing. W. Verbeke, D. Martens, C. Mues, B. Baesens. Building comprehensible customer churn prediction models with advanced rule induction techniques Expert Syst. Appl., 38 (3) (2011), pp. 2354-2364. M.R. Colgate, P.J. Danaher. Implementing a customer relationship strategy: the asymmetric impact of poor versus excellent execution J. Acad. Mark. Sci., 28 (3) (2000), pp. 375-387. J. Ganesh, M.J. Arnold, K.E. Reynolds. Understanding the customer base of service providers: an examination of the differences between switchers and stayers.J. Mark., 64 (3) (2000), pp. 65-87. D.V. den Poel, B. Larivière. Customer attrition analysis for financial services using proportional hazard models. Eur. J. Oper. Res., 157 (1) (2004), pp. 196-217. "],["prediction-with-rfm.html", "6 Prediction with RFM 6.1 The Problem 6.2 A Possible Solution 6.3 Background Information 6.4 Recency - Frequency - Monetary Analysis 6.5 Assignment 6.6 Visaul exploration 6.7 Response rate in training data 6.8 Prediction of response rate - single cluster 6.9 Prediction of response rate - 3 clusters 6.10 Reference", " 6 Prediction with RFM Figure 6.1: Foto from pexels.com The Charles Book Club case was derived, with the assistance of Ms. Vinni Bhandari, from The Bookbinders Club, a Case Study in Database Marketing, prepared by Nissan Levin and Jacob Zahavi, Tel Aviv University The Charles Book Club (CBC) was established in December 1986 on the premise that a book club could differentiate itself through a deep understanding of its customer base and by delivering uniquely tailored offerings. CBC focused on selling specialty books by direct marketing through a variety of channels, including media advertising (TV, magazines, newspapers) and mailing. CBC is strictly a distributor and does not publish any of the books that it sells. In line with its commitment to understanding its customer base, CBC built and maintained a detailed database about its club members. Upon enrollment, readers were required to fill out an insert and mail it to CBC. Through this process, CBC created an active database of 500,000 readers; most were acquired through advertising in specialty magazines. Historically, book clubs offered their readers different types of membership programs. Two common membership programs are the continuity and negative option programs, which are both extended contractual relationships between the club and its members. Under a continuity program, a reader signs up by accepting an offer of several books for just a few dollars (plus shipping and handling) and an agreement to receive a shipment of one or two books each month thereafter at more-standard pricing. The continuity program is most common in the childrens book market, where parents are willing to delegate the rights to the book club to make a selection, and much of the clubs prestige depends on the quality of its selections. In a negative option program, readers get to select how many and which additional books they would like to receive. However, the clubs selection of the month is delivered to them automatically unless they specifically mark no on their order form by a deadline date. Negative option programs sometimes result in customer dissatisfaction and always give rise to significant mailing and processing costs. In an attempt to combat these trends, some book clubs have begun to offer books on a positive option basis, but only to specific segments of their customer base that are likely to be receptive to specific offers. Rather than expanding the volume and coverage of mailings, some book clubs are beginning to use database-marketing techniques to target customers more accurately. Information contained in their databases is used to identify who is most likely to be interested in a specific offer. This information enables clubs to design special programs carefully tailored to meet their customer segments varying needs. 6.1 The Problem CBC sent mailings to its club members each month containing the latest offerings. On the surface, CBC appeared very successful: mailing volume was increasing, book selection was diversifying and growing,and their customer database was increasing. However, their bottom-line profits were falling. The decreasing profits led CBC to revisit their original plan of using database marketing to improve mailing yields and to stay profitable. 6.2 A Possible Solution CBC embraced the idea of deriving intelligence from their data to allow them to know their customers better and enable multiple targeted campaigns where each target audience would receive appropriate mailings. CBCs management decided to focus its efforts on the most profitable customers and prospects, and to design targeted marketing strategies to best reach them. The two processes they had in place were: 1. Customer acquisition: New members would be acquired by advertising in specialty magazines, newspapers, and on TV. Direct mailing and telemarketing would contact existing club members. Every new book would be offered to club members before general advertising. 2. Data collection: All customer responses would be recorded and maintained in the database. Any information not being collected that is critical would be requested from the customer. For each new title, they decided to use a two-step approach: Conduct a market test involving a random sample of 4000 customers from the database to enable analysis of customer responses. The analysis would create and calibrate response models for the current book offering. Based on the response models, compute a score for each customer in the database. Use this score and a cutoff value to extract a target customer list for direct-mail promotion. Targeting promotions was considered to be of prime importance. Other opportunities to create successful marketing campaigns based on customer behavior data (returns, inactivity, complaints, compliments, etc.) would be addressed by CBC at a later stage. 6.3 Background Information A new title, The Art History of Florence, is ready for release. CBC sent a test mailing to a random sample of 4000 customers from its customer base. The customer responses have been collated with past purchase data. The dataset was randomly partitioned into three parts: Training Data (1800 customers): initial data to be used to fit models, Validation Data (1400 customers): holdout data used to compare the performance of different models, and Test Data (800 customers): data to be used only after a final model has been selected to estimate the probable performance of the model when it is deployed. Each row (or case) in the spreadsheet (other than the header) corresponds to one market test customer. Each column is a variable, with the header row giving the name of the variable. 6.4 Recency - Frequency - Monetary Analysis The segmentation process in database marketing aims to partition customers in a list of prospects into homogeneous groups (segments) that are similar with respect to buying behavior. The homogeneity criterion we need for segmentation is the propensity to purchase the offering. However, since we cannot measure this attribute, we use variables that are plausible indicators of this propensity. In the direct marketing business, the most commonly used variables are the RFM variables: R = recency, time since last purchase F = frequency, number of previous purchases from the company over a period M = monetary, amount of money spent on the companys products over a period The assumption is that the more recent the last purchase, the more products bought from the company in the past, and the more money spent in the past buying the companys products, the more likely the customer is to purchase the product offered. The 1800 observations in the dataset were divided into recency, frequency, and monetary categories as follows: Recency: Recency Recode 02 months Rcode = 1 36 months Rcode = 2 712 months Rcode = 3 13 months and up Rcode = 4 Frequency: Frequency Recode 1 book Fcode = 1 2 books Fcode = 2 3 books Fcode = 3 Monetary: Monetary Recode 0  25 Mcode = 1 2650 Mcode = 2 51100 Mcode = 3 101200 Mcode = 4 201 and up Mcode = 5 Note: Montary values are denoted in USD 6.5 Assignment Partition the data into training (60%) and validation (40%). Use seed = 1. What is the response rate for the training data customers taken as a whole? What is the response rate for each of the 4×5×3 = 60 combinations of RFM categories? Which combinations have response rates in the training data that are above the overall response in the training data? Before we answer the question, let us explore the data set: ## Rows: 4,000 ## Columns: 24 ## $ Seq. &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,... ## $ ID. &lt;int&gt; 25, 29, 46, 47, 51, 60, 61, 79, 81, 90, 95, 100, 1... ## $ Gender &lt;int&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,... ## $ M &lt;int&gt; 297, 128, 138, 228, 257, 145, 190, 187, 252, 240, ... ## $ R &lt;int&gt; 14, 8, 22, 2, 10, 6, 16, 14, 10, 6, 2, 2, 4, 14, 4... ## $ F &lt;int&gt; 2, 2, 7, 1, 1, 2, 1, 1, 1, 3, 4, 3, 1, 1, 2, 9, 6,... ## $ FirstPurch &lt;int&gt; 22, 10, 56, 2, 10, 12, 16, 14, 10, 20, 20, 18, 4, ... ## $ ChildBks &lt;int&gt; 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2,... ## $ YouthBks &lt;int&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,... ## $ CookBks &lt;int&gt; 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 3, 2,... ## $ DoItYBks &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ RefBks &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,... ## $ ArtBks &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,... ## $ GeogBks &lt;int&gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,... ## $ ItalCook &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,... ## $ ItalAtlas &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,... ## $ ItalArt &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,... ## $ Florence &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,... ## $ Related.Purchase &lt;int&gt; 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 6, 2,... ## $ Mcode &lt;int&gt; 5, 4, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5,... ## $ Rcode &lt;int&gt; 4, 3, 4, 1, 3, 2, 4, 4, 3, 2, 1, 1, 2, 4, 2, 4, 4,... ## $ Fcode &lt;int&gt; 2, 2, 3, 1, 1, 2, 1, 1, 1, 3, 3, 3, 1, 1, 2, 3, 3,... ## $ Yes_Florence &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,... ## $ No_Florence &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,... First, we will convert the response variable to factor class. data$Yes_Florence &lt;- factor(data$Yes_Florence, labels = c(&quot;No&quot;,&quot;Yes&quot;),levels = c(0:1)) data$No_Florence &lt;-factor(data$No_Florence, labels = c(&quot;No&quot;,&quot;Yes&quot;),levels = c(0:1)) Second, we would need to compute RFM score by merging three scores into one cell. # Calculating RFM score data$RFM_score&lt;- paste(data$Rcode,data$Fcode,data$Mcode) data$RFM_score &lt;- gsub(&quot; &quot;,&quot;&quot;,data$RFM_score) data$RFM_score&lt;-as.factor(data$RFM_score) data[1:10,&quot;RFM_score&quot;] ## [1] 425 324 434 115 315 224 414 414 315 235 ## 51 Levels: 111 112 113 114 115 122 123 124 125 132 133 134 135 211 212 ... 435 Now we will proceed with data partition. For that we will use sample() function, where we indicate rownames and the number of randomly selected row numbers we want to separate from the remaining data set. In our case, out of 4000 rows (Customers), we will randomly assign 2400 to train data set, and the rest to test data. # Train data set.seed(1) trainIndex &lt;- caret::createDataPartition(data$Florence, p = .6, list = FALSE, times = 1) train_data &lt;- data[ trainIndex,] The remaining part of the data set will be assigned to the validation data set. # Validation set validation_data &lt;- data[-trainIndex,] 6.6 Visaul exploration It would be beneficial to inspect relationship between recency, frequency and monetary value in the whole data set before we continue. A quite convenient way to do it is a heatmap. There we can plot all three variables at the same time and inspect the monetary value (= amount spend in the time frame observed) of each customer based on his/her frequency and recency. Unsuprisingly, customers who purchased more frequently generated more revenue compared to those who visited less frequently. However, customers who spent the most are not the most recent ones (= last purchase in 0-2): the heaviest spenders are the frequent ones, but they havent made a purchase 3 to 6 months. Usually, the customers who visited in the recent past (0-2 months) are more likely to return compared to those who made a purchase some time ago as most of those could potentially be lost customers. As such, higher revenue would be associated with most recent visits, but we see that is not really the case here. This could be related to the nature of books as products and the fact that they are not frequently purchased items such as daily products for instance. Nevertheless, it would definitively be worth to consider giving incentives to these customers who spent the most, but the company has not heard of them for 3-6 months. A legit question for better understanding of our target group would be about the difference in recency of customers who responded to advertising of Florence. data %&gt;% rename(Revenue=M, Response=Yes_Florence) %&gt;% ggplot(aes(x=Response,y=F,fill=Response))+ geom_boxplot()+ labs(x=&quot;Response to Campaign&quot;,y=&quot;Frequency (Number of Purchases)&quot;,title = &quot;How Frequent Are Customers Interested in &#39;Florence&#39;?&quot;, subtitle = &quot;Boxplots depicting frequency of respondents and non-respondents&quot;)+ stat_summary(fun.y=mean, geom=&quot;point&quot;, shape=20, size=10, color=&quot;red&quot;, fill=&quot;red&quot;) + theme_bw() There seem to be difference in means of the two groups. Non-respondents are somewhat less frequent customers, while respondents belong to more frequent customers. Let us see about their recency. Customers who responded to the Florence campaign have on average lower recency than customers who did not respond. All in all, we can conclude that customers who responded to the campaign are, on average, slightly more frequent and recent than customers who did not respond to campaign. 6.7 Response rate in training data Now we can start with addressing the first question: what is the response rate for training data customers taken as whole? Let us now inspect the response rate for training data customers. Response rate of customers from the training data is around 9%. ## [1] 0.08708333 6.8 Prediction of response rate - single cluster Now we would need to inspect response rates from all 60 (possible) combinations, and compare them with the overall one. ## # A tibble: 51 x 2 ## RFM_score Response_rate ## &lt;fct&gt; &lt;dbl&gt; ## 1 111 0 ## 2 112 0 ## 3 113 0 ## 4 114 0.25 ## 5 115 0.115 ## 6 122 0.5 ## 7 123 0 ## 8 124 0 ## 9 125 0.118 ## 10 132 1 ## # ... with 41 more rows The following RFM scores indicate response rate higher than 9%: ## # A tibble: 19 x 3 ## RFM_score Response_rate n ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 114 0.25 16 ## 2 115 0.115 26 ## 3 122 0.5 2 ## 4 125 0.118 17 ## 5 132 1 1 ## 6 135 0.170 53 ## 7 211 0.333 3 ## 8 212 0.333 9 ## 9 213 0.136 22 ## 10 215 0.179 28 ## 11 223 0.167 24 ## 12 224 0.114 35 ## 13 225 0.114 44 ## 14 233 0.333 6 ## 15 234 0.146 41 ## 16 235 0.151 93 ## 17 323 0.1 40 ## 18 335 0.153 215 ## 19 433 0.118 17 Suppose that we decide to send promotional mail only to the above average RFM combinations identified in part. Now we should compute the response rate in the validation data using these above average combinations. Predicted response rate from the training data set is said to be at around 15.1%, while the true response rate taken from validation set is at 10.3%. From comparison of true response rate in the validation data set and the predicted response rate from the training data set it seems that the latter pretty much deviates from the former. In the next section we will split customers from the training set into 3 clusters, and again compare predicted with the true response rate. 6.9 Prediction of response rate - 3 clusters Let us now segment our customers in 3 different segments: Segment 1: RFM combinations that have response rates that exceed twice the overall response rate Segment 2: RFM combinations that exceed the overall response rate but do not exceed twice that rate Segment 3: the remaining RFM combinations ## # A tibble: 51 x 3 ## RFM_score Response_rate Cluster ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 111 0 RR_Below ## 2 112 0 RR_Below ## 3 113 0 RR_Below ## 4 114 0.25 RR_Twice ## 5 115 0.115 RR_Above ## 6 122 0.5 RR_Twice ## 7 123 0 RR_Below ## 8 124 0 RR_Below ## 9 125 0.118 RR_Above ## 10 132 1 RR_Twice ## # ... with 41 more rows We classified RFM scores based on response rate into 3 segments. Now we will identify customers with those RFM scores in the validation set, and compare predicted/expected response rate from the training data set with the actual response rate. By visual inspection we could see that predicted response rates are above the true ones in two out of 3 segments. Customers who had response rate at least twice the initial response rate (9%) were expected to have around 25% response rate, but the true response rate is at significantly lower 9.5% response rate. Similarly, the RFM prediction in case of customers who had response rate between 9 and 18% was slightly inaccurate as well(predicted 14.1 % vs 10.4% true response rate). The only case where true response rate exceeded the predicted response rate is the segment with the response rate below 9%. However, in order to find out how well our model performed, we will create a gain chart. It helps us determine how effectively we can proceed with our campaign. We aim at selecting a relatively small number of customers and getting a relatively large portion of respondents. For a given number of customers expressed in percentages, the gain curve value on the y-axis will shows us how much better we are doing compared to random choice of customers. Based on the gain chart, prediction based on our RFM model performs a bit better than baseline, i.e. random guessing. More specifically, if we select top 20% cases based on our model, we would attract 29% of the target customers, i.e. customers who would respond to our marketing campaign. As the graph is interactive, you are able to check any other percentage you wish. Although we managed to create a model that performs better than just guessing, in the further analyses we will try out other approaches, such as logistic regression, that may provide us better results. 6.10 Reference Shmueli, G., Bruce, P. C., Yahav, I., Patel, N. R., &amp; Lichtendahl, K. C. (2018). Data mining for business analytics: Concepts, techniques, and applications in R. "],["recommendation-system-for-beauty-products.html", "7 Recommendation system for beauty products 7.1 Data collection 7.2 Data preparation and preprocessing 7.3 Exploratory data analysis 7.4 Building a model 7.5 Interpretation and managerial implications 7.6 Bonus analysis: Text Mining 7.7 Future work 7.8 Limitations", " 7 Recommendation system for beauty products Figure 7.1: Foto von Andrea Piacquadio von Pexels Based on some studies it has been proven that personalized product recommendations drive 24% of the orders and 26% of the revenue. This explains the influence recommendation has on volume of orders and generally on sales figures. What is more, it has been proven that product recommendations lead to reoccurring visits and that purchases on recommendation mark higher average-order value. Consequently, we decided to use method called user-based collaborative filtering to build our recommendation system (Reference). First, we proceed with data preparation and pre-processing, then we build our recommender system, and finally draw business implications. 7.1 Data collection As we earlier mentioned, we use data on Amazon customer reviews of beauty products. The data used in this project can be accessed in this link. It contains the following features: Variable Description Product price How much a product costs. Product ID ASIN number of a product on Amazon. Product title Name of a product. Review helpfulness Fraction of users who found the review helpful. Profile name Name of the profile on Amazon. Review score Rating of the product. Review summary Concise summary of the review text. Review text Review text. Review time Review time. Review userId Review userId Note: Information collected from http://snap.stanford.edu/data/web-Amazon-links.html 7.2 Data preparation and preprocessing 7.2.1 Packages #Packages library(R.utils) library(dplyr) library(tidyr) library(janitor) library(recommenderlab) library(tm) library(NLP) library(qdap) library(readr) library(wordcloud) 7.2.2 Data collection After downloading data locally we load in data by usingreadLines() function: # Loading in data my_data &lt;- readRDS(&quot;data/amazon_beauty_full.RDS&quot;) Let us first have a look at the dimension of our data. Our data set is currently in a form of a single vector with 2772616 elements. Obviously, this is not the optimal form of the data we would like to work with. That is why we need to work around this data set to make it more convenient for further analysis. What we can do first is to remove all fields with no characters: my_data &lt;- my_data[sapply(my_data, nchar) &gt; 0] Then we can convert it to data frame: my_data &lt;- as.data.frame(my_data) colnames(my_data) &lt;- &quot;product&quot; One of the critical steps is separating the column to multiple columns: # Separate one column to two (&quot;:&quot; separator) my_data &lt;- separate(my_data,col = product, into = c(&quot;Info&quot;,&quot;Product&quot;), sep = &quot;:&quot;) Inspecting first 10 values: head(my_data,10) ## Info ## 1 product/productId ## 2 product/title ## 3 product/price ## 4 review/userId ## 5 review/profileName ## 6 review/helpfulness ## 7 review/score ## 8 review/time ## 9 review/summary ## 10 review/text ## Product ## 1 B00064C0IU ## 2 Oscar Eau de Toilette for Women by Oscar de La Renta ## 3 24.19 ## 4 A1FWT811DSZLC8 ## 5 Heidi M ## 6 0/0 ## 7 3.0 ## 8 1360368000 ## 9 doesn&#39;t last ## 10 very light scent that doesn&#39;t last very long. pretty bottle but I was hoping for more of a freesia scent. which it was not. The data set is loaded in .txt format, which makes it a bit challenging to work with. In the following sections we will undertake data manipulation in order to bring the data set in more suitable form. First, we will convert it from the current long-format to the wide-format, where each column will represent a product, and each row a feature: #Converting long format to wide my_data &lt;- my_data %&gt;% group_by(Info) %&gt;% mutate(Order = seq_along(Info)) %&gt;% spread(key = Order, value = Product) Since the column names are labeled with numbers, we will apply first row as a label for the corresponding column name: my_data &lt;- as.data.frame(t(my_data)) my_data&lt;-my_data%&gt;% row_to_names(row_number = 1) Delete rows with at least 1 NAs: my_data &lt;- my_data[rowSums(is.na(my_data))==0,] Trim white space at the beginning or ending the string: my_data$`review/userId`&lt;- trimws(my_data$`review/userId`) my_data$`product/productId`&lt;- trimws(my_data$`product/productId`) my_data$`product/price`&lt;- trimws(my_data$`product/price`) my_data$`product/title`&lt;- trimws(my_data$`product/title`) Filtering out reviews with unknown userID and productId: my_data&lt;-filter(my_data,`review/userId`!=&quot;unknown&quot; &amp; `product/productId`!=&quot;unknown&quot; &amp; `product/price`!=&quot;unknown&quot;) Correcting column classes: my_data$`product/productId` &lt;- as.factor(my_data$`product/productId`) my_data$`review/score`&lt;- as.numeric(my_data$`review/score`) my_data$`review/userId`&lt;-as.factor(my_data$`review/userId`) my_data$`product/price`&lt;-as.numeric(my_data$`product/price`) 7.2.3 How many times users reviewed products? In order to use relevant data, we would need to define the minimum number of reviews per user. Since majority of users left only one review. Therefore, we will remove all single-review users and all other users who left less then 2 reviews. Filtering out users who left 2 or more reviews: freq&lt;-as.data.frame(table(my_data$`review/userId`)) index&lt;-filter(freq, freq$Freq&gt;=1)$Var1 We are now left with 1316 users who reviewed certain beauty product at least 2 times. my_data &lt;- subset(my_data,`review/userId` %in% index) 7.3 Exploratory data analysis 7.3.1 Head of data head(my_data) ## product/price product/productId ## 1 24.19 B00064C0IU ## 2 24.19 B00064C0IU ## 3 24.19 B00064C0IU ## 4 24.19 B00064C0IU ## 5 5.99 B000K5JBZU ## 6 5.99 B000K5JBZU ## product/title ## 1 Oscar Eau de Toilette for Women by Oscar de La Renta ## 2 Oscar Eau de Toilette for Women by Oscar de La Renta ## 3 Oscar Eau de Toilette for Women by Oscar de La Renta ## 4 Oscar Eau de Toilette for Women by Oscar de La Renta ## 5 Optimum Care Anti-Breakage Therapy Moisture Replenish Cream Hairdress ## 6 Optimum Care Anti-Breakage Therapy Moisture Replenish Cream Hairdress ## review/helpfulness review/profileName review/score ## 1 0/0 Heidi M 3 ## 2 0/0 Donna Mpaulin &quot;PURPLE RAVEN&quot; 5 ## 3 0/0 M. Avila 1 ## 4 0/0 Kim M. Colt 5 ## 5 1/1 PloveJ 5 ## 6 0/0 LADY-I 5 ## review/summary ## 1 doesn&#39;t last ## 2 Smells divine. ## 3 Very disappointed! ## 4 Nice gift ## 5 TRULY MADE A DIFFERENCE! ## 6 Excellent Product for treatment of Breakage of ends ## review/text ## 1 very light scent that doesn&#39;t last very long. pretty bottle but I was hoping for more of a freesia scent. which it was not. ## 2 This is my second bottle of sheer freesia, I just love this fragrance, its a light delicate sent, beautiful, like a summer day. ## 3 This perfume is just AWFUL! Smells nothing like freesia.The gift recipient was not impressed. The worst is that it can&#39;t be returned! ## 4 This was a gift for my sister. It wowed her. She has always worn Freesia but typically can only find it in body lotion or such. This product really thrilled her, a nice perfume in her favorite scent with a beautiful package. ## 5 I have been using this product for a couple years now. I started using it because my hair had gotten so dry from all the chemical treatments and relaxers. This actualy came in the relaxer kit. I tried it and could not beleive the difference it made with one use. I could not find it in any of the stores at the time so I searched Amazon and they had it. I absolutely love it. It is the best moisturizing product I have used so far. My hair is soft with good elasticity and it is not breaking anywhere close to what it was. I am very happy with it. ## 6 I Tried this product in the past and fell in love with it. I could not find it for some time and am now very pleased that I was abe to find it again here on amazon. It smoothes the hair and faciltate ease in removing tangles. ## review/time review/userId ## 1 1360368000 A1FWT811DSZLC8 ## 2 1358467200 A1THE6V6O8ROD4 ## 3 1357084800 A176IQ7MVD3N6T ## 4 1326240000 A34BDX4JVMG23Y ## 5 1351987200 A3UWJXJI7S3T05 ## 6 1360972800 A1QAXWETH56D6J 7.3.2 How many unique products are reviewed? length(unique(my_data$`product/productId`)) ## [1] 928 There are 928 products which were reviewed. 7.3.3 How many reviewers do we have? length(unique(my_data$`review/userId`)) ## [1] 8002 There are 8002 unique reviewers/customers who reviewed products. 7.3.4 How many scores do we have? length(my_data$`review/score`) ## [1] 8581 There are 8581 ratings. 7.3.5 What is the distribution of ratings? hist(as.numeric(my_data$`review/score`),main = &quot;Histogramm of scores&quot;,xlab = &quot;Score&quot;) Products seem to be favorably rated as the distribution of scores showes that the best score is the most frequent. 7.3.6 What is the average number of reviews per user? my_data %&gt;% group_by(`review/userId`) %&gt;% summarise(Freq=n())%&gt;% select(Freq) %&gt;% summary() ## Freq ## Min. : 1.000 ## 1st Qu.: 1.000 ## Median : 1.000 ## Mean : 1.072 ## 3rd Qu.: 1.000 ## Max. :37.000 In the original data set It users left on average left a review only once. After filtering, we see that our average is at 3 reviews per user. 7.3.7 What is the average score per user? (grand.mean &lt;- my_data %&gt;% dplyr::summarise(Grand.mean=mean(`review/score`))) ## Grand.mean ## 1 4.196248 It seems that beauty products on Amazon are well received by users as the average score per user is quite high, at 4.1962475. 7.4 Building a model 7.4.1 Final data outlook Here is a glimpse in our data before we start building the recommnder: head(my_data) ## product/price product/productId ## 1 24.19 B00064C0IU ## 2 24.19 B00064C0IU ## 3 24.19 B00064C0IU ## 4 24.19 B00064C0IU ## 5 5.99 B000K5JBZU ## 6 5.99 B000K5JBZU ## product/title ## 1 Oscar Eau de Toilette for Women by Oscar de La Renta ## 2 Oscar Eau de Toilette for Women by Oscar de La Renta ## 3 Oscar Eau de Toilette for Women by Oscar de La Renta ## 4 Oscar Eau de Toilette for Women by Oscar de La Renta ## 5 Optimum Care Anti-Breakage Therapy Moisture Replenish Cream Hairdress ## 6 Optimum Care Anti-Breakage Therapy Moisture Replenish Cream Hairdress ## review/helpfulness review/profileName review/score ## 1 0/0 Heidi M 3 ## 2 0/0 Donna Mpaulin &quot;PURPLE RAVEN&quot; 5 ## 3 0/0 M. Avila 1 ## 4 0/0 Kim M. Colt 5 ## 5 1/1 PloveJ 5 ## 6 0/0 LADY-I 5 ## review/summary ## 1 doesn&#39;t last ## 2 Smells divine. ## 3 Very disappointed! ## 4 Nice gift ## 5 TRULY MADE A DIFFERENCE! ## 6 Excellent Product for treatment of Breakage of ends ## review/text ## 1 very light scent that doesn&#39;t last very long. pretty bottle but I was hoping for more of a freesia scent. which it was not. ## 2 This is my second bottle of sheer freesia, I just love this fragrance, its a light delicate sent, beautiful, like a summer day. ## 3 This perfume is just AWFUL! Smells nothing like freesia.The gift recipient was not impressed. The worst is that it can&#39;t be returned! ## 4 This was a gift for my sister. It wowed her. She has always worn Freesia but typically can only find it in body lotion or such. This product really thrilled her, a nice perfume in her favorite scent with a beautiful package. ## 5 I have been using this product for a couple years now. I started using it because my hair had gotten so dry from all the chemical treatments and relaxers. This actualy came in the relaxer kit. I tried it and could not beleive the difference it made with one use. I could not find it in any of the stores at the time so I searched Amazon and they had it. I absolutely love it. It is the best moisturizing product I have used so far. My hair is soft with good elasticity and it is not breaking anywhere close to what it was. I am very happy with it. ## 6 I Tried this product in the past and fell in love with it. I could not find it for some time and am now very pleased that I was abe to find it again here on amazon. It smoothes the hair and faciltate ease in removing tangles. ## review/time review/userId ## 1 1360368000 A1FWT811DSZLC8 ## 2 1358467200 A1THE6V6O8ROD4 ## 3 1357084800 A176IQ7MVD3N6T ## 4 1326240000 A34BDX4JVMG23Y ## 5 1351987200 A3UWJXJI7S3T05 ## 6 1360972800 A1QAXWETH56D6J 7.4.2 Subsetting data In order to model a recommender system, three variables in our case are of great importance: User ID Product ID Score / Rating Our model will be based on these three variables. Additionally, we will make use of the remaining features by utilizing some text mining techniques, but you will find more details at some later point. Now, we will make a subset of our data with 3 mentioned variables: subset_my_data &lt;- subset(my_data, select = c(`review/userId`,`product/productId`,`review/score`)) head(subset_my_data) ## review/userId product/productId review/score ## 1 A1FWT811DSZLC8 B00064C0IU 3 ## 2 A1THE6V6O8ROD4 B00064C0IU 5 ## 3 A176IQ7MVD3N6T B00064C0IU 1 ## 4 A34BDX4JVMG23Y B00064C0IU 5 ## 5 A3UWJXJI7S3T05 B000K5JBZU 5 ## 6 A1QAXWETH56D6J B000K5JBZU 5 Let us inspect the dimensions: dim(subset_my_data) ## [1] 8581 3 7.4.3 Formatting data Our data is currently in the long format, i.e. one row for one rating. However, we would want to get a matrix with ratings where the rows represent the users IDs and the columns the Product IDs. Thus, we will transform our data to so called rating matrix: ratings &lt;- as(subset_my_data, &quot;realRatingMatrix&quot;) In order to avoid high/low rating bias from users who give high (or low) ratings to all the products they reviewed, we will need to normalize our data. That would prevent certain bias in the results. ratings &lt;- normalize(ratings) 7.4.4 Inspecting real rating matrix We can plot an image of the rating matrix for the first 250 users and 250 products: image(ratings[1:250,1:250]) From the visualisation we can see that rating matrix is very sparse, i.e. that not every user did rate/review every product in our data set. We can inspect the data for the first 10 users and the first 4 products: ratings[1:10, 1:4]@data ## 10 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## B000052Z5B B000052Z5L B000052Z5M B000052Z89 ## A00275441WYR3489IKNAB . . . . ## A0353671240B3B6L8WKZB . . . . ## A0793784FP3F6ZXZDTN6 . . . . ## A10013UITIMJVI . . . . ## A1008GFLTBL76H . . . . ## A100VLYGYI6FXY . . . . ## A100W0JWG5GB6G . . . . ## A1016Z89IM29SK . . . . ## A102NKLXRT5KEM . . . . ## A102U9TVYZC0DX . . . . As we already saw in the visualisation, the data is sparse and the first 10 users did not review first 4 products visualised in the matrix above. 7.4.5 Building a recommender Finally, we will now build our recommendation system based on User-based collaborative filtering User-based collaborative filtering search for similar users and gives them recommendations based on what other users with similar rating patterns appreciated: recommender &lt;- Recommender(ratings, method=&quot;UBCF&quot;) recommender ## Recommender of type &#39;UBCF&#39; for &#39;realRatingMatrix&#39; ## learned using 8002 users. Additionally, in order to compare results of two methods, we would like to apply item-based collaborative filtering method to build another recommender system. In contrast to user-based collaborative filtering, item-based collaborative filtering looks for similarity patterns between items and recommends them to users based on the computed information. recommenderIBCF &lt;- Recommender(ratings, method=&quot;IBCF&quot;) recommenderIBCF ## Recommender of type &#39;IBCF&#39; for &#39;realRatingMatrix&#39; ## learned using 8002 users. As reported, both recommendation systems are built using 8002 users. 7.5 Interpretation and managerial implications Now we would like to interpret the output of our recommender systems. First we start with UBCF-based recommender system. current.user &lt;- 45 recommendations &lt;- predict(recommender, current.user, data = ratings, n = 5) We decided to take user number 45 and inspect 5 recommendations provided to him/her. Now we can inspect what our recommendation system provided in the end: str(recommendations) ## Formal class &#39;topNList&#39; [package &quot;recommenderlab&quot;] with 4 slots ## ..@ items :List of 1 ## .. ..$ A10N19OL0CKYDV: int [1:2] 173 772 ## ..@ ratings :List of 1 ## .. ..$ A10N19OL0CKYDV: num [1:2] 5 5 ## ..@ itemLabels: chr [1:928] &quot;B000052Z5B&quot; &quot;B000052Z5L&quot; &quot;B000052Z5M&quot; &quot;B000052Z89&quot; ... ## ..@ n : int 5 We can see that the user ID of the user number 45 is A10N19OL0CKYDV. Our system found 2 products to recommend to this user, and we can find product index (173, 772) as well as ratings that the system calculated from the ratings of the closest users (5,5). Let us create a prediction made by IBCF-based recommender: recommendationsIBCF &lt;- predict(recommenderIBCF,current.user,data = ratings, n=5) str(recommendationsIBCF) ## Formal class &#39;topNList&#39; [package &quot;recommenderlab&quot;] with 4 slots ## ..@ items :List of 1 ## .. ..$ A10N19OL0CKYDV: int(0) ## ..@ ratings :List of 1 ## .. ..$ A10N19OL0CKYDV: num(0) ## ..@ itemLabels: chr [1:928] &quot;B000052Z5B&quot; &quot;B000052Z5L&quot; &quot;B000052Z5M&quot; &quot;B000052Z89&quot; ... ## ..@ n : int 5 We will inspect potential recommended products: head(as(recommendationsIBCF,&quot;list&quot;)) ## $A10N19OL0CKYDV ## character(0) Unfortunately, our item-based collaborative filtering system did not generate any recommendation for the user number 45. 7.5.1 Identification of the recommended products Let us now identify the products recommended by UBCF-based recommender. First we need to extract the index of the recommended products: index&lt;- as.vector(as.factor(unlist(as(recommendations, &quot;list&quot;)))) Then we find corresponding product in our initial data set: (recommendation_26&lt;-my_data[match(index, my_data$`product/productId`),]) ## product/price product/productId ## 6952 4.97 B00027EG9C ## 4190 0.01 B000NCQK68 ## product/title review/helpfulness ## 6952 Neutrogena Fresh Foaming Cleanser, 6.7 Ounce 0/0 ## 4190 Revlon ColorSilk Beautiful Color Hair Coloring Products 10/11 ## review/profileName review/score review/summary ## 6952 armicat 4 I love it! ## 4190 Grace M. 4 Pretty good ## review/text ## 6952 Neutrogena Fresh Foaming Cleanser is really good! I am using everyday, I can recommend this item to my friends. But the problem is packaging was bad. ## 4190 I&#39;ve had black/blue-black hair for a while now, and have become somewhat of an expert in the field of cheap-o boxed drugstore hair color. I keep coming back to this color! I was suspicious at first to be honest because its SO cheap, I figured my hair would fall out or turn an ugly color... but I tried it anyway. I have long, super thick hair but there was enough dye to get everything - no streaks. It lasts the longest out of any boxed hair color I&#39;ve tried and it looks SO good!OK, yeah, it comes with cheap crappy gloves and the world&#39;s smallest packet of conditioner, but those are minor issues. You can get 50 pairs of latex gloves at rite-aid for 2 bucks (a good idea anyway so you dont have to keep reusing the dirty color-stained gloves from the box) and if you don&#39;t already have conditioner to supplement the conditioner in the box, you shouldn&#39;t be dying your hair anyway.The only drawback I&#39;ve experienced is the color DOES run when I wash it and I have definitely stained my bath towels/pillowcase a few times. It could be something I&#39;m doing wrong though... ## review/time review/userId ## 6952 1361232000 A3HUAIQVMNXGKL ## 4190 1308700800 A1DBAOA82KSVND Two products recommended are : Neutrogena Fresh Foaming Cleanser, 6.7 Ounce - facial cleansing cream Revlon ColorSilk Beautiful Color Hair Coloring Products - color for hair Let us now inspect products that the user A10N19OL0CKYDV rated: my_data[match(&quot;A10N19OL0CKYDV&quot;,my_data$`review/userId`),] ## product/price product/productId product/title review/helpfulness ## 1913 1.89 B000NGLR62 Opi Ridge Filler .5 oz. 0/0 ## review/profileName review/score review/summary ## 1913 Erica 4 Great Base Coat ## review/text ## 1913 I got this as a base coat, and for that purpose it works really well. It&#39;s easy to apply (the brush is easy to use) and dries to a matte, translucent white coat. Nail polish glides smoothly over it, just as I was expecting. It protects my nails from dark polishes and prevents my nails from chipping easily. I&#39;m unsure of how different it is than other regular base coats, but I really like and recommend this product. ## review/time review/userId ## 1913 1356912000 A10N19OL0CKYDV 7.5.2 Implications As we could see, this user reviewed only one product, called Opi Ridge Filler .5 oz., and it is a nail-care product. We could assume that this person is a female user since the product she bought is typically associated with female beauty care. What is more, two recommended products are as well very strongly associated to being typical female beauty products. Finally, we have the name of the user (Erica), so we can be sure that the user is a female. From the qualitative perspective it seems that our recommendation system provides descent recommendations!. 7.6 Bonus analysis: Text Mining In addition to our recommender system, we will apply some basic text mining techniques to explore reviews text. Text mining helps us to mine opinions of users (in this case) about the reviewed products at scale. 7.6.1 Wordcloud Here we create a wordcloud of words from product reviews of recommended products to the user 45. Beforehand we would need to pre-process the text of reviews in the following manner: # Split text into parts using new line character: text.docs &lt;- Corpus(VectorSource(recommendation_26$`review/text`)) toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) text.docs &lt;- tm_map(text.docs, toSpace, &quot;/&quot;) text.docs &lt;- tm_map(text.docs, toSpace, &quot;@&quot;) text.docs &lt;- tm_map(text.docs, toSpace, &quot;\\\\|&quot;) text.docs &lt;- tm_map(text.docs, content_transformer(tolower)) text.docs &lt;- tm_map(text.docs, removeNumbers) text.docs &lt;- tm_map(text.docs, stripWhitespace) text.docs &lt;- tm_map(text.docs, removeWords, stopwords(&quot;english&quot;)) text.docs &lt;- tm_map(text.docs, removePunctuation) dtm &lt;- DocumentTermMatrix(text.docs, control=list(weighting=weightTf)) m &lt;- as.matrix(t(dtm)) v &lt;- sort(rowSums(m),decreasing=TRUE) d &lt;- data.frame(word = names(v),freq=v) set.seed(1234) wordcloud(words = d$word, freq = d$freq, min.freq = 10, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) From the wordcloud we can see that words color, hair and gloves are quite frequent in the text corpus analyzed. That could be a hint that the user was referring to the usage of the product. The term cheap could be easily spotted as well. This word is not very likable among marketers as it brings unfavorable image to the brand. Nevertheless, it seems that the user believes that the product is affordable. 7.7 Future work This data set provides multiple possibility for the further analysis besides recommender systems. Here are some ideas what can be further done: Sentiment analysis - Sentiment analysis can be done and scores (typically from -3 to +3) accompanied to each review description. That would tell us more about the sentiment that users have about the products reviewed. Prediction of ratings - In case that we would have enough data (ratings) about one product, regardless of customers, it would be possible to develop a machine learning model which based on current features (e.g. price) and additional features (such as sentiment or words in the review) could predict the rating that one product might have. Prediction of the sentiment - in the similar manner as the previous point, it would be useful to train a machine learning model to predict a sentiment that would hypotetically emerge in a reviewer. Topic modeling - topic modeling is unsupervised machine learning technique that could help us identify topics which users discuss in the text of reviews. 7.8 Limitations Limitation related to this data set and building a recommender system is the fact that the majority of users have left only one review: table(as.data.frame(table(my_data$`review/userId`))$Freq) ## ## 1 2 3 4 5 6 9 10 37 ## 7638 259 58 30 9 5 1 1 1 Let us take a look which users left the most reviews: limitations &lt;-as.data.frame(table(my_data$`review/userId`)) limitations %&gt;% arrange(desc(Freq))%&gt;%rename(UserID=Var1)%&gt;% head() ## UserID Freq ## 1 A3M174IC0VXOS2 37 ## 2 A3KEZLJ59C1JVH 10 ## 3 A3QEE0ZPMT3W6P 9 ## 4 A281NPSIMI1C2R 6 ## 5 A2D4GHWUW440K3 6 ## 6 A2FJZE9Y420X77 6 We can see that users under IDs A3M174IC0VXOS2,A3KEZLJ59C1JVH,A3QEE0ZPMT3W6P are rare examples of users who left multiple product reviews. "],["covid-19-dashboard.html", "8 COVID 19 Dashboard 8.1 Data 8.2 Curve of confirmed cases 8.3 Total cases per 1 million people 8.4 Deaths vs Recovered 8.5 Active cases vs New cases", " 8 COVID 19 Dashboard Here you go directly to the dashboard 8.1 Data # Github covid19_confirmed_git &lt;-&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&quot; covid19_confirmed_git &lt;- read_csv(url(covid19_confirmed_git)) # Worldometer ## Covid codivid19_all &lt;- &quot;https://www.worldometers.info/coronavirus/&quot; main_table &lt;- codivid19_all%&gt;% xml2::read_html()%&gt;% html_nodes(xpath=&#39;//*[@id=&quot;main_table_countries_today&quot;]&#39;) %&gt;% html_table() main_table &lt;- as.data.frame(main_table) ## Filtering data for Balkan countries (plus Italy and Austria) balkan &lt;- filter(main_table,Country.Other == &quot;Bosnia and Herzegovina&quot; | Country.Other == &quot;Italy&quot; | Country.Other == &quot;Croatia&quot; | Country.Other == &quot;Serbia&quot; | Country.Other == &quot;Montenegro&quot; | Country.Other == &quot;Slovenia&quot; | Country.Other == &quot;Austria&quot; | Country.Other == &quot;North Macedonia&quot; | Country.Other == &quot;Greece&quot;) # Removing comma from numbers balkan[c(&quot;TotalRecovered&quot;,&quot;TotalDeaths&quot;,&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;ActiveCases&quot;,&quot;Tot.Cases.1M.pop&quot;)] &lt;- lapply(balkan[c(&quot;TotalRecovered&quot;,&quot;TotalDeaths&quot;,&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;ActiveCases&quot;,&quot;Tot.Cases.1M.pop&quot;)], function(x) gsub(&quot;,&quot;,&quot;&quot;,x)) # Turning columns to numeric balkan[c(&quot;TotalRecovered&quot;,&quot;TotalDeaths&quot;,&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;ActiveCases&quot;,&quot;Tot.Cases.1M.pop&quot;)] &lt;- lapply(balkan[c(&quot;TotalRecovered&quot;,&quot;TotalDeaths&quot;,&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;ActiveCases&quot;,&quot;Tot.Cases.1M.pop&quot;)], as.numeric) 8.2 Curve of confirmed cases The graph shows the number of confirmed cases by the last date shown.Updates daily at around 23:59 UTC. # Curve of confirmed cases---- columns &lt;- colnames(covid19_confirmed_git)[5:ncol(covid19_confirmed_git)] final &lt;-as.data.frame(pivot_longer(covid19_confirmed_git, cols = columns, names_to = &quot;Year&quot;, values_to = &quot;Confirmed&quot;)) final$Year &lt;- as.Date.character(final$Year,&quot;%m/%d/%y&quot;) colnames(final) &lt;- c(&quot;Province&quot;,&quot;Country&quot;,&quot;Lat&quot;,&quot;Long&quot;,&quot;Year&quot;,&quot;Confirmed&quot;) filter &lt;- filter(final, Country == &quot;Bosnia and Herzegovina&quot; | Country == &quot;Italy&quot; | Country == &quot;Croatia&quot; | Country == &quot;Serbia&quot; | Country == &quot;Slovenia&quot; | Country == &quot;Montenegro&quot; | Country == &quot;Austria&quot; | Country == &quot;North Macedonia&quot; | Country == &quot;Greece&quot;) p &lt;-ggplot(filter, aes(x = Year, y = Confirmed)) + geom_line(aes(color = Country), size = 1) + scale_color_brewer(palette=&quot;Set1&quot;)+ theme(legend.title = element_text(size = 6),legend.text = element_text(size = 6), # Remove panel background panel.background = element_blank(), # Add axis line axis.line = element_line(colour = &quot;grey&quot;))+ scale_y_log10(labels = comma)+ scale_x_date(date_labels = &quot;%b-%d&quot;, date_breaks = &quot;4 week&quot;)+ ylab(&quot;Confirmed cases&quot;)+ labs(caption=&quot;Data source: https://github.com/CSSEGISandData/COVID-19&quot;) ggplotly(p) 8.3 Total cases per 1 million people tot_cases_1m &lt;- melt(balkan[,c(&quot;Tot.Cases.1M.pop&quot;,&quot;Country.Other&quot;)]) head(tot_cases_1m) ## Country.Other variable value ## 1 Italy Tot.Cases.1M.pop 40834 ## 2 Austria Tot.Cases.1M.pop 44792 ## 3 Serbia Tot.Cases.1M.pop 44004 ## 4 Croatia Tot.Cases.1M.pop 55959 ## 5 Slovenia Tot.Cases.1M.pop 75913 ## 6 Greece Tot.Cases.1M.pop 14622 p&lt;-ggplot(tot_cases_1m, aes(x=Country.Other,y=value,fill=Country.Other)) + geom_bar(stat = &quot;identity&quot;)+ scale_fill_manual(name=&quot;Country&quot;, values = c(&quot;#E41A1C&quot;, &quot;#377EB8&quot;, &quot;#4DAF4A&quot;, &quot;#984EA3&quot;, &quot;#FF7F00&quot;, &quot;#FFFF33&quot;, &quot;#A65628&quot;, &quot;#F781BF&quot;, &quot;#999999&quot;), labels=c(&quot;Austria&quot;, &quot;Bosnia and Herzegovina&quot;, &quot;Croatia&quot;, &quot;Greece&quot;, &quot;Italy&quot;, &quot;Montenegro&quot;, &quot;N.Macedonia&quot;, &quot;Serbia&quot;, &quot;Slovenia&quot;))+ labs(x=&quot;&quot;,y=&quot;Total Cases per 1M people&quot;, title = &quot;Total Cases per 1m people - Currently&quot;)+ theme(legend.title = element_text(size = 8), axis.text.x = element_blank(), legend.text = element_text(size = 8), panel.background = element_blank(), axis.line = element_line(colour = &quot;grey&quot;)) ggplotly(p) 8.4 Deaths vs Recovered The bar chart shows the total number of recovered people in comparison to the total number of death cases. Updates daily at around 23:59 UTC. options(scipen = 9999) p &lt;- ggplot(balkan) + geom_segment( aes(x=Country.Other, xend=Country.Other, y=TotalRecovered, yend=TotalDeaths), color=&quot;grey&quot;) + geom_point( aes(x=Country.Other, y=TotalRecovered), color=rgb(0.2,0.7,0.1,0.5), size=3 ) + geom_point( aes(x=Country.Other, y=TotalDeaths), color=rgb(0.7,0.2,0.1,0.5), size=3 ) + coord_flip()+ scale_y_log10()+ theme_minimal() + theme( legend.position = &quot;none&quot;, panel.background = element_blank(), panel.grid = element_blank(), axis.line = element_line(colour = &quot;grey&quot;)) + xlab(&quot;&quot;) + ylab(&quot;Number of cases&quot;)+ ggtitle(label = &quot;Death vs Recovered&quot;) ggplotly(p) 8.5 Active cases vs New cases The bar chart shows the number of active cases in comparison to the number of new cases being constantly reported. # Active cases vs New cases active_and_new &lt;- melt(balkan[,c(&quot;ActiveCases&quot;,&quot;NewCases&quot;,&quot;Country.Other&quot;)]) p &lt;- ggplot(active_and_new, aes(x=Country.Other, y=value, fill=variable)) + geom_bar(stat=&#39;identity&#39;, position=&#39;dodge&#39;, color=&quot;black&quot; ,aes(text=paste(&quot;Country: &quot;,Country.Other, &quot;\\n&quot;, variable,&quot;:&quot;,value, sep=&quot;&quot;))) + scale_fill_brewer(palette = &quot;Paired&quot;)+ scale_y_continuous(labels=comma, trans = &quot;log10&quot;) + ylab(&quot;Number of cases&quot;)+ xlab(&quot;&quot;)+ theme_minimal()+ labs(title = &quot;&quot;,fill=&quot;&quot;)+ coord_flip() ggplotly(p, tooltip = &quot;text&quot;) "]]
