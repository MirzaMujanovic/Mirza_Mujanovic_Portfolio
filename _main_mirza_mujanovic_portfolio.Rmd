--- 
knit: "bookdown::render_book"
title: "Mirza Mujanovic - Portfolio"
site: bookdown::bookdown_site
output_dir: "docs"
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

# About me {-}

```{r, out.width = "40%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("Graphics/my_picture.jpg")
```


<div style="text-align: justify"> 

My name is Mirza Mujanović, I am an ever-curious, eager to learn and ready to adapt student in his third semester of Marketing master program at the Vienna University of Economics and Business. Originally, I come from Bosnia & Herzegovina, however, I completed a bachelor’s program in business administration at the Vienna University of Economics and Business (WU). Currently, I hold a part-time position of teaching and eLearning assistant at the WU. Thinking of my future plans I see myself working in the field of data analytics or data science for marketing 


</div>
 
<div style="text-align: justify"> 

Random facts about me:

* Big basketball fan. Nowadays, fan of sports analytics.
* Interested in psychology-related topics.
* Used to collect old money together with my older sister. 
* Favorite food: Bulgogi and pudding (but not together, of course!).

</div>

<!--chapter:end:index.Rmd-->

---
title: "Emotions_In_Online_Customer_Reviews"
---

# Emotions In Online Customer Reviews 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error= FALSE, message = FALSE, warning = FALSE, fig.align = "center")
```

<div style="text-align: justify"> 

Consumers usually seek quality and relevant information when buying new products. With the expansion and availability of the Internet, online consumer reviews have become a valuable resource to look at. Several studies tried to demystify relationship between product sales and online customer reviews. On the one hand, some of them, such as [Senecal and Nantel (2004)](https://www.researchgate.net/publication/222519112_The_Influence_of_Online_Product_Recommendations_on_Consumers'Online_Choices), suggest that participants who consulted product recommendations selected these products twice as often as those who did not consult recommendations. On the other hand, [Zhang and Dellarocas (2006)](https://www.researchgate.net/publication/227600950_Exploring_the_Value_of_Online_Product_Reviews_in_Forecasting_Sales_The_Case_of_Motion_Pictures) find that online reviews and do not influence sales and serve solely as prediction.

Between these two opinion fronts, one thing is certain: both sides aim to find out how consumers perceive and process word-of-mouth in a digital environment. In the academic paper [The Role of Emotions for the Perceived Usefulness in Online Customer Reviews](#https://www.jstor.org/stable/pdf/20619095.pdf
) authors suggests that emotions impact the helpfulness ratings, i.e., the quality of online reviews as perceived by other customers. They found that, on average, the most prominent emotion dimensions that influence helpfulness ratings are **trust, joy, and anticipation**. Inspired by these findings, I decided to apply natural language processing techniques to analyze online customer reviews of a bestselling product on Amazon and try to detect those emotions using available lexicons. Final insights will show us whether trust, joy and anticipation can be identified in the reviews, thus improve helpfulness of reviews for potential customers.

</div>

## What to expect in this article? {-}

First, I will extract text via web-scrapping and form a corpus. Next, the text in the corpus will be pre-processed. Subsequently, from the pre-processed text will be stored in form of document-term-matrices or term-document matrices. Finally, an exploratory text analysis will be conducted and corresponding marketing implications pointed out.

```{r,echo=FALSE}
# Packages ----
library(sentimentr)
library(purrr)
library(textdata)
library(ggplot2)
library(ggthemes)
library(xml2)
library(rvest)
library(wordcloud)
library(RColorBrewer)
library(NLP)
library(tm)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggthemes)
library(plotly)
library(tidyverse)
library(broom)
library(remotes)
library(janeaustenr)
library(qdap)
library(syuzhet)
library(sjmisc)
library(topicmodels)
```

## Dictionaries for NLP

For this exercise I will use 3 different lexicons available for R.
One of them is AFINN, a lexicon of words rated for valence between minus five (indicating negative valence) and plus five (indicating positive valence). Next, I will use NRC Emotion Lexicon, which consists of English words and their labels for eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).

```{r}
# Dictionaries ----
afinn <- get_sentiments("afinn")
bing <- get_sentiments("bing")
loughran <- get_sentiments("loughran")
nrc <- get_sentiment_dictionary('nrc', language = "english")
```

## Data set

```{r,eval=FALSE, echo=FALSE}
# Web scraping ----
scraping <- function(ASIN, page_num){
  
  url_reviews <- paste0("https://www.amazon.com/product-reviews/",ASIN,"/?pageNumber=",page_num)
  
  doc <- read_html(url_reviews)
  
  # Review Title
review_title <- doc %>% 
    html_nodes("[class='a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold']") %>%
    html_text()
  
  # Review Text
review_text <- doc %>% 
    html_nodes("[class='a-size-base review-text review-text-content']") %>%
    html_text()
  
  # Number of stars in review
review_star <-  doc %>%
    html_nodes("[data-hook='review-star-rating']") %>%
    html_text()
  
  # Return a tibble
  tibble(review_title,
         review_text,
         review_star,
         page = page_num) %>% return()
}

#scraping(ASIN = "B087LW2KFG", page_num = 5)

#----
ASIN <- "B081FZV45H" # New Apple MacBook Pro (16-inch, 16GB RAM, 512GB Storage, 2.6GHz Intel Core i7) - Space Gray
page_range <- 1:20 # Let's say we want to scrape pages 1 to 10

# Create a table that scrambles page numbers using `sample()`
# For randomising page reads!
match_key <- tibble(n = page_range,
                    key = sample(page_range,length(page_range)))

lapply(page_range, function(i){
  j <- match_key[match_key$n==i,]$key
  
  message("Getting page ",i, " of ",length(page_range), "; Actual: page ",j) # Progress bar
  
  Sys.sleep(3) # Take a three second break
  
  if((i %% 3) == 0){ # After every three scrapes... take another two second break
    
    message("Taking a break...") # Prints a 'taking a break' message on your console
    
    Sys.sleep(2) # Take an additional two second break
  }
  scraping(ASIN = ASIN, page_num = j) # Scrape
}) -> output_list
```


For our analysis, we will use text of 200 online customer reviews from *Apple MacBook Pro (16-inch, 16GB RAM, 512GB Storage, 2.6GHz Intel Core i7)* obtained in unpre-processed form:

```{r, echo=FALSE}
# Load in data
output_list <- readRDS("data/MacBook.rds")
# Compile online customer reviews with corresponding page
review <- bind_rows(output_list, .id = "page")
# Transform the text to UTF-8 
review$review_text <- iconv(review$review_text, 'utf-8', 'ascii', sub='')
# Observe the text
head(review$review_text,10)
```

## Corpus cleaning

From the results above we could see that text contains unnecessary characters. Therefore, I will use some usual procedure to clean up the reviews' text and make it more understandable.

For the purpose of this exercise and for efficiency reasons, we will use the volatile corpus, that stores the collection of documents in RAM memory. To create a volatile corpus, I need to pass reviews' text in such a form that each review text is interpretated as a document.

```{r}
# Creation of volatile corpus
review.corpus <- VCorpus(VectorSource(review$review_text))
```
We see that the volatile corpus contains as many documents as many online reviews we collected.

To undertake a custom transformation, I will use `tm` package and `content_transformer()` function.
It takes a custom function as input, which defines what transformation needs to be done: 

```{r}
review.toSpace<- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
review.corpus <- tm_map(review.corpus, review.toSpace, "/") # remove "/"
review.corpus <- tm_map(review.corpus, review.toSpace, "@") # remove "@"
review.corpus <- tm_map(review.corpus, review.toSpace, "\\|") # remove "\\|"
review.corpus <- tm_map(review.corpus, content_transformer(tolower)) # convert all capital letters to small
review.corpus <- tm_map(review.corpus, removeNumbers) # convert all capital letters to small
review.corpus <- tm_map(review.corpus, removeWords, stopwords("english")) # remove stop-words 
review.corpus <- tm_map(review.corpus, removePunctuation) # remove punctuation
review.corpus <- tm_map(review.corpus, stripWhitespace) # strip extra whitespace from a document
```

After cleaning the corpus, we can use document-term-matrix to store our cleaned corpus:

```{r}
review.dtm <- DocumentTermMatrix(review.corpus)
```

However, document-term-matrix is not the most suitable to work with, because it stores review texts in rows and terms frequencies in columns. We will transform it with `tidy` function:

```{r}
# Tidy up the document-term-matrix
review.tidy <- tidy(review.dtm)
review.tidy$count <-as.numeric(review.tidy$count) # Ensure correct class
colnames(review.tidy)[2]<- 'word' # change name of the column from "term" to "word"
review.tidy$document <- as.numeric(review.tidy$document) # Ensure correct class
```

Our tidy format has dimensions 6907 (the total number of terms) x 3 (document, term and count of the term in corresponding document):

```{r}
dim(review.tidy) # Dimensions
head(review.tidy)# Display first 6 rows
```

## Visualisations of terms frequency

### Bar charts with the most frequent terms

We would be interested in the most frequent words used in customer reviews. Sometimes just a glimpse of the most frequent words is sufficient to get some insights. 

Here we see that word "love" and "great" appears among most frequent terms.

```{r}
# Most frequent terms ----
review.tdm <- TermDocumentMatrix(review.corpus)
review.m <- as.data.frame.matrix(review.tdm)
review.v <- sort(rowSums(review.m),decreasing=TRUE)
word.names<-names(review.v)
df.review.v<-data.frame(review.v,word.names)
colnames(df.review.v)<-c("n","word")
p<-ggplot(data=df.review.v[1:20,], aes(x=reorder(word,n), y=n)) +
  geom_bar(stat="identity",fill="steelblue") + 
  coord_flip() + 
  ggtitle("20 most frequent words in customer reviews - MacBook Pro")+
  xlab("Count")+
  ylab("Word")+
  theme_test()
ggplotly(p)

```

### Wordcloud with the most frequent terms

Similarly to the bar chart with the most frequent words, we could use **wordcloud** as well. It displays words from the corpus and signalizes their frequency by displaying more frequent words bigger relative to those that appear less frequently in the corpus. In the wordcloud below you can see 200 most frequent words, where the minimum frequency was set to 1.

```{r}
# Wordcloud
review.tdm <- TermDocumentMatrix(review.corpus)
review.m <- as.matrix(review.tdm)
review.v <- sort(rowSums(review.m),decreasing=TRUE)
review.d <- data.frame(word = names(review.v),freq=review.v)
set.seed(1234)
wordcloud(words = review.d$word, freq = review.d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))

```

### The most frequent terms indicating emotions

When it comes to anticipation, words such as "good","time","happy" or "powerful" indicates that this emotion can be identified among customer reviews. On the other hand, there are some words that could be a signal both for good and bad experience: "finally","money" or "wait". 

```{r}
# Anticipation words----
nrc.anticipation <- subset(nrc, nrc$sentiment=="anticipation")
review.anticipation.words <- inner_join(review.tidy, nrc.anticipation)
review.anticipation.words <- count(review.anticipation.words, word)
review.anticipation.words <- review.anticipation.words[order(review.anticipation.words$n,decreasing = TRUE),]
p<-ggplot(data=review.anticipation.words[1:20,], aes(x=reorder(word,n), y=n)) +
  geom_bar(stat="identity",fill="orange") + 
  coord_flip() + 
  ggtitle("20 most frequent anticipation words in customer reviews")+
  xlab("Count")+
  ylab("Word")+
  theme_minimal()
ggplotly(p)
```

Similarly to anticipation, now we observe a list of top 20 words that indicate trust. It reveals new quite frequent term in the corpus: "recommend". 

```{r}
# Trust words----
nrc.trust <- subset(nrc, nrc$sentiment=="trust")
review.trust.words <- inner_join(review.tidy, nrc.trust)
review.trust.words <- count(review.trust.words, word)
review.trust.words <- review.trust.words[order(review.trust.words$n,decreasing = TRUE),]
p<-ggplot(data=review.trust.words[1:20,], aes(x=reorder(word,n), y=n)) +
  geom_bar(stat="identity",fill="royalblue1") + 
  coord_flip() + 
  ggtitle("20 most frequent trust words in customer reviews")+
  xlab("Count")+
  ylab("Word")+
  theme_minimal()
ggplotly(p)
```

Although at the bottom of the list, "The top 20 list" of joy words displays some additional words that we did not observe previously such as "beautiful","gorgeous","wonderful","improvement","excellent".

```{r}
# Joy words ----
nrc.joy <- subset(nrc, nrc$sentiment=="joy")
review.joy.words <- inner_join(review.tidy, nrc.joy)
review.joy.words <- count(review.joy.words, word)
review.joy.words <- review.joy.words[order(review.joy.words$n,decreasing = TRUE),]
p<-ggplot(data=review.joy.words[1:20,], aes(x=reorder(word,n), y=n)) +
  geom_bar(stat="identity",fill="darkorange1") + 
  coord_flip() + 
  ggtitle("20 most frequent trust words in customer reviews")+
  xlab("Count")+
  ylab("Word")+
  theme_minimal()
ggplotly(p)
```

## Sentiment analysis

### Polarity timeline

One usual way to compare and quantify emotions in text is via polarity. We simply count number of unique words in each document (=review) labelled as negative and deduct from the count of unique positive words. For instance, the first review contains 2 unique positive words ("great" and "strong") and none negative unique words. Therefore, its polarity score is 2.

This polarity timeline suggests very important implication: the reviews' sentiment is moving above the 0, bearly going even below +2, giving an indication that this product continuously meet customers' expectations. That is a good signal to believe that customers are rather satisfied with the product.    

```{r}
# Polarity timeline ----
review.sentiment <- inner_join(review.tidy, bing)
review.sentiment <- count(review.sentiment, sentiment, index=document)
review.sentiment <- spread(review.sentiment, sentiment, n, fill=0)
review.sentiment$polarity <- review.sentiment$positive - review.sentiment$negative
review.sentiment$pos <- ifelse(review.sentiment$polarity >=0, "Positive", "Negative")
p<-ggplot(review.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat="identity", position="identity", width=1)+theme_gdocs()+ggtitle(label="Polarity timeline")
ggplotly(p)

# Smooth curve
review.smooth <- ggplot(review.sentiment, aes(index, polarity))
p<-review.smooth + stat_smooth() + theme_gdocs() + ggtitle("Polarity timeline - smooth")
ggplotly(p)

```

In the polarity graph at index 81 we identify a review with sentiment score of even 34! This seems to be a thrilled customer every brand loves! Let us take closer look:

```{r}
review.sentiment <- inner_join(review.tidy, bing)
doc_81<-filter(review.sentiment, document=="81")
head(doc_81[order(doc_81$count,decreasing = T),])
```

Finally, it certainly pays off to check the actual review:

```{r, out.width="50%"}
# Outlier in polarity score
review$review_text[81]
```

It seems that our assumption was correct! The customer was definitely thrilled! This is a nice example how you can identify and take closer look at reviews that stand out based on its polarity score.

### Analysis on sentence-level

Text analysis provides freedom to choose level of observation. So far, we explored words and their frequencies, we explored customer reviews and quantified their sentiment in two dimensions (positive and negative). Next, we will approach the task of identifying the most negative and positive reviews by organizing text by sentences. By doing so, we will directly access those sentences whose average sentiment stand out.   

```{r}
# Calculating the average sentiment
review.highlighted<-review$review_text%>%
  get_sentences() %>%
  sentiment_by()
head(review.highlighted)
```

```{r}
# Preparing data
review.score <- subset(review.highlighted, select = c("ave_sentiment","element_id"))
review.worst <- review.score[order(review.score$ave_sentiment,decreasing = FALSE),]
review.worst<-review.worst$element_id[1:10]
review.best <- review.score[order(review.score$ave_sentiment, decreasing = TRUE),]
review.best <- review.best$element_id[1:10]
sentences<-review$review_text %>% get_sentences()
sentences<-as.matrix(sentences)
```

And here we have "the worst 10 sentences" from customer reviews;
```{r}
# 10 worst sentences
sentences[review.worst]
```
Despite the fact that positive sentiment prevails, we see that there are certain problems associated with MacBook laptop. Issues with screen, problems with woofers, disappointment that there are no ports, unsatisfying value-price ratio.

```{r}
# 10 most positive sentences
sentences[review.best]
```
If we take a look at "10 most positive sentences" from customer reviews, we would find a similar evidence as we obtained with polarity score. However, by reading those sentences a reader can have better feeling what the reviewer is actually satisfied or unsatisfied with. Here we see that some people admire the speed for instance.

### What are the most emotional reviews?

Package `sentimentr` provides nice function `emotion()` which uses a dictionary to find emotion words and then compute the rate per sentence. The final emotion score ranges between 0 (no emotion used) and 1 (all words used were emotional).

```{r}
# Extract emotions terms
reviews.emotion <- review$review_text %>% get_sentences() %>% emotion()

# Top 50 sentences with the highest emotion score 
top_emotional_sentences <- unique(reviews.emotion[order(reviews.emotion$emotion,decreasing = TRUE),]$element_id[1:50])

# The most emotional reviews
sentences[top_emotional_sentences,]
```

We can see that identified sentences very clearly reflect emotions that customers expressed. It seems that intensity of emotions is high in both positive and negative direction.

Finally, we can plot detected emotions in order to get a bit more clear insight in emotional structure detected in the reviews:

```{r}
# Plot of emotion
plot(reviews.emotion,
     transformation.function = syuzhet::get_dct_transform,
     drop.unused.emotions = TRUE, facet = TRUE)
```

INTERPRETATION


<!--chapter:end:01-Emotions_In_Online_Customer_Reviews.Rmd-->

---
title: '"CO2mustGO" Iniciative'
---

```{r, echo=FALSE}
library(shinycssloaders)
library(reshape2)
library(ggplot2)
library(dplyr)
library(rvest)
library(plotly)
library(scales)
library(tidyverse)
library(wesanderson)    
library(shiny)
library(shinydashboard)
library(twitteR)
library(rtweet)
library(sentimentr)
library(tidyverse)
library(rvest)
library(purrr)
library(devtools)
library(textdata)
library(ggplot2)
library(ggthemes)
library(xml2)
library(qdap)
library(rvest)
library(wordcloud)
library(RColorBrewer)
library(NLP)
library(tm)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggthemes)
library(plotly)
library(tidyverse)
library(broom)
library(remotes)
library(janeaustenr)
library(qdap)
library(glue)
library(syuzhet)
library(rtweet)
library(ggplot2)
library(igraph)
library(ggraph)
library(widyr)
library(syuzhet)
```


# Twitter Analysis for CO2mustGo initiative

Tweets downloaded on 10.06.2020

```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
carbon_tweets <- read_twitter_csv("data/#carbonfootprintOR#greenhouse-tweets.csv", unflatten = T)
carbon_tweets <- carbon_tweets[, colSums(is.na(carbon_tweets)) != nrow(carbon_tweets)]
```


```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Removing any character that you don’t want to show in our text such as hyperlinks, @ mentions or punctuations
carbon_tweets$stripped_text <- gsub("https\\S*","",  carbon_tweets$text)
carbon_tweets$stripped_text <- gsub("@\\S*","", carbon_tweets$stripped_text)
carbon_tweets$stripped_text <- gsub("amp","",carbon_tweets$stripped_text)
carbon_tweets$stripped_text <- gsub("[\r\n]","",carbon_tweets$stripped_text)
carbon_tweets$stripped_text <- gsub("[[:punct:]]", "",carbon_tweets$stripped_text)

# Convert text to lowercase, punctuation is removed, occurrence/frequency of the each word will be added
carbon_tweets_clean <- carbon_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)

# Remove stop words
data("stop_words")
carbon_tweets_words <- carbon_tweets_clean %>%
  anti_join(stop_words)
```

## Tweets distribution 

```{r,out.width="100%",error=FALSE, message=FALSE, warning=FALSE}
# Distribution of tweets considered in the data.
search_term <- '#carbonfootprint OR #co2'
by <- 'hour'
p <- ts_plot(carbon_tweets, by = by, trim = 2) + geom_point() + theme_minimal() + labs(title = paste0("Tweets with ",search_term," by ",by),x = 'Date', y = 'Count', caption = 'Source: Twitter API')
ggplotly(p)
```

The peak of tweets has been registered on 6th of June. This happening has to be more closely analysed.


## Word Frequency in tweets with  #carbonfootprint or #co2 
```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
p <- carbon_tweets_words %>%
  dplyr::count(word, sort=T) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  theme_minimal()+
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = "Count of unique words found in tweets")
ggplotly(p)
```

No major surprises regarding the most frequent words. 

##  Word Network in tweets with #carbonfootprint or #co2
```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Remove punctuation, convert to lowercase, add id for each tweet!
carbon_tweets_paired_words <- carbon_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)

#carbon_tweets_paired_words %>%
 # dplyr::count(paired_words, sort = TRUE)

#library(tidyr)
carbon_tweets_separated_words <- carbon_tweets_paired_words %>%
  separate(paired_words, c("word1", "word2"), sep = " ")

carbon_tweets_filtered <- carbon_tweets_separated_words %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

carbon_words_counts <- carbon_tweets_filtered %>%
  dplyr::count(word1, word2, sort = TRUE)

#library(igraph)
#library(ggraph)

# Plot carbon change word network
p<- carbon_words_counts %>%
  filter(n >=30) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = 0.5, edge_width = n)) +
  geom_node_point(color = "darkslategray4", size = 3) +
  geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
  labs(title = "Word Network: Tweets using the hashtag #carbonfootprint or #co2",
       subtitle = "Text mining twitter data ",
       x = "", y = "")
p

```

The word network is made based on bi-grams. Basically, based on the number of times two words shows up together. Interesting observation is that the word network shows the word "justiceforvinayaki" appering together with "climatecrisis". More specifically, "justiceforvinayaki" is actually a hashtag related to the story behind the pregnant elephant's killing in Kerala's Palakkad. More you can read [here](https://zeenews.india.com/india/justiceforvinayaki-story-behind-the-pregnant-elephants-killing-in-keralas-palakkad-2288223.html).

## Wordcloud of words with #carbonfootprint #co2
```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Text preparation
carbon_tweets$stripped_text <- iconv(carbon_tweets$stripped_text, 'utf-8', 'ascii', sub='')
review.docs <- Corpus(VectorSource(carbon_tweets$stripped_text))
review.toSpace<- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
review.docs <- tm_map(review.docs, review.toSpace, "/")
review.docs <- tm_map(review.docs, review.toSpace, "@")
review.docs <- tm_map(review.docs, review.toSpace, "\\|")
review.docs <- tm_map(review.docs, content_transformer(tolower))
review.docs <- tm_map(review.docs, removeNumbers)
review.docs <- tm_map(review.docs, removeWords, stopwords("english"))
review.docs <- tm_map(review.docs, content_transformer(tolower))
review.docs <- tm_map(review.docs, removePunctuation)
review.docs <- tm_map(review.docs, stripWhitespace)
# Text stemming
#review.docs <- tm_map(review.docs, stemDocument)
review.tdm <- TermDocumentMatrix(review.docs)
review.m <- as.matrix(review.tdm)
review.v <- sort(rowSums(review.m),decreasing=TRUE)
review.d <- data.frame(word = names(review.v),freq=review.v)
set.seed(1234)
wordcloud(words = review.d$word, freq = review.d$freq,
               max.words = 200,
               min.freq = 10,
               random.order=FALSE, rot.per=0.15,
               colors=brewer.pal(8, "Dark2"), scale=c(8,.3), vfont=c("sans serif","plain"))
```

Minimum word frequency: 10
Number of words in the wordcloud: 200


```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
library(wordcloud) 
library(reshape2)
par(mar = rep(0, 4))
set.seed(1234)
carbon_tweets_words%>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment,sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("darkred","darkgreen"),
                   max.words = 400,
                   min.freq= 10,
                   scale = c(4.0,0.25))
```


## Sentiment analysis:
```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Sentiment analysis 
sentiment <- carbon_tweets[,3:5] %>% unnest_tokens(output = 'word', input = 'text')
#Add sentiment dataset
sentiment_dataset <- get_sentiments("afinn")
sentiment_dataset <- arrange(sentiment_dataset, -value)

#Merge
sentiment <- merge(sentiment, sentiment_dataset, by = 'word')

#Clean
sentiment$word <- NULL
sentiment$screen_name <- NULL

#Time
sentiment$hour <- format(base::round.POSIXt(sentiment$created_at, units="hours"), format="%H:%M")

#Pivot
pivot <- sentiment %>%
  group_by(hour) %>%
  summarise(sentiment = mean(value))

#head(pivot)

#plot
p <- ggplot(pivot[-1,], aes(x = hour, y = sentiment)) + geom_line(group = 1) + geom_point() + theme_minimal() + labs(title = paste0('Average sentiment of tweetings mentioning "',search_term,'"'),x = 'Date', y = 'Sentiment', caption = 'Source: Twitter API')

#p <- ggplot(pivot[-1,], aes(x = hour, y = sentiment)) + geom_line(group = 1) + geom_point() + theme_minimal() + labs(title = paste0('Average sentiment of tweetings mentioning "',search_term,'"'),subtitle = paste0(pivot$hour[2],' - ',pivot$hour[nrow(pivot)],' on ',format(sentiment$created_at[1],'%d %B %Y')),x = 'Date', y = 'Sentiment', caption = 'Source: Twitter API')


ggplotly(p)
```

##Visualize the emotions
```{r, fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Get sentiments using the four different lexicons
syuzhet <- get_sentiment(carbon_tweets$stripped_text, method="syuzhet")
bing <- get_sentiment(carbon_tweets$stripped_text, method="bing")
afinn <- get_sentiment(carbon_tweets$stripped_text, method="afinn")
nrc <- get_sentiment(carbon_tweets$stripped_text, method="nrc")
sentiments <- data.frame(syuzhet, bing, afinn, nrc)

# get the emotions using the NRC dictionary
nrc.sentiment <- get_nrc_sentiment(carbon_tweets$stripped_text)
emo_bar = colSums(nrc.sentiment)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])

# Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~count, type="bar", color=~emotion) %>%
  layout(xaxis=list(title=""), showlegend=FALSE,
         title="Distribution of emotion categories")
```

```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Comparison word cloud
all = c(
  paste(carbon_tweets$stripped_text[nrc.sentiment$anger > 0], collapse=" "),
  paste(carbon_tweets$stripped_text[nrc.sentiment$anticipation > 0], collapse=" "),
  paste(carbon_tweets$stripped_text[nrc.sentiment$disgust > 0], collapse=" "),
  paste(carbon_tweets$stripped_text[nrc.sentiment$fear > 0], collapse=" "),
  paste(carbon_tweets$stripped_text[nrc.sentiment$joy > 0], collapse=" "),
  paste(carbon_tweets$stripped_text[nrc.sentiment$sadness > 0], collapse=" "),
  paste(carbon_tweets$stripped_text[nrc.sentiment$surprise > 0], collapse=" "),
  paste(carbon_tweets$stripped_text[nrc.sentiment$trust > 0], collapse=" ")
)
all <- removeWords(all, stopwords("english"))
# create corpus
corpus = Corpus(VectorSource(all))
#
# create term-document matrix
tdm = TermDocumentMatrix(corpus)
#
# convert as matrix
tdm = as.matrix(tdm)
tdm1 <- tdm[nchar(rownames(tdm)) < 11,]
#
# add column names
colnames(tdm) = c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')
colnames(tdm1) <- colnames(tdm)
comparison.cloud(tdm1, random.order=FALSE,
                 colors = c("#00B2FF", "red", "#FF0099", "#6600CC", "green", "orange", "blue", "brown"), title.size=1, max.words=250, scale=c(2.5, 0.4),rot.per=0.4)
```


## Top retweeted Tweets

### Top retweets (with equal or more than 60 mentions)
```{r, fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Select top retweeted tweets
selected <- which(carbon_tweets$retweet_count >= 60)

# Plot 
dates <-as.POSIXct(strptime(carbon_tweets$created_at, format="%Y-%m-%d"))
plot(x=dates, y=carbon_tweets$retweet_count, type="l", col="grey",
     xlab="Date", ylab="Times retweeted")
colors <- rainbow(10)[1:length(selected)]
points(dates[selected], carbon_tweets$retweet_count[selected],
       pch=19, col=colors)
```


### Interactive graph with retweets' text 
```{r, fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Plotly
carbon_tweets$created_at <-as.POSIXct(strptime(carbon_tweets$created_at, format="%Y-%m-%d"))
p<-ggplot(carbon_tweets, aes(x=created_at, y=retweet_count, col=retweet_count, size=retweet_count, retweet_text=retweet_text, created_at=created_at, retweet_name=retweet_name))+geom_point() +xlab(label="Date")+ylab(label="Retweet count")+ggtitle(label="Top retweeted tweets(hover over points to see text)")

ggplotly(p,tooltip = c("retweet_text","retweet_name"))
```

## Network of retweets
```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Create data frame for the network
rt_df <- carbon_tweets[, c("screen_name" , "retweet_screen_name" )]

# Remove rows with missing values
rt_df_new <- rt_df[complete.cases(rt_df), ]

# Convert to matrix
matrx <- as.matrix(rt_df_new)

# Create the retweet network
nw_rtweet <- graph_from_edgelist(el = matrx, directed = TRUE)

# View the retweet network
print.igraph(nw_rtweet)
```

### Follower count of network users
```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
followers <-carbon_tweets[, c("screen_name" , "followers_count" )]
# Remove rows with missing values
followers <- followers[complete.cases(rt_df), ]
followers <-unique(followers)
# Categorize high and low follower count
dim(followers)
followers$follow <- ifelse(followers$followers_count > 500, "1", "0")
# Assign external network attributes to retweet network
V(nw_rtweet)$followers <- followers$follow

```


### Putting twitter data on the map (use plotly zoom in locations!)
```{r,error=FALSE, message=FALSE, warning=FALSE,fig.width=12}
# Extract geolocation data and append new columns
library(rtweet)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(rgeos)

pol_coord <- lat_lng(carbon_tweets)
pol_geo <- na.omit(pol_coord[, c("lat", "lng","location","retweet_count")])
world <- ne_countries(scale = "medium", returnclass = "sf")
p<-ggplot(data = world) +
    geom_sf() +
    xlab("Longitude") + ylab("Latitude") +
    geom_point(data= pol_geo,aes(x=lng, y=lat,loc=location,retweet_count=retweet_count),col = "#00acee")+
    theme(panel.grid.major = element_line(color = gray(.25), linetype ="dashed",    size = 0.15),panel.background = element_rect(fill = "aliceblue"))+
    ggtitle("World map with tweets location and retweet count", subtitle = paste0("(", length(unique(pol_geo$location)), " countries)"))
ggplotly(p,tooltip = c("location","retweet_count"))  

```


### Users who retweet the most
```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
# Calculate the out-degree scores
out_degree <- degree(nw_rtweet, mode = c("out"))

# Sort the users in descending order of out-degree scores
out_degree_sort <- sort(out_degree, decreasing = TRUE)
head(out_degree_sort,10)
# INTERPRETATION: Users who retweeted the most.

#Hubs: Tweeter accounts with a lot of outgoing edges.
hs <- hub_score(nw_rtweet, weights=NA)$vector
sort(hs, decreasing = TRUE)[1:20]
# #Ex-kurs: An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices.
# They are likely to retweet.
```

### Users who are the most retweeted
```{r,error=FALSE, message=FALSE, warning=FALSE,fig.width=12}
# Calculate the in-degree scores
in_degree <- degree(nw_rtweet, mode = c("in"))
# Sort the users in descending order of in-degree scores
in_degree_sort <- sort(in_degree, decreasing = TRUE)
head(in_degree_sort,10)
# INTERPRETATION: Users whose posts were retweeted most.

#Authorities: Tweeter accounts with a lot of incoming edges.
as <- authority_score(nw_rtweet, weights=NA)$vector
sort(as, decreasing = TRUE)[1:20]
#Ex-kurs: An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices.
# They are likely to be retweeted.
```

### Users with important role in allowing information to pass through network
Users with higher betweenness has more control over the network.
```{r,error=FALSE, message=FALSE, warning=FALSE,fig.width=12}
# Calculate the betweenness scores of the network
betwn_nw <- betweenness(nw_rtweet, directed = TRUE)
# Sort the users in descending order of betweenness scores
betwn_nw_sort <- betwn_nw %>%
  sort(decreasing = TRUE) %>%
  round() %>% head(10)
betwn_nw_sort
```


### Clustering
```{r,fig.width=12,error=FALSE, message=FALSE, warning=FALSE}
largest_cliques(nw_rtweet) #list only 20 vertices in that cluster
```

### Community detection 
```{r,error=FALSE, message=FALSE, warning=FALSE,fig.width=12}
#https://rpubs.com/cosmopolitanvan/twitternetworks

#Community detection based on edge betweenness (Newman-Girvan)
comm <- cluster_edge_betweenness(nw_rtweet)
sort(sizes(comm), decreasing = T)[1:20]
comm_1 <- communities(comm)

# Tweet accounts in the Community 60 (the biggest community)
comm_1$`60` 
# Tweet accounts in the Community 44 (the second biggest community)
comm_1$`44`
# Tweet accounts in the Community 16 (the second biggest community)
comm_1$`16`
```

<!--chapter:end:02-C02_must_go_iniciative.Rmd-->

---
title: "Text Analysis and Novels"
---

```{r, echo=FALSE}
options(warn = -1)
library(NLP)
library(RColorBrewer)
library(tm)
library(ggplot2)
library(topicmodels)
library(skmeans)
library(clue)
library(fpc)
library(cluster)
library(tm)
library(wordcloud)
library(stringdist)
library(slam)
library(wordcloud)
library(textstem)
library(grid)
library(gtable)
library(tidytext)
library(dplyr)
library(tidyr)
library(plotly)
library(udpipe)
library(igraph)
library(SnowballC)
library(textstem)
```

Downloading Novels 'The Awakening' and 'The Aspern Papers'
```{r}
awakening <- scan("http://www.gutenberg.org/files/160/160-0.txt", what="character", blank.lines.skip = TRUE, sep="\n")
aspern <- scan("http://www.gutenberg.org/files/211/211-0.txt", what="character", blank.lines.skip = TRUE, sep="\n")

awakening<- iconv(awakening, 'utf-8', 'ascii', sub='')
aspern<- iconv(aspern, 'utf-8', 'ascii', sub='')
```


Cleaning the novels
```{r}
awakening.begin <- which(awakening=="THE AWAKENING")[2]
awakening.end   <- which(awakening=="*****") - 1
awakening.v<- awakening[awakening.begin:awakening.end]
head(awakening.v)
tail(awakening.v)


aspern.begin <- which(aspern=="Macmillan and Co., 1888.")+1
aspern.end   <- which(aspern=="End of the Project Gutenberg EBook of The Aspern Papers, by Henry James") - 1
aspern.v <- aspern[aspern.begin:aspern.end]
head(aspern.v)
tail(aspern.v)
```

Awakening:

1. Cutting in chapters and corpus creation
```{r}
awakening.v <- gsub("^I*(X|V)*I*$", "@@@", awakening.v)
awakening.string <- paste(awakening.v, collapse = " ")
awakening.chapters <- strsplit(awakening.string, "@@@ ") 
awakening.df <- as.data.frame(awakening.chapters, stringsAsFactors = FALSE)
awakening.df <-awakening.df[2:38,1]
awakening.df <- as.data.frame(awakening.df)
colnames(awakening.df) <- "chapters"    
awakening.docs <- Corpus(VectorSource(awakening.df$chapters))
```

2. Text preprocessing
```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
awakening.docs <- tm_map(awakening.docs, toSpace, "/")
awakening.docs <- tm_map(awakening.docs, toSpace, "@")
awakening.docs <- tm_map(awakening.docs, toSpace, "\\|")
awakening.docs <- tm_map(awakening.docs, content_transformer(tolower))
awakening.docs <- tm_map(awakening.docs, removeNumbers)
awakening.docs <- tm_map(awakening.docs, removeWords, stopwords("english"))
awakening.docs <- tm_map(awakening.docs, removePunctuation)
awakening.docs <- tm_map(awakening.docs, stripWhitespace)
```

3. WordCloud preparation and wordcloud graph
```{r}
awakening.dtm <- DocumentTermMatrix(awakening.docs, control=list(weighting=weightTf))
awakening.m <- as.matrix(t(awakening.dtm))
awakening_v <- sort(rowSums(awakening.m),decreasing=TRUE)
awakening.d <- data.frame(word = names(awakening_v),freq=awakening_v)
head(awakening.d, 10)
set.seed(1234)
wordcloud(words = awakening.d$word, freq = awakening.d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))
```


Aspern

1. Cutting in chapters and corpus creation
```{r}
aspern.v <- gsub("^I*(X|V)*I*$", "@@@", aspern.v)
aspern.string <- paste(aspern.v, collapse = " ")
aspern.chapters <- strsplit(aspern.string, "@@@ ") 
aspern.df <- as.data.frame(aspern.chapters, stringsAsFactors = FALSE)
aspern.df <-aspern.df[2:38,1]
aspern.df <- as.data.frame(aspern.df)
colnames(aspern.df) <- "chapters"
aspern.docs <- Corpus(VectorSource(aspern.df$chapters))

```


2. Text preprocessing

```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
aspern.docs <- tm_map(aspern.docs, toSpace, "/")
aspern.docs <- tm_map(aspern.docs, toSpace, "@")
aspern.docs <- tm_map(aspern.docs, toSpace, "\\|")
aspern.docs <- tm_map(aspern.docs, content_transformer(tolower))
aspern.docs <- tm_map(aspern.docs, removeNumbers)
aspern.docs <- tm_map(aspern.docs, removeWords, stopwords("english"))
aspern.docs <- tm_map(aspern.docs, removePunctuation)
aspern.docs <- tm_map(aspern.docs, stripWhitespace)

```

3. WordCloud preparation and wordcloud graph

```{r}
aspern.dtm <- DocumentTermMatrix(aspern.docs, control=list(weighting=weightTf))
aspern.m <- as.matrix(t(aspern.dtm))
aspern_v <- sort(rowSums(aspern.m),decreasing=TRUE)
aspern.d <- data.frame(word = names(aspern_v),freq=aspern_v)
head(aspern.d, 10)
set.seed(1234)
wordcloud(words = aspern.d$word, freq = aspern.d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))

```

Commonality cloud and Comparison cloud

1. Creation of a corpus out of the both texts
```{r}
cc.docs <- Corpus(VectorSource(c(awakening.string,aspern.string)))
```

2. Text preprocessing

```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
cc.docs <- tm_map(cc.docs, toSpace, "/")
cc.docs <- tm_map(cc.docs, toSpace, "@")
cc.docs <- tm_map(cc.docs, toSpace, "\\|")
cc.docs <- tm_map(cc.docs, content_transformer(tolower))
cc.docs <- tm_map(cc.docs, removeNumbers)
cc.docs <- tm_map(cc.docs, removeWords, stopwords("english"))
cc.docs <- tm_map(cc.docs, removePunctuation)
cc.docs <- tm_map(cc.docs, stripWhitespace)

```

3.Comparison and Commonality cloud

```{r}
cc.dtm <- DocumentTermMatrix(cc.docs)
cc.m <- as.matrix(t(cc.dtm))
colnames(cc.m)<- c("Awakening","Aspern")
comparison.cloud(cc.m,max.words = 100,min.frrandom.order=FALSE)
commonality.cloud(cc.m,max.words = 100,colors = "steelblue1",min.frrandom.order=FALSE)
```


Sentiment timeline
```{r}
library(tidytext)
library(dplyr)
library(tidyr)
library(plotly)
library(ggthemes)
data("sentiments")
afinn <- subset(sentiments, sentiments$lexicon=='AFINN')
bing <- subset(sentiments, sentiments$lexicon=='bing')
nrc <- subset(sentiments, sentiments$lexicon=='nrc')
```


1.Awakening
```{r}
sent.awakening <- readLines("http://www.gutenberg.org/files/160/160-0.txt")
sent.awakening <-iconv(sent.awakening, 'utf-8', 'ascii', sub='')
awakening.corpus <- VCorpus(VectorSource(sent.awakening))
awakening.corpus <- tm_map(awakening.corpus, content_transformer(tolower))
awakening.corpus <- tm_map(awakening.corpus, removeNumbers)
awakening.corpus <- tm_map(awakening.corpus, removeWords, stopwords("english"))
awakening.corpus <- tm_map(awakening.corpus, removePunctuation)
awakening.corpus <- tm_map(awakening.corpus, stripWhitespace)
awakening.dtm <- DocumentTermMatrix(awakening.corpus)
awakening.tidy <- tidy(awakening.dtm)
awakening.tidy$count <-as.numeric(awakening.tidy$count)
colnames(awakening.tidy)[2]<- 'word'
awakening.tidy$document <- as.numeric(awakening.tidy$document)
nrc.joy <- subset(sentiments, sentiments$lexicon=="nrc" & sentiments$sentiment=="joy")
joy.words <- inner_join(awakening.tidy, nrc.joy)
joy.words <- count(joy.words, word)

bing <- subset(sentiments, sentiments$lexicon=='bing')[,-4]
awakening.sentiment <- inner_join(awakening.tidy, bing)
awakening.sentiment <- count(awakening.sentiment, sentiment, index=document)
awakening.sentiment <- spread(awakening.sentiment, sentiment, n, fill=0)
awakening.sentiment$polarity <- awakening.sentiment$positive - awakening.sentiment$negative
awakening.sentiment$pos <- ifelse(awakening.sentiment$polarity >=0, "pos", "neg")
ggplot(awakening.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat="identity", position="identity", width=1)
awakening.smooth <- ggplot(awakening.sentiment, aes(index, polarity))

(p2<-awakening.smooth + stat_smooth())


```

2.Aspern
```{r}
sent.aspern <- readLines("http://www.gutenberg.org/files/211/211-0.txt")
aspern.corpus <- VCorpus(VectorSource(sent.aspern))
aspern.corpus <- tm_map(aspern.corpus, content_transformer(tolower))
aspern.corpus <- tm_map(aspern.corpus, removeNumbers)
aspern.corpus <- tm_map(aspern.corpus, removeWords, stopwords("english"))
aspern.corpus <- tm_map(aspern.corpus, removePunctuation)
aspern.corpus <- tm_map(aspern.corpus, stripWhitespace)
sent.aspern <-iconv(sent.aspern, 'utf-8', 'ascii', sub='')
aspern.dtm <- DocumentTermMatrix(aspern.corpus)
aspern.tidy <- tidy(aspern.dtm)
aspern.tidy$count <-as.numeric(aspern.tidy$count)
colnames(aspern.tidy)[2]<- 'word'
aspern.tidy$document <- as.numeric(aspern.tidy$document)
nrc.joy <- subset(sentiments, sentiments$lexicon=="nrc" & sentiments$sentiment=="joy")
joy.words <- inner_join(aspern.tidy, nrc.joy)
joy.words <- count(joy.words, word)

bing <- subset(sentiments, sentiments$lexicon=='bing')[,-4]
aspern.sentiment <- inner_join(aspern.tidy, bing)
aspern.sentiment <- count(aspern.sentiment, sentiment, index=document)
aspern.sentiment <- spread(aspern.sentiment, sentiment, n, fill=0)
aspern.sentiment$polarity <- aspern.sentiment$positive - aspern.sentiment$negative
aspern.sentiment$pos <- ifelse(aspern.sentiment$polarity >=0, "pos", "neg")
ggplot(aspern.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat="identity", position="identity", width=1)
aspern.smooth <- ggplot(aspern.sentiment, aes(index, polarity))

(p3<-aspern.smooth + stat_smooth())
```

3. Merging Aspern and Awakening

```{r}
g2 <- ggplotGrob(p2)
g3 <- ggplotGrob(p3)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
grid.newpage()
grid.draw(g)

```


Topic Modelling - Pick up 5 topics and try to make sense of the topics giving a label to them

1.Awakening

```{r}
top.mod.awakening.dtm <- DocumentTermMatrix(awakening.corpus, control = list(weighting=weightTf))
burnin = 1000
iter = 1000
keep = 50
set.seed(510)
top.mod.awakening.n <- nrow(top.mod.awakening.dtm)
top.mod.awakening.dtm <- top.mod.awakening.dtm[row_sums(top.mod.awakening.dtm > 0) > 1,]
top.mod.awakening.lda_basic.model<- LDA(top.mod.awakening.dtm, k = 5L, method = "Gibbs",
                      control = list(burnin = burnin, iter = iter, keep = keep, alpha = 1) )
top.mod.awakening.lda.topics <- as.matrix(topics(top.mod.awakening.lda_basic.model))
#top.mod.awakening.lda.topics
top.mod.awakening.lda.terms <- as.matrix(terms(top.mod.awakening.lda_basic.model, 10))
top.mod.awakening.lda.terms<- iconv(top.mod.awakening.lda.terms, 'utf-8', 'ascii', sub='')
#top.mod.awakening.lda.terms


awakening.top10termsPerTopic <- terms(top.mod.awakening.lda_basic.model, 10)
awakening.top10termsPerTopic <- iconv(top.mod.awakening.lda.terms, 'utf-8', 'ascii', sub='')
#awakening.top10termsPerTopic
colnames(awakening.top10termsPerTopic)<-c("Edna in her house/room"," Work, life and love - Leaving the papers after he/she died","Madame Edna - Mademoiselle Ratignolle - Robert","Emotions - Description of a face ","Mrs.Pontellier - Edna - Robert")
awakening.top10termsPerTopic
awakening.topicNames <- apply(awakening.top10termsPerTopic, 2, paste, collapse=" ")
#awakening.topicNames
```

2. Aspern
```{r}
top.mod.aspern.dtm <- DocumentTermMatrix(aspern.corpus, control = list(weighting=weightTf))
burnin = 1000
iter = 1000
keep = 50
set.seed(510)
top.mod.aspern.n <- nrow(top.mod.aspern.dtm)
top.mod.aspern.dtm <- top.mod.aspern.dtm[row_sums(top.mod.aspern.dtm > 0) > 1,]
top.mod.aspern.lda_basic.model<- LDA(top.mod.aspern.dtm, k = 5L, method = "Gibbs",
                                        control = list(burnin = burnin, iter = iter, keep = keep, alpha = 1) )
top.mod.aspern.lda.topics <- as.matrix(topics(top.mod.aspern.lda_basic.model))
#top.mod.aspern.lda.topics
top.mod.aspern.lda.terms <- as.matrix(terms(top.mod.aspern.lda_basic.model, 10))
top.mod.aspern.lda.terms<- iconv(top.mod.aspern.lda.terms, 'utf-8', 'ascii', sub='')
#top.mod.aspern.lda.terms

aspern.top10termsPerTopic <- terms(top.mod.aspern.lda_basic.model, 10)
aspern.top10termsPerTopic <- iconv(top.mod.aspern.lda.terms, 'utf-8', 'ascii', sub='')
#aspern.top10termsPerTopic
colnames(aspern.top10termsPerTopic)<-c("Home - Seeing somebody - description of eyes - leaving","Jeffrey","Tita","Aunt","Time - Woman - Venice")
aspern.top10termsPerTopic
#aspern.topicNames <- apply(aspern.top10termsPerTopic, 2, paste, collapse=" ")
#aspern.topicNames

```


Extract verbs
```{r}
library(udpipe)
library(igraph)
library(SnowballC)
library(textstem)
model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = 'english-ewt-ud-2.3-181115.udpipe')
```

1.Awakening

```{r}
awakening.corpus <- VCorpus(VectorSource(sent.awakening))
awakening.corpus <- tm_map(awakening.corpus, content_transformer(tolower))
awakening.corpus <- tm_map(awakening.corpus, removeNumbers)
awakening.corpus <- tm_map(awakening.corpus, removeWords, stopwords("english"))
awakening.corpus <- tm_map(awakening.corpus, removePunctuation)
awakening.corpus <- tm_map(awakening.corpus, stripWhitespace)
awakening.corpus <- tm_map(awakening.corpus, lemmatize_strings)
awakening.corpus <-SnowballC::wordStem(awakening.corpus, language = 'en')
awakening.verbs.s <- udpipe_annotate(udmodel_english, awakening.corpus)
awakening.verbs.x <- data.frame(awakening.verbs.s)
library(lattice)
awakening.stats <- subset(awakening.verbs.x, upos %in% c("VERB")) 
awakening.stats <- txt_freq(awakening.stats$token)
awakening.stats$key <- factor(awakening.stats$key, levels = rev(awakening.stats$key))
awakening.verbs.barchart<- barchart(key ~ freq, data = head(awakening.stats, 20), col = "gold", 
         main = "Most occurring Verbs - Awakening", xlab = "Freq")
awakening.verbs.barchart
```

2. Aspern

```{r}
aspern.corpus <- VCorpus(VectorSource(sent.aspern))
aspern.corpus <- tm_map(aspern.corpus, content_transformer(tolower))
aspern.corpus <- tm_map(aspern.corpus, removeNumbers)
aspern.corpus <- tm_map(aspern.corpus, lemmatize_strings)
aspern.corpus <- tm_map(aspern.corpus, removeWords, stopwords("english"))
aspern.corpus <- tm_map(aspern.corpus, removePunctuation)
aspern.corpus <- tm_map(aspern.corpus, stripWhitespace)
aspern.corpus <-SnowballC::wordStem(aspern.corpus, language = 'en')
aspern.verbs.s <- udpipe_annotate(udmodel_english, aspern.corpus)
aspern.verbs.x <- data.frame(aspern.verbs.s)
library(lattice)
aspern.stats <- subset(aspern.verbs.x, upos %in% c("VERB")) 
aspern.stats <- txt_freq(aspern.stats$token)
aspern.stats$key <- factor(aspern.stats$key, levels = rev(aspern.stats$key))
aspern.verbs.barchart <- barchart(key ~ freq, data = head(aspern.stats, 20), col = "gold", 
         main = "Most occurring Verbs - Aspern", xlab = "Freq")
aspern.verbs.barchart
```

Main points of interpretation:

1. The most common words are names of characters (Edna, Tita...)
2. According to the sentiment analysis, Awakening novel is slightly from the beginning to the end. On the other hand, Aspern has a slight drop in the negative  sentiment, but the end is positive.
3.Awakening is a longer text than Aspern
4.Due to lemmatization could not address the time of speaking (wheater it is told in past or future)
5. Words "said",little","think","house","good","like","one","come","back","thought" are the most common.

<!--chapter:end:03-Text_Analysis_and_Fairy_Tales.Rmd-->

