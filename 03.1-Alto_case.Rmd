---
title: "Marketing Analyst Job-Interview Task"
output:
  html_document:
    keep_md: true
    toc: yes
    df_print: paged
  html_notebook: default
  pdf_document:
    toc: yes
---

# Job-interview task: Social media data

```{r,echo=FALSE, fig.align='center', out.width="50%",fig.cap="Foto from Google"}
knitr::include_graphics("Graphics/zealpath.jpg")
```

<div style="text-align: justify"> 

This tasks is a part of a case study given as a job-exercise. Here I aim to demonstrate how one can solve analytics task with R very efficiently. The exact task reads as follow:

*At the company we create clarity, out of the chaos of digital noise. Our big data analytics platform and services combine technology and human expertise to help organizations around the world achieve clear and actionable insights every day.*

*In our team of data scientists, you will become part of the human layer that develops specialized expertise for organizations, we explore hypotheses and dig deeper into big data assets and uncover actionable insights.*


*This assignment is designed to give you a glimpse of some of the challenges you will be facing in this role. Please be aware there are no perfect solutions - for us, it's more important to see how you find solutions, process your ideas, structure your thoughts and how you make your decision paths.*

*Be creative but realistic about what's possible. We are thrilled to get to know a bit more about the way you solve tasks.*

</div> 

## Data

```{r,warning=FALSE,message=FALSE}
library(XML)
library(tibble)
library(tidyverse)
library(readr)
library(kableExtra)
library(ggplot2)
library(plotly)
```

As usual, we first load in data by using `readxl_xlsx()` function and take a glimpse at it:

```{r}
data<-readxl::read_xlsx("data/Alto-Case_Study_Dataset (1).xlsx")
glimpse(data)
```

The data has `r dim(data)[2]` features and `r dim(data)[1]` observations. Let us inspect missing values:

```{r}
apply(is.na(data),2,sum)
```

There are 5 missing entries in "BODY" column and 15 in "PERSONAL-WEBISTE". Consequently, we will remove these entries.

```{r}
data<-(data[complete.cases(data), ])
data
```

Finally, our data has the following structure:

```{r}
glimpse(data)
```

## How many authors have interacted in the database? 

My first task is to identify the number of authors who had interaction with the database.
In order to do so, we will use column "Autors". This column has a class "character", so none class conversion is necessary. Let us inspect number of authors asked:

```{r}
# Total number of interactions interacted
length(data$Autors)
# Number of authors interacted with the data base
length(unique(data$Autors))
```

Out of total `r length(data$Autors)` interactions, `r length(unique(data$Autors))` are unique. Thus, we can say that the number of unique interactions is `r length(unique(data$Autors))`. 

```{r}
# Authors
(task1<-as.data.frame(table(data$Autors)) %>%
  rename(Authors=Var1, Count=Freq) %>%
  arrange(desc(Count))) %>% 
  head(10)%>% kableExtra::kable(escape = T) %>%
  kable_paper(c("hover"), full_width = F)
```

Let us visualize it:

```{r}
library(ggplot2)
task1 %>% 
  arrange(desc(Count)) %>%
  head(10) %>%
  ggplot(aes(x=reorder(Authors,Count),y=Count,fill=Authors)) +
  geom_bar(stat="identity")+
  labs(x="Authors",y="Count",title = "Top 10 Most Active Authors on Twitter")+
  coord_flip()
```


The most active authors on Twitter are:

* "O2" as the most active by far
* "SoldoutGigs" and "O2JobsFeed" are following.

## Which one is the most used media? 

```{r}
media <- as.data.frame(table(data$`PUBLISHER-NAME`))
colnames(media) <- c("Media","Freq")

# Top 10 media used
media <- head(media[order(media$Freq,decreasing = T),],10)
media%>% kableExtra::kable(escape = T) %>%
  kableExtra::kable_paper(c("hover"), full_width = F)

# Plot
ggplot(media,aes(fill=Media)) +
  geom_bar(stat = "identity",aes(reorder(Media,Freq),Freq)) +
  coord_flip() +
  theme(legend.position = "none")+
  scale_y_log10()+
  labs(x="Media", y="Count",title = "Top 10 Most Used Media")
```


The most used media is Twitter, followed by GiffGaff and Facebook.

## What is the percentage of each used media? 

```{r}
media_perc<-as.data.frame(prop.table(table(data$`PUBLISHER-NAME`)))
media_perc$Freq <- round(media_perc$Freq*100,4)
head(media_perc[order(media_perc$Freq,decreasing = T),],10) %>%
  rename(Author=Var1,Percentage=Freq)%>% kable(escape = T) %>%
  kable_paper(c("hover"), full_width = F)
```

## What is the percentage of positive, negative and neutral comments?

```{r}
as.data.frame(prop.table(table(data$SENTIMENT))*100) %>%
  rename(Sentiment=Var1,Percentage=Freq) %>%
  arrange(desc(Percentage))%>% kable(escape = T) %>%
  kable_paper(c("hover"), full_width = F)
```


Based on the analysis, around 77% of comments are neutral, 12% slightly negative and 11% slightly positive. Percentage of extremely positive or extremely negative comments is in total around 0.8%.

## What is the average sentiment in Twitter?

```{r}
mean(data$SENTIMENT)
```


## Visalisation task

<div style="text-align: justify"> 

Make a scatter plot of the database using 3 variables. Two of them are provided here:

* Media: Twitter, Facebook and Instagram.

* Visibility: total of comments and average sentiment.


By combining information about publishers (Twitter, Facebook and Instagram), date of publishing, sentiment and average sentiment we are able to create a multiple line plot to explain sentiment in each publisher in the given period of a day.

First we filtered data to retain publishers such as Twitter, Facebook, Instagram, GiffGaff and O2 UK.
Subsequently, we pivot the table so that the final sheet look like this (only first 6 rows):

</div> 

```{r,eval=TRUE,warning=FALSE,message=FALSE}
plot<-subset(data,`PUBLISHER-NAME`=="Twitter" | `PUBLISHER-NAME`=="Facebook" | `PUBLISHER-NAME`=="Instagram" | `PUBLISHER-NAME`=="GiffGaff" | `PUBLISHER-NAME`=="O2 UK") %>%
  rename(Publisher=`PUBLISHER-NAME`,Sentiment=SENTIMENT) %>%
  group_by(Publisher,PUBDATE) %>%
  mutate(Date=PUBDATE,
         Publisher=as.factor(Publisher))%>% 
  summarise(Sentiment=mean(Sentiment))
plot%>% head()%>% kable(escape = T) %>%
  kable_paper(c("hover"), full_width = F)
```


<div style="text-align: justify"> 

In the first column are publishers we retained. The second column is the exact date and time of publishing the comment. Finally, the last column denotes the sentiment score associated with each comment.

We are in a position to vizualise sentiment scores across platforms in the given observation time.

```{r,eval=TRUE}
ggplot(plot, aes(x = PUBDATE, y = Sentiment)) + 
  geom_point(aes(color = Publisher), size = 1.5) +
  labs(title = "How is sentiment across platform?",x="",subtitle ="Number of comments: FB=177; GiffGaff=204; IG=4; O2 UK=59; TW=3798")+
  facet_grid(Publisher~.)+
  theme_bw()
```


We could see that Twitter is the most balanced publisher, as there are not many deviations. In addition, Facebook and Twitter seem to mimic each other to some extent. Some good news were published on May 27 after 18 PM as the sentiment scores for Twitter, Fabook and O2 UK in this period were extremely positive.

</div> 

