[
["index.html", "Who I am?", " Who I am? My name is Mirza Mujanović, I am an ever-curious, eager to learn and ready to adapt student in his third semester of Marketing master program at the Vienna University of Economics and Business. Originally, I come from Tuzla, Bosnia &amp; Herzegovina, where I finished high school and first two semesters at the Faculty of Economics. In 2015, I moved to Vienna to study, and in 2019 completed a bachelor’s program in business administration at the Vienna University of Economics and Business (WU) in 2019. Currently, I hold a part-time position of teaching and eLearning assistant at the WU. Thinking of my future plans I see myself working in the field of data analytics or data science for marketing. Random facts about me: Big basketball fan. Nowadays, fan of sports analytics. Interested in psychology-related topics. Used to collect old money together with my older sister. Favorite food: Bulgogi and pudding (but not together, of course!). You can download my CV here "],
["emotions-in-online-customer-reviews.html", "1 Emotions In Online Customer Reviews What to expect in this article? 1.1 Dictionaries for NLP 1.2 Data set 1.3 Corpus cleaning 1.4 Visualisations of terms frequency 1.5 Sentiment analysis", " 1 Emotions In Online Customer Reviews Figure 1.1: Foto von Andrea Piacquadio von Pexels Consumers usually seek quality and relevant information when buying new products. With the expansion and availability of the Internet, online consumer reviews have become a valuable resource to look at. Several studies tried to demystify relationship between product sales and online customer reviews. On the one hand, some of them, such as Senecal and Nantel (2004), suggest that participants who consulted product recommendations selected these products twice as often as those who did not consult recommendations. On the other hand, Zhang and Dellarocas (2006) find that online reviews and do not influence sales and serve solely as prediction. Between these two opinion fronts, one thing is certain: both sides aim to find out how consumers perceive and process word-of-mouth in a digital environment. In the academic paper The Role of Emotions for the Perceived Usefulness in Online Customer Reviews authors suggests that emotions impact the helpfulness ratings, i.e., the quality of online reviews as perceived by other customers. They found that, on average, the most prominent emotion dimensions that influence helpfulness ratings are trust, joy, and anticipation. Inspired by these findings, I decided to apply natural language processing techniques to analyze online customer reviews of a bestselling product on Amazon and try to detect those emotions using available lexicons. Final insights will show us whether trust, joy and anticipation can be identified in the reviews, thus improve helpfulness of reviews for potential customers. What to expect in this article? First, I will extract text via web-scrapping and form a corpus. Next, the text in the corpus will be pre-processed. Subsequently, from the pre-processed text will be stored in form of document-term-matrices or term-document matrices. Finally, an exploratory text analysis will be conducted and corresponding marketing implications pointed out. 1.1 Dictionaries for NLP For this exercise I will use 3 different lexicons available for R. One of them is AFINN, a lexicon of words rated for valence between minus five (indicating negative valence) and plus five (indicating positive valence). Next, I will use NRC Emotion Lexicon, which consists of English words and their labels for eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). # Dictionaries ---- afinn &lt;- get_sentiments(&quot;afinn&quot;) bing &lt;- get_sentiments(&quot;bing&quot;) loughran &lt;- get_sentiments(&quot;loughran&quot;) nrc &lt;- get_sentiment_dictionary(&#39;nrc&#39;, language = &quot;english&quot;) 1.2 Data set For our analysis, we will use text of 200 online customer reviews from Apple MacBook Pro (16-inch, 16GB RAM, 512GB Storage, 2.6GHz Intel Core i7) obtained in unpre-processed form: ## [1] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Great product. Replacement for my other macbook that lasted 7 years and was still going strong besides the battery. Great upgrade\\n\\n \\n&quot; ## [2] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Top notch until you don&#39;t use external monitor as well. Its common issue\\n\\n \\n&quot; ## [3] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n This is my first Mac and I love it. Apple is the BEST!\\n\\n \\n&quot; ## [4] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Basically, the laptop is a work of art. You know exactly what youre buying with an Apple product. I love that it comes with 16 GB of memory as default. The video card is very powerful. The touchpad is huge and works flawlessly. It is expensive, but worth every penny.\\n\\n \\n&quot; ## [5] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n A los dos dias de usarlo la pantalla se puso a rayas\\n\\n \\n \\n&quot; ## [6] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Product too heavy. Returned and have not received refund.\\n\\n \\n&quot; ## [7] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n I love this laptop - the screen resolution is beautiful, the light up touch bar is super nice and an innovative touch. I have recently converted form a PC user to MAC after all the years that I have enjoyed other apple products such as Ipad, and I doubt I&#39;d switch back.\\n\\n \\n&quot; ## [8] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n It is well made, Liked it had two positions when opened rather than just one. However, it would have been better if it had three reading positions when opened.\\n\\n \\n&quot; ## [9] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Great laptop! I have been using it for months now and havent had any issues. Highly recommended\\n\\n \\n&quot; ## [10] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n I opened the box and was like, Hello Lover It&#39;s gorgeous.\\n\\n \\n&quot; 1.3 Corpus cleaning From the results above we could see that text contains unnecessary characters. Therefore, I will use some usual procedure to clean up the reviews’ text and make it more understandable. For the purpose of this exercise and for efficiency reasons, we will use the volatile corpus, that stores the collection of documents in RAM memory. To create a volatile corpus, I need to pass reviews’ text in such a form that each review text is interpretated as a document. # Creation of volatile corpus review.corpus &lt;- VCorpus(VectorSource(review$review_text)) We see that the volatile corpus contains as many documents as many online reviews we collected. To undertake a custom transformation, I will use tm package and content_transformer() function. It takes a custom function as input, which defines what transformation needs to be done: review.toSpace&lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;/&quot;) # remove &quot;/&quot; review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;@&quot;) # remove &quot;@&quot; review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;\\\\|&quot;) # remove &quot;\\\\|&quot; review.corpus &lt;- tm_map(review.corpus, content_transformer(tolower)) # convert all capital letters to small review.corpus &lt;- tm_map(review.corpus, removeNumbers) # convert all capital letters to small review.corpus &lt;- tm_map(review.corpus, removeWords, stopwords(&quot;english&quot;)) # remove stop-words review.corpus &lt;- tm_map(review.corpus, removePunctuation) # remove punctuation review.corpus &lt;- tm_map(review.corpus, stripWhitespace) # strip extra whitespace from a document After cleaning the corpus, we can use document-term-matrix to store our cleaned corpus: review.dtm &lt;- DocumentTermMatrix(review.corpus) However, document-term-matrix is not the most suitable to work with, because it stores review texts in rows and terms frequencies in columns. We will transform it with tidy function: # Tidy up the document-term-matrix review.tidy &lt;- tidy(review.dtm) review.tidy$count &lt;-as.numeric(review.tidy$count) # Ensure correct class colnames(review.tidy)[2]&lt;- &#39;word&#39; # change name of the column from &quot;term&quot; to &quot;word&quot; review.tidy$document &lt;- as.numeric(review.tidy$document) # Ensure correct class Our tidy format has dimensions 6907 (the total number of terms) x 3 (document, term and count of the term in corresponding document): dim(review.tidy) # Dimensions ## [1] 6907 3 head(review.tidy)# Display first 6 rows ## # A tibble: 6 x 3 ## document word count ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 battery 1 ## 2 1 besides 1 ## 3 1 going 1 ## 4 1 great 2 ## 5 1 lasted 1 ## 6 1 macbook 1 1.4 Visualisations of terms frequency 1.4.1 Bar charts with the most frequent terms We would be interested in the most frequent words used in customer reviews. Sometimes just a glimpse of the most frequent words is sufficient to get some insights. Here we see that word “love” and “great” appears among most frequent terms. # Most frequent terms ---- review.tdm &lt;- TermDocumentMatrix(review.corpus) review.m &lt;- as.data.frame.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) word.names&lt;-names(review.v) df.review.v&lt;-data.frame(review.v,word.names) colnames(df.review.v)&lt;-c(&quot;n&quot;,&quot;word&quot;) p&lt;-ggplot(data=df.review.v[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;steelblue&quot;) + coord_flip() + ggtitle(&quot;20 most frequent words in customer reviews - MacBook Pro&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_test() ggplotly(p) 1.4.2 Wordcloud with the most frequent terms Similarly to the bar chart with the most frequent words, we could use wordcloud as well. It displays words from the corpus and signalizes their frequency by displaying more frequent words bigger relative to those that appear less frequently in the corpus. In the wordcloud below you can see 200 most frequent words, where the minimum frequency was set to 1. # Wordcloud review.tdm &lt;- TermDocumentMatrix(review.corpus) review.m &lt;- as.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) review.d &lt;- data.frame(word = names(review.v),freq=review.v) set.seed(1234) wordcloud(words = review.d$word, freq = review.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) 1.4.3 The most frequent terms indicating emotions When it comes to anticipation, words such as “good”,“time”,“happy” or “powerful” indicates that this emotion can be identified among customer reviews. On the other hand, there are some words that could be a signal both for good and bad experience: “finally”,“money” or “wait”. # Anticipation words---- nrc.anticipation &lt;- subset(nrc, nrc$sentiment==&quot;anticipation&quot;) review.anticipation.words &lt;- inner_join(review.tidy, nrc.anticipation) review.anticipation.words &lt;- count(review.anticipation.words, word) review.anticipation.words &lt;- review.anticipation.words[order(review.anticipation.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.anticipation.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;orange&quot;) + coord_flip() + ggtitle(&quot;20 most frequent anticipation words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) Similarly to anticipation, now we observe a list of top 20 words that indicate trust. It reveals new quite frequent term in the corpus: “recommend”. # Trust words---- nrc.trust &lt;- subset(nrc, nrc$sentiment==&quot;trust&quot;) review.trust.words &lt;- inner_join(review.tidy, nrc.trust) review.trust.words &lt;- count(review.trust.words, word) review.trust.words &lt;- review.trust.words[order(review.trust.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.trust.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;royalblue1&quot;) + coord_flip() + ggtitle(&quot;20 most frequent trust words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) Although at the bottom of the list, “The top 20 list” of joy words displays some additional words that we did not observe previously such as “beautiful”,“gorgeous”,“wonderful”,“improvement”,“excellent”. # Joy words ---- nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) review.joy.words &lt;- inner_join(review.tidy, nrc.joy) review.joy.words &lt;- count(review.joy.words, word) review.joy.words &lt;- review.joy.words[order(review.joy.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.joy.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;darkorange1&quot;) + coord_flip() + ggtitle(&quot;20 most frequent trust words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) 1.5 Sentiment analysis 1.5.1 Polarity timeline One usual way to compare and quantify emotions in text is via polarity. We simply count number of unique words in each document (=review) labelled as negative and deduct from the count of unique positive words. For instance, the first review contains 2 unique positive words (“great” and “strong”) and none negative unique words. Therefore, its polarity score is 2. This polarity timeline suggests very important implication: the reviews’ sentiment is moving above the 0, bearly going even below +2, giving an indication that this product continuously meet customers’ expectations. That is a good signal to believe that customers are rather satisfied with the product. # Polarity timeline ---- review.sentiment &lt;- inner_join(review.tidy, bing) review.sentiment &lt;- count(review.sentiment, sentiment, index=document) review.sentiment &lt;- spread(review.sentiment, sentiment, n, fill=0) review.sentiment$polarity &lt;- review.sentiment$positive - review.sentiment$negative review.sentiment$pos &lt;- ifelse(review.sentiment$polarity &gt;=0, &quot;Positive&quot;, &quot;Negative&quot;) p&lt;-ggplot(review.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1)+theme_gdocs()+ggtitle(label=&quot;Polarity timeline&quot;) ggplotly(p) # Smooth curve review.smooth &lt;- ggplot(review.sentiment, aes(index, polarity)) p&lt;-review.smooth + stat_smooth() + theme_gdocs() + ggtitle(&quot;Polarity timeline - smooth&quot;) ggplotly(p) In the polarity graph at index 81 we identify a review with sentiment score of even 34! This seems to be a thrilled customer every brand loves! Let us take closer look: review.sentiment &lt;- inner_join(review.tidy, bing) doc_81&lt;-filter(review.sentiment, document==&quot;81&quot;) head(doc_81[order(doc_81$count,decreasing = T),]) ## # A tibble: 6 x 4 ## document word count sentiment ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 81 best 3 positive ## 2 81 better 3 positive ## 3 81 improved 3 positive ## 4 81 amazing 2 positive ## 5 81 breeze 2 positive ## 6 81 good 2 positive Finally, it certainly pays off to check the actual review: # Outlier in polarity score review$review_text[81] ## [1] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n If you&#39;ve been waiting for Apple to wake up and address the concerns raised by the Apple community -- your wait is now over. This is the MacBook Pro we&#39;ve all been wanting for years. This review is for the higher end model, stock.The good.- Keyboard. It&#39;s fantastic. The key travel has been extended to 1mm, which is about half of the original keyboard found on the 2015 and prior model years. It feels just as good to type on because apple improved the tactile feedback. The keys sort of spring back. The keyboard is quiet and very comfortable to type on. The keys are slightly smaller but do not make typing on them any more difficult. The directional arrows are now properly setup and the physical Esc key is back, making it a breeze to flip through open applications.- Screen. The difference in size is subtle but noticeable. It&#39;s technically even more dense, but just barely. You won&#39;t notice much difference from recent MacBook Pros but true to Apple, the display is absolutely gorgeous. The 0.6 inch difference in size retains the same logical resolution, so everything should look just slightly bigger which I welcome.- Processor. The base model has the same chipset as the 2019 15.4\\&quot; model. The performance, however, is about 10% better due to significantly improved airflow and larger heatsinks. The issue of heat related throttling has been largely addressed. On the upper model, the machine now comes with 8 cores and serious performance bump. There is literally nothing you can throw at this MacBook that it won&#39;t handle with breeze.- Graphics. The traditional setup is still here. You have the onboard Intel chipset, which operates when the demand for visual performance is low. You also have a discrete AMD chip which delivers significant improvement over the previous generation. In fact, the base model delivers performance in excess of the upper spec Vega chipset from last year. The leap is extraordinary. As before, the system will automatically choose which graphics card to use depending on demand.- Sound. In one world, amazing. Imagine Apple took a HomePod and flattened it to fit it in the housing of the MacBook Pro. That is essentially the experience. Bass is pronounced and crisp and treble is sharp. The sound is rich and room-filling. There are six speakers instead of two in the last generation.-Microphone. Major improvements with three mics instead of one in the previous generation. I use the MacBook to make calls using an iPhone and the sound on the other end is clear and free from background noice. I&#39;ve been told it sounds a lot better than before, but that is of course subjective. On paper, you&#39;re getting better noise reduction and improved sound fidelity.- Touch Bar. Controversial in the past, I think it may now be the \\&quot;happy medium\\&quot; between physical keys and the useful Touch Bar which adapts to the content on the screen. The Esc key is back and on the right hand side you&#39;ll find Touch ID and power button.- SSD. You&#39;ll love the fact that now base starting size of the SSD has been doubled on both the entry model and the upper model with 512GB and 1TB respectively.- Gaming. This has to be mentioned. The graphics card offers incredible leap in performance. Modern games that would get 14-16FPS on high setting now perform at 35-40FPS with ease. Same settings. Same games. Huge improvement. It&#39;s now possible to play AAA games on the MacBook Pro with reasonable performance and high visual settings.- Productivity. This machine is a beast. I use the full Affinity suite and do some limited video editing. In addition, I have multiple productivity programs open, over a dozen Safari tabs, two email clients, and dozens of other apps, such as CRM, task managers, notes, etc. Everything runs smoothly.- Value. Yes, value. This expensive laptop brings the best value in the lineup of the 15.\\&quot;4 (now 16\\&quot;) offering to date. If you carefully look at the costs of truly compatible Windows offerings, you&#39;ll find the MacBook Pro to be competitively priced.The Bad- Weight. The machine is slightly heavier but I welcome the added bulky. Finally Apple went with functionality over its obsession with thiner and lighter hardware, giving us a machine with proper thermal management, proper keyboard and more. But if you plan on taking it with you places, you&#39;ll feel the extra bulk.- BTO Updates. The cost of BTO options is still quite high, with the noted exception of the 8GB option on the GPU. You&#39;re still paying a significant premium for each incremental upgrade over the base model.To sum up -- This is the best MacBook Pro in many years. It&#39;s a well-rounded, powerful machine that brings about incredible performance and value. I would highly recommend it to any Pro.If you&#39;re upgrading from the 13\\&quot; model and wonder how much more real estate you can expect, see attached side-by-side comparison photo.Update 3/15/20 -- Absolutely a beast of a machine. I love it. It has been pretty much perfect and its performance continues to surprise me. I have the top-speced model and it smokes pretty much everything I&#39;ve used to date. In fact, it will perform on par with the new Mac Pro base configuration. Don&#39;t believe me? Google it. This is by far the best MacBook Pro to date and an amazing value. Well worth the price of admission if you need it.\\n\\n \\n&quot; It seems that our assumption was correct! The customer was definitely thrilled! This is a nice example how you can identify and take closer look at reviews that stand out based on its polarity score. 1.5.2 Analysis on sentence-level Text analysis provides freedom to choose level of observation. So far, we explored words and their frequencies, we explored customer reviews and quantified their sentiment in two dimensions (positive and negative). Next, we will approach the task of identifying the most negative and positive reviews by organizing text by sentences. By doing so, we will directly access those sentences whose average sentiment stand out. # Calculating the average sentiment review.highlighted&lt;-review$review_text%&gt;% get_sentences() %&gt;% sentiment_by() head(review.highlighted) ## element_id word_count sd ave_sentiment ## 1: 1 2 NA 0.35355339 ## 2: 2 16 NA 0.12500000 ## 3: 3 2 NA 0.35355339 ## 4: 4 10 NA 0.06324555 ## 5: 5 3 NA -0.57735027 ## 6: 6 9 NA 0.25000000 # Preparing data review.score &lt;- subset(review.highlighted, select = c(&quot;ave_sentiment&quot;,&quot;element_id&quot;)) review.worst &lt;- review.score[order(review.score$ave_sentiment,decreasing = FALSE),] review.worst&lt;-review.worst$element_id[1:10] review.best &lt;- review.score[order(review.score$ave_sentiment, decreasing = TRUE),] review.best &lt;- review.best$element_id[1:10] sentences&lt;-review$review_text %&gt;% get_sentences() sentences&lt;-as.matrix(sentences) And here we have “the worst 10 sentences” from customer reviews; # 10 worst sentences sentences[review.worst] ## [1] &quot;The screen is nice but has ridiculous ghosting.&quot; ## [2] &quot;Very disappointed.&quot; ## [3] &quot;Force cancelling woofers keep the annoying vibration at louder playback at bay.&quot; ## [4] &quot;I am very disappointed!&quot; ## [5] &quot;Having ports would have been nice (especially seeing how much space would be available for ports) but unfortunately, that is how Apple operates.&quot; ## [6] &quot;This is a Finder issue, as apps launch but don&#39;t leap to the top.&quot; ## [7] &quot;Im glad I didnt have important files to transfer or I would have been upset.&quot; ## [8] &quot;I guess I got a lemon, which is disappointing considering the high cost.&quot; ## [9] &quot;Small issue: fan comes up too often ( I do application development with it).&quot; ## [10] &quot;Very suspicious batch.&quot; Despite the fact that positive sentiment prevails, we see that there are certain problems associated with MacBook laptop. Issues with screen, problems with woofers, disappointment that there are no ports, unsatisfying value-price ratio. # 10 most positive sentences sentences[review.best] ## [1] &quot;Yes its considerably big and heavy compared to others, but Honestly that just feels like better quality to me&quot; ## [2] &quot;Quality is very good.&quot; ## [3] &quot;Extremely pleased with my NEW MacBook Pro!&quot; ## [4] &quot;This MacBook Pro 16\\&quot; is outstanding and much much better than the previous generation.&quot; ## [5] &quot;Overall, this new MacBook is rather pricey, but there&#39;s a reason it is called \\&quot;Pro\\&quot; because it is designed for professional.&quot; ## [6] &quot;Very fast!&quot; ## [7] &quot;Very fast, great screen, fantastic keyboard and wonderful speakers.&quot; ## [8] &quot;If you&#39;re upgrading from the 13\\&quot; model and wonder how much more real estate you can expect, see attached side-by-side comparison photo.&quot; ## [9] &quot;Well made computerBest specsNo much innovation from the 15 MacBook Pro that I Have.&quot; ## [10] &quot;I absolutely love this new Mac!&quot; If we take a look at “10 most positive sentences” from customer reviews, we would find a similar evidence as we obtained with polarity score. However, by reading those sentences a reader can have better feeling what the reviewer is actually satisfied or unsatisfied with. Here we see that some people admire the speed for instance. 1.5.3 What are the most emotional reviews? Package sentimentr provides nice function emotion() which uses a dictionary to find emotion words and then compute the rate per sentence. The final emotion score ranges between 0 (no emotion used) and 1 (all words used were emotional). # Extract emotions terms reviews.emotion &lt;- review$review_text %&gt;% get_sentences() %&gt;% emotion() # Top 50 sentences with the highest emotion score top_emotional_sentences &lt;- unique(reviews.emotion[order(reviews.emotion$emotion,decreasing = TRUE),]$element_id[1:50]) # The most emotional reviews sentences[top_emotional_sentences,] ## [1] &quot;Frustrated!&quot; &quot;PERFECT!!!!&quot; &quot;Finally!&quot; ## [4] &quot;Hot damn.&quot; &quot;Hot.&quot; &quot;Damn.&quot; ## [7] &quot;LOVE LOVE LOVE it!&quot; &quot;The Bad- Weight.&quot; &quot;Highly recommend.&quot; ## [10] &quot;Highly recommend.&quot; &quot;Outstanding laptop.&quot; &quot;Excellent computer&quot; ## [13] &quot;Loving it!&quot; &quot;Good computer.&quot; &quot;good product&quot; ## [16] &quot;Huge improvement.&quot; &quot;So disappointed.....&quot; &quot;Very disappointed.&quot; ## [19] &quot;so disappointed.&quot; &quot;Good keyboard.&quot; &quot;Ridiculously good.&quot; We can see that identified sentences very clearly reflect emotions that customers expressed. It seems that intensity of emotions is high in both positive and negative direction. Finally, we can plot detected emotions in order to get a bit more clear insight in emotional structure detected in the reviews: # Plot of emotion p&lt;-plot(reviews.emotion, transformation.function = syuzhet::get_dct_transform, drop.unused.emotions = TRUE, facet = TRUE) ggplotly(p) This plot depicts emotional inclination of the reviews. Curves indicating emotional propensity of trust, joy and anticipation suggest strong inclination towards mentioned emotions. In line with the academic paper The Role of Emotions for the Perceived Usefulness in Online Customer Reviews, we have found an evidence that online reviews analyzed encode emotions contributing to the higher helpfulness rating, i.e. quality of reviews. In the end, it is important to mention that the role and influence of emotions defined by Plutchik on the quality of reviews differ across different product categories. "],
["twitter-analysis-for-co2mustgo-initiative.html", "2 Twitter Analysis for CO2mustGo initiative What is CO2mustGO? 2.1 Sankey diagram: Energy production capacity Twitter data analysis 2.2 Text cleaning and preprocessing 2.3 Tweets distribution 2.4 Word Frequency in Tweets 2.5 Word Network in Tweets 2.6 Wordcloud 2.7 Sentiment analysis 2.8 Visualize the emotions 2.9 Top retweeted Tweets 2.10 Network of retweets", " 2 Twitter Analysis for CO2mustGo initiative What is CO2mustGO? Figure 2.1: Foto von Andrea Piacquadio von Pexels At the beginning of 2020 I heared that my working colleagues from another institute at the WU have launched an initiative called CO2mustGO initiative. The initiative aims to gather multinational group of students, researchers and teachers from the Vienna University of Economics and Business and other universities around Europe, to unite around the single issue of carbon price. This project started as a university course, with the vision of uniting students and scientists in an international movement supporting every serious carbon price initiative globally. Since my hometown, Tuzla, heavily suffers from the air pollution, I felt that I should give my contribution to this initiative and joined them in April 2020. I decided to support it with my R programming skills. Consequently, I ended up analyzing Twitter data, as it is one of the hot-spots for this topic. My role was to spark interest of stakeholders regarding this issue by using data available. Therefore, my tasks were related to using R to create understandable visualizations and to make use of Twitter data available. 2.1 Sankey diagram: Energy production capacity # Creation of source table$typ_11 &lt;- factor(table$typ_11,levels = c(&quot;c1&quot;,&quot;c2&quot;,&quot;g1&quot;,&quot;g2&quot;,&quot;g3&quot;,&quot;g4&quot;,&quot;n&quot;,&quot;o1&quot;,&quot;o2&quot;,&quot;o3&quot;,&quot;o4&quot;,&quot;r1&quot;,&quot;res&quot;),labels = c(&quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;,&quot;11&quot;,&quot;12&quot;)) source1 &lt;- as.vector.factor(table$typ_11) source1 &lt;- as.numeric(source1) #Country as mediator table2 &lt;- table table2$country&lt;-as.character(table2$country) table2$country[table$country==&quot;AT&quot;] &lt;- 14 table2$country[table$country==&quot;BE&quot;] &lt;- 15 table2$country[table$country==&quot;BG&quot;] &lt;- 16 table2$country[table$country==&quot;CH&quot;] &lt;- 17 table2$country[table$country==&quot;CY&quot;] &lt;- 18 table2$country[table$country==&quot;CZ&quot;] &lt;- 19 table2$country[table$country==&quot;DE&quot;] &lt;- 20 table2$country[table$country==&quot;DK&quot;] &lt;- 21 table2$country[table$country==&quot;EE&quot;] &lt;- 22 table2$country[table$country==&quot;ES&quot;] &lt;- 23 table2$country[table$country==&quot;FI&quot;] &lt;- 24 table2$country[table$country==&quot;FR&quot;] &lt;- 25 table2$country[table$country==&quot;GR&quot;] &lt;- 26 table2$country[table$country==&quot;HR&quot;] &lt;- 27 table2$country[table$country==&quot;HU&quot;] &lt;- 28 table2$country[table$country==&quot;IE&quot;] &lt;- 29 table2$country[table$country==&quot;IT&quot;] &lt;- 30 table2$country[table$country==&quot;LT&quot;] &lt;- 31 table2$country[table$country==&quot;LU&quot;] &lt;- 32 table2$country[table$country==&quot;LV&quot;] &lt;- 33 table2$country[table$country==&quot;NL&quot;] &lt;- 34 table2$country[table$country==&quot;NO&quot;] &lt;- 35 table2$country[table$country==&quot;PL&quot;] &lt;- 36 table2$country[table$country==&quot;PT&quot;] &lt;- 37 table2$country[table$country==&quot;RO&quot;] &lt;- 38 table2$country[table$country==&quot;SE&quot;] &lt;- 39 table2$country[table$country==&quot;SI&quot;] &lt;- 40 table2$country[table$country==&quot;SK&quot;] &lt;- 41 table2$country[table$country==&quot;UK&quot;] &lt;- 42 target1&lt;- table2$country #Creation of mediator(country),source and value value &lt;- table2 %&gt;% group_by(country,typ_11) %&gt;% count(mw_2018) #1st part of Target target1 &lt;- value$country target1&lt;-as.numeric(target1) #1st part of Value value1&lt;-value$mw_2018 #Creation of the final target value.df &lt;-table2 %&gt;% group_by(country)%&gt;% summarise(sum(mw_2018)) value.df$country&lt;-as.numeric(value.df$country) #2nd part of Source(from countries to total capacity) target1.2 &lt;-value.df$country #2nd part of Values (total of countries to the grand total) value1.2&lt;-value.df$`sum(mw_2018)` #Final target ft&lt;-rep(13,29) final_target &lt;- c(target1,ft) #final source final_soruce &lt;-c(source1,target1.2) #final value final_value &lt;-c(value1,value1.2) fig &lt;- plot_ly( type = &quot;sankey&quot;, orientation = &quot;h&quot;, valuesuffix = &quot;MW&quot;, arrangement = &quot;snap&quot;, node = list( label = c( &quot;Coal-lignite&quot;, # Node 0 &quot;Coal-hard&quot;, # Node 1 &quot;G1-gas&quot;, # Node 2 &quot;G2-gas&quot;, # Node 3 &quot;G3-gas&quot;, # Node 4 &quot;G4-gas&quot;, # Node 5 &quot;Nunclear&quot;, # Node 6 &quot;O1-oil&quot;, # Node 7 &quot;O2-oil&quot;, # Node 8 &quot;O3-oil&quot;, # Node 9 &quot;O4-oil&quot;, # Node 10 &quot;Renewables&quot;,# Node 11 &quot;Hydro&quot;, # Node 12 &quot;Total capacity&quot;, # Node 13 &quot;AT&quot;,#Node 14 &quot;BE&quot;,#Node 15 &quot;BG&quot;,#Node 16 &quot;CH&quot;,#Node 17 &quot;CY&quot;,#Node 18 &quot;CZ&quot;,#Node 19 &quot;DE&quot;,#Node 20 &quot;DK&quot;,#Node 21 &quot;EE&quot;,#Node 22 &quot;ES&quot;,#Node 23 &quot;FI&quot;,#Node 24 &quot;FR&quot;,#Node 25 &quot;GR&quot;,#Node 26 &quot;HR&quot;,#Node 27 &quot;HU&quot;,#Node 28 &quot;IE&quot;,#Node 29 &quot;IT&quot;,#Node 30 &quot;LT&quot;,#Node 31 &quot;LU&quot;,#Node 32 &quot;LV&quot;,#Node 33 &quot;NL&quot;,#Node 34 &quot;NO&quot;,#Node 35 &quot;PL&quot;,#Node 36 &quot;PT&quot;,#Node 37 &quot;RO&quot;,#Node 38 &quot;SE&quot;,#Node 39 &quot;SI&quot;,#Node 40 &quot;SK&quot;,#Node 41 &quot;UK&quot;)),#Node 42 link = list( source = final_soruce, target = final_target, value = final_value)) total.capacity &lt;- fig %&gt;% layout(title = &quot;Energy Production Capacity in the EU&quot;) total.capacity Figure 2.2: Energy production capacities in the EU In order to depict the nature of energy production facilities across EU countries, I created this sankey diagram. On the left side, you can see types of energy sources across the EU countries (data is not updated). In the middle, you can see respective countries. Finally, on the right you see the total of energy production. Especially nice feature of the visualization is the interactive component, so that, for instance, using “Box select” option, you can merge certain number of components of the same type (e.g., select 3 sources of energy) and the visualization changes accordingly. Twitter data analysis Twitter is nowadays among platforms that host strong communities. Carbon footprint and its repercussions belongs to prominent climate issue. Consequently, I decided to analyze Tweets downloaded via Twitter API on on 10.06.2020. The aim was simply to explore data about tweets with hashtags #carbonfootprint or #co2 and try to draw conclusions on how to approach social media presence on Twitter. 2.1.1 Data Our data was stored in CSV format.rtweet package provides nice function to read in data conveniently. #library(rtweet) # Read in Tweets data carbon_tweets &lt;- read_twitter_csv(&quot;data/#carbonfootprintOR#greenhouse-tweets.csv&quot;, unflatten = T) # Delete empty columns carbon_tweets &lt;- carbon_tweets[, colSums(is.na(carbon_tweets)) != nrow(carbon_tweets)] # Head of the data set dim(carbon_tweets) ## [1] 1858 86 Our data set contains 1858 Tweets and 86 features. 2.2 Text cleaning and preprocessing Tweets text in our data set requires some preprocessing and cleaning as it contains elements that are not helpful for our analysis. # Text cleaning carbon_tweets$stripped_text &lt;- gsub(&quot;https\\\\S*&quot;,&quot;&quot;, carbon_tweets$text) carbon_tweets$stripped_text &lt;- gsub(&quot;@\\\\S*&quot;,&quot;&quot;, carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;amp&quot;,&quot;&quot;,carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;[\\r\\n]&quot;,&quot;&quot;,carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;[[:punct:]]&quot;, &quot;&quot;,carbon_tweets$stripped_text) # Text to lowercase, punctuation removed, frequency of the each word added carbon_tweets_clean &lt;- carbon_tweets %&gt;% dplyr::select(stripped_text) %&gt;% unnest_tokens(word, stripped_text) # Remove stop words data(&quot;stop_words&quot;) carbon_tweets_words &lt;- carbon_tweets_clean %&gt;% anti_join(stop_words) 2.3 Tweets distribution Since our data set contains information about the time Tweets are posted, I was interested to see the distribution of Tweets in the given period. # Distribution of tweets considered in the data. search_term &lt;- &#39;#carbonfootprint OR #co2&#39; by &lt;- &#39;hour&#39; p &lt;- ts_plot(carbon_tweets, by = by, trim = 2) + geom_point(col = &quot;#00acee&quot;) + theme_minimal() + labs(title = paste0(&quot;Tweets with &quot;,search_term,&quot; by &quot;,by),x = &#39;Date&#39;, y = &#39;Count&#39;) ggplotly(p) The period captured in our data set is from 15:00 on 2nd of June to 11:00 A.M. on 10th of June. Interestingly enough, there was certain occasion on 6th of June important for our topic. We see from the number of Tweets with hashtags #carbonfootprint or #co2, which stood at 43! This happening has to be more closely analyzed. 2.4 Word Frequency in Tweets Next, I wanted to inspect the word frequency in the Tweets from the data set. p &lt;- carbon_tweets_words %&gt;% dplyr::count(word, sort=T) %&gt;% top_n(10) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x = word, y = n)) + geom_col(fill=&quot;deepskyblue&quot;) + theme_minimal()+ xlab(NULL) + coord_flip() + labs(x = &quot;Count&quot;, y = &quot;Unique words&quot;, title = &quot;Count of unique words found in tweets&quot;) ggplotly(p) Generally, no major surprises regarding the most frequent words. It can be noted that term “worldenvironmentday” appears frequently as the World Environment Day is on 5th of June. 2.5 Word Network in Tweets Knowing that Tweets are short messages, I decided to inspect word network in order to possibly observe some unusual word combinations. The word network is made based on bi-grams. Basically, based on the number of times two words shows up together. # Remove punctuation, convert to lowercase, add id for each tweet! carbon_tweets_paired_words &lt;- carbon_tweets %&gt;% dplyr::select(stripped_text) %&gt;% unnest_tokens(paired_words, stripped_text, token = &quot;ngrams&quot;, n = 2) #library(tidyr) carbon_tweets_separated_words &lt;- carbon_tweets_paired_words %&gt;% separate(paired_words, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) carbon_tweets_filtered &lt;- carbon_tweets_separated_words %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) carbon_words_counts &lt;- carbon_tweets_filtered %&gt;% dplyr::count(word1, word2, sort = TRUE) #library(igraph) #library(ggraph) # Plot carbon change word network p&lt;- carbon_words_counts %&gt;% filter(n &gt;=30) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = 0.5, edge_width = n)) + geom_node_point(color = &quot;darkslategray4&quot;, size = 3) + geom_node_text(aes(label = name), vjust = 1.8, size = 3) + labs(title = &quot;Word Network: Tweets using the hashtag #carbonfootprint or #co2&quot;, subtitle = &quot;Text mining twitter data &quot;, x = &quot;&quot;, y = &quot;&quot;) p An interesting observation is that the word network shows the word “justiceforvinayaki” appering together with “climatecrisis”. More specifically, “justiceforvinayaki” is actually a hashtag related to the story behind the pregnant elephant’s killing in Kerala’s Palakkad. More you can read here. 2.6 Wordcloud In order to get a bigger picture of frequent terms in Tweets, I created a wordcloud. # Text preparation carbon_tweets$stripped_text &lt;- iconv(carbon_tweets$stripped_text, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) review.docs &lt;- Corpus(VectorSource(carbon_tweets$stripped_text)) review.toSpace&lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;/&quot;) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;@&quot;) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;\\\\|&quot;) review.docs &lt;- tm_map(review.docs, content_transformer(tolower)) review.docs &lt;- tm_map(review.docs, removeNumbers) review.docs &lt;- tm_map(review.docs, removeWords, stopwords(&quot;english&quot;)) review.docs &lt;- tm_map(review.docs, content_transformer(tolower)) review.docs &lt;- tm_map(review.docs, removePunctuation) review.docs &lt;- tm_map(review.docs, stripWhitespace) review.tdm &lt;- TermDocumentMatrix(review.docs) review.m &lt;- as.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) review.d &lt;- data.frame(word = names(review.v),freq=review.v) set.seed(1234) wordcloud(words = review.d$word, freq = review.d$freq, max.words = 200, min.freq = 10, random.order=FALSE, rot.per=0.15, colors=brewer.pal(8, &quot;Dark2&quot;), scale=c(8,.3), vfont=c(&quot;sans serif&quot;,&quot;plain&quot;)) Minimum word frequency is set to 10, and the wordcloud depicts 200 most frequent terms. As you probably already know, Twitter is a place where discussions are going on frequently. Therefore, it made sense to try to reorganize the wordcloud to indicate positive vs negative terms. library(wordcloud) library(reshape2) par(mar = rep(0, 4)) set.seed(1234) p&lt;-carbon_tweets_words%&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment,sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;darkred&quot;,&quot;darkgreen&quot;), max.words = 400, min.freq= 10, scale = c(4.0,0.25)) p 2.7 Sentiment analysis Knowing the nature of the carbon footprint related topics and based on the previous assumption about vivid discussions on Twitter, an analysis of emotions in Tweets would help us in opinion mining. # Sentiment analysis sentiment &lt;- carbon_tweets[,3:5] %&gt;% unnest_tokens(output = &#39;word&#39;, input = &#39;text&#39;) #Add sentiment dataset sentiment_dataset &lt;- get_sentiments(&quot;afinn&quot;) sentiment_dataset &lt;- arrange(sentiment_dataset, -value) #Merge sentiment &lt;- merge(sentiment, sentiment_dataset, by = &#39;word&#39;) #Clean sentiment$word &lt;- NULL sentiment$screen_name &lt;- NULL #Time sentiment$hour &lt;- format(base::round.POSIXt(sentiment$created_at, units=&quot;hours&quot;), format=&quot;%H:%M&quot;) #Pivot pivot &lt;- sentiment %&gt;% group_by(hour) %&gt;% summarise(sentiment = mean(value)) #Plot p &lt;- ggplot(pivot[-1,], aes(x = hour, y = sentiment)) + geom_line(group = 1, color=&quot;deepskyblue&quot;) + geom_point() + theme_minimal() + labs(title = paste0(&#39;Average sentiment of tweetings mentioning &quot;&#39;,search_term,&#39;&quot;&#39;),x = &#39;Date&#39;, y = &#39;Sentiment&#39;, caption = &#39;Source: Twitter API&#39;) ggplotly(p) The visualisation above depicts the average sentiment score of Tweets during a day. It seems that at 10:00 and 15:00 Tweets tend to be less positive than at 17:00 for instance. This information helps for scheduling Tweets so that they don’t get caught in the “bad moment”. 2.8 Visualize the emotions In order to decide what sort of discussion was going on the period of observation, we can visualise the count of words indicating emotions. # Get sentiments using the four different lexicons syuzhet &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;syuzhet&quot;) bing &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;bing&quot;) afinn &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;afinn&quot;) nrc &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;nrc&quot;) sentiments &lt;- data.frame(syuzhet, bing, afinn, nrc) # get the emotions using the NRC dictionary nrc.sentiment &lt;- get_nrc_sentiment(carbon_tweets$stripped_text) emo_bar = colSums(nrc.sentiment) emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar)) emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)]) # Visualize the emotions from NRC sentiments plot_ly(emo_sum, x=~emotion, y=~count, type=&quot;bar&quot;, color=~emotion) %&gt;% layout(xaxis=list(title=&quot;&quot;), showlegend=FALSE, title=&quot;Distribution of emotion categories&quot;) It seems that positivity, trust, anticipation and joy are far more present in Tweets than emotions usually associated with something negative. Again, we can organize wordcloud so that it does not show just positive or negative words, but rather words associated with corresponding emotion. # Comparison word cloud all = c( paste(carbon_tweets$stripped_text[nrc.sentiment$anger &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$anticipation &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$disgust &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$fear &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$joy &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$sadness &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$surprise &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$trust &gt; 0], collapse=&quot; &quot;) ) all &lt;- removeWords(all, stopwords(&quot;english&quot;)) # create corpus corpus = Corpus(VectorSource(all)) # # create term-document matrix tdm = TermDocumentMatrix(corpus) # # convert as matrix tdm = as.matrix(tdm) tdm1 &lt;- tdm[nchar(rownames(tdm)) &lt; 11,] # # add column names colnames(tdm) = c(&#39;anger&#39;, &#39;anticipation&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;sadness&#39;, &#39;surprise&#39;, &#39;trust&#39;) colnames(tdm1) &lt;- colnames(tdm) comparison.cloud(tdm1, random.order=FALSE, colors = c(&quot;#00B2FF&quot;, &quot;red&quot;, &quot;#FF0099&quot;, &quot;#6600CC&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;blue&quot;, &quot;brown&quot;), title.size=1, max.words=250, scale=c(2.5, 0.4),rot.per=0.4) 2.9 Top retweeted Tweets Top retweets (with equal or more than 60 mentions) Next task was to identify Tweets that stand out. The graph below helped me to identify days on which certain Tweets were re-tweeted substaintially more than usually. # Select top retweeted tweets selected &lt;- which(carbon_tweets$retweet_count &gt;= 60) # Plot dates &lt;-as.POSIXct(strptime(carbon_tweets$created_at, format=&quot;%Y-%m-%d&quot;)) plot(x=dates, y=carbon_tweets$retweet_count, type=&quot;l&quot;, col=&quot;grey&quot;, xlab=&quot;Date&quot;, ylab=&quot;Times retweeted&quot;) colors &lt;- rainbow(10)[1:length(selected)] points(dates[selected], carbon_tweets$retweet_count[selected], pch=19, col=colors) Interactive graph with retweets’ text In this interactive graph I manage to identify Tweets with their text that were re-tweeted frequently. If you hover over big sky-blue points you will see the actual text of each Tweet. # Plotly carbon_tweets$created_at &lt;-as.POSIXct(strptime(carbon_tweets$created_at, format=&quot;%Y-%m-%d&quot;)) p&lt;-ggplot(carbon_tweets, aes(x=created_at, y=retweet_count, col=retweet_count, size=retweet_count, retweet_text=retweet_text, created_at=created_at, retweet_name=retweet_name))+geom_point() +xlab(label=&quot;Date&quot;)+ylab(label=&quot;Retweet count&quot;)+ggtitle(label=&quot;Top retweeted tweets&quot;) ggplotly(p,tooltip = c(&quot;retweet_text&quot;,&quot;retweet_name&quot;)) The most re-tweeted Tweet comes from Forest Products Resolute, which is a global leader in the forest products industry with a diverse range of products, including market pulp, tissue, wood products and papers, which are marketed in close to 70 countries. They tweeted that they are going to take decisive actions towards lowering carbon footprint. 2.10 Network of retweets Finally, I decided to explore the network of the Tweets in order to identify some communities on Twitter that the initiative could potentially join. # Create data frame for the network rt_df &lt;- carbon_tweets[, c(&quot;screen_name&quot; , &quot;retweet_screen_name&quot; )] # Remove rows with missing values rt_df_new &lt;- rt_df[complete.cases(rt_df), ] # Convert to matrix matrx &lt;- as.matrix(rt_df_new) # Create the retweet network nw_rtweet &lt;- graph_from_edgelist(el = matrx, directed = TRUE) # View the retweet network print.igraph(nw_rtweet) ## IGRAPH a3dd67f DN-- 1100 936 -- ## + attr: name (v/c) ## + edges from a3dd67f (vertex names): ## [1] Amrapali_c -&gt;cathrinejahnsen hanopcan -&gt;GreenTech_SWest ## [3] MehmetO33440789-&gt;UKHaulier m_carmody -&gt;crowdfarmingco ## [5] amrendrakumar02-&gt;SUPERGASind abhishekk85 -&gt;theswitchfix ## [7] MarkCNorwich -&gt;52WeeksForEarth RPiUptime -&gt;TrafficlyApp ## [9] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [11] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [13] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [15] RPiUptime -&gt;TrafficlyApp madebyhyphae -&gt;GreenTech_SWest ## + ... omitted several edges ## [1] 828 2 2.10.1 Where do people tweet the most? The interactive world-map plot below allows you to zoom-in certain regions to have a better look what countries are hot-spots when it comes to carbon footprint and co2-related topics. # Extract geolocation data and append new columns library(rtweet) library(sf) library(rnaturalearth) library(rnaturalearthdata) library(rgeos) pol_coord &lt;- lat_lng(carbon_tweets) pol_geo &lt;- na.omit(pol_coord[, c(&quot;lat&quot;, &quot;lng&quot;,&quot;location&quot;,&quot;retweet_count&quot;)]) world &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) p&lt;-ggplot(data = world) + geom_sf() + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + geom_point(data= pol_geo,aes(x=lng, y=lat,loc=location,retweet_count=retweet_count),col = &quot;#00acee&quot;)+ theme(panel.grid.major = element_line(color = gray(.25), linetype =&quot;dashed&quot;, size = 0.15),panel.background = element_rect(fill = &quot;aliceblue&quot;))+ ggtitle(&quot;World map with tweets location and retweet count&quot;, subtitle = paste0(&quot;(&quot;, length(unique(pol_geo$location)), &quot; countries)&quot;)) ggplotly(p,tooltip = c(&quot;location&quot;,&quot;retweet_count&quot;)) 2.10.2 Who are the users who retweet the most? # Calculate the out-degree scores out_degree &lt;- degree(nw_rtweet, mode = c(&quot;out&quot;)) # Sort the users in descending order of out-degree scores out_degree_sort &lt;- sort(out_degree, decreasing = TRUE) head(out_degree_sort,10) ## imagine_garden RPiUptime taxa_monocot Eco1stArt greenhousedave ## 9 8 7 5 5 ## pepparsteve LazarovMartin7 PetrovichBilly greentechdon researchmrx ## 4 4 4 4 3 # INTERPRETATION: Users who retweeted the most. #Hubs: Tweeter accounts with a lot of outgoing edges. hs &lt;- hub_score(nw_rtweet, weights=NA)$vector sort(hs, decreasing = TRUE)[1:20] ## SaachinPatel virendrathor007 ModheraAjay YuvraajsO mahisawOfficial ## 1 1 1 1 1 ## MayankS08111059 Maveric94280289 ujjain_live PayaswiniShett1 MaheshK70846514 ## 1 1 1 1 1 ## er_gaurav_singh mohanbhadri rushessensedood ShivanshikaF GMahindroo ## 1 1 1 1 1 ## avng47 berojgaradami SandeepKumawat_ _AdityaRaje kirtischandel ## 1 1 1 1 1 Ex-kurs: An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices.They are likely to retweet. 2.10.3 Who are the most retweeted users? # Calculate the in-degree scores in_degree &lt;- degree(nw_rtweet, mode = c(&quot;in&quot;)) # Sort the users in descending order of in-degree scores in_degree_sort &lt;- sort(in_degree, decreasing = TRUE) head(in_degree_sort,10) ## ReSanskrit PeacockSolar rockerblonde StellaYeahilike WaterlooEnergy ## 81 41 25 19 19 ## gulf_intel DanAlluf BPCLimited ByronTweetsData TrafficlyApp ## 17 17 15 14 13 Let’s identify who are authorities, i.e., Twitter accounts with a lot of incoming edges. as &lt;- authority_score(nw_rtweet, weights=NA)$vector sort(as, decreasing = TRUE)[1:20] ## ReSanskrit TrafficlyApp PeacockSolar OrchidOfTheDay rockerblonde ## 1.000000e+00 1.677825e-14 3.459879e-15 1.982355e-15 1.238491e-15 ## WaterlooEnergy DanAlluf BPCLimited ByronTweetsData IntlPeaceBureau ## 8.468956e-16 8.319809e-16 8.051224e-16 7.793320e-16 7.423804e-16 ## starindia L_FudgerGalvez ESA_EO gulf_intel StellaYeahilike ## 7.423804e-16 6.518274e-16 5.949006e-16 5.739893e-16 5.672778e-16 ## crowdfarmingco katyalston gikiearth CrownPlatform AyiccZim ## 5.267684e-16 5.267684e-16 5.197328e-16 4.809261e-16 4.755414e-16 An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices.They are likely to be retweeted. 2.10.4 Who are users important for connecting with others in network? Here I aimed to identify users with important role in allowing information to pass through network. Usually, users with higher betweenness has more control over the network. # Calculate the betweenness scores of the network betwn_nw &lt;- betweenness(nw_rtweet, directed = TRUE) # Sort the users in descending order of betweenness scores betwn_nw_sort &lt;- betwn_nw %&gt;% sort(decreasing = TRUE) %&gt;% round() %&gt;% head(10) betwn_nw_sort ## gikiearth Privatecarfree ChinyeRumby DioxideMat LifexSoles ## 6 6 4 2 1 ## esuohneerg swift_iron NGO_NISWARTH BPCLBareilly Va3tsal ## 1 1 1 1 1 2.10.5 Clustering largest_cliques(nw_rtweet) #list only 20 vertices in that cluster ## [[1]] ## + 3/1100 vertices, named, from a3dd67f: ## [1] BpclStateLPGUP BPCLimited BPCLBareilly ## ## [[2]] ## + 3/1100 vertices, named, from a3dd67f: ## [1] iona_nyandoro LifexSoles LxS_Build ## ## [[3]] ## + 3/1100 vertices, named, from a3dd67f: ## [1] MKY110987 BPCLimited BPCLBareilly ## ## [[4]] ## + 3/1100 vertices, named, from a3dd67f: ## [1] DarlacGardening WaterWorxUK swift_iron 2.10.6 Community detection Finally, I tried to identify communities, i.e., groups of account that might be interested in engaging in conversation about carbon footprint and co2 emissions. #Community detection based on edge betweenness (Newman-Girvan) comm &lt;- cluster_edge_betweenness(nw_rtweet) sort(sizes(comm), decreasing = T)[1:20] ## Community sizes ## 60 44 16 74 98 31 108 233 205 58 42 114 91 110 21 57 4 37 48 99 ## 82 42 31 25 22 20 20 18 15 14 13 13 12 12 11 11 10 10 10 10 comm_1 &lt;- communities(comm) # Tweet accounts in the Community 60 (the biggest community) comm_1$`60` ## [1] &quot;SaachinPatel&quot; &quot;ReSanskrit&quot; &quot;virendrathor007&quot; &quot;ModheraAjay&quot; ## [5] &quot;YuvraajsO&quot; &quot;mahisawOfficial&quot; &quot;MayankS08111059&quot; &quot;Maveric94280289&quot; ## [9] &quot;ujjain_live&quot; &quot;PayaswiniShett1&quot; &quot;MaheshK70846514&quot; &quot;er_gaurav_singh&quot; ## [13] &quot;mohanbhadri&quot; &quot;rushessensedood&quot; &quot;ShivanshikaF&quot; &quot;GMahindroo&quot; ## [17] &quot;avng47&quot; &quot;berojgaradami&quot; &quot;SandeepKumawat_&quot; &quot;_AdityaRaje&quot; ## [21] &quot;kirtischandel&quot; &quot;Sai_3196&quot; &quot;Abhi_Gosavi17&quot; &quot;g_one01&quot; ## [25] &quot;amritanshu20712&quot; &quot;Shrirang_4u&quot; &quot;ChandanSahaDas1&quot; &quot;HGhumnar&quot; ## [29] &quot;im_zala&quot; &quot;the_bhaveshp&quot; &quot;SolankiDhavalJ&quot; &quot;AshokDh10070811&quot; ## [33] &quot;VGilankar&quot; &quot;free_sridhar&quot; &quot;looookeeee&quot; &quot;_dactar_babu&quot; ## [37] &quot;QnalH&quot; &quot;Bhuhan_Raut12&quot; &quot;thakurmanish25&quot; &quot;Dvipalgoswami&quot; ## [41] &quot;SSSPrashantS&quot; &quot;pahariparul&quot; &quot;shwetarathi0301&quot; &quot;Pournimakothap1&quot; ## [45] &quot;SKMaisuriya&quot; &quot;gandhecha_deep&quot; &quot;ravi_patel21&quot; &quot;SarveshdKumar&quot; ## [49] &quot;jenilsaurabh&quot; &quot;Byomkesh5&quot; &quot;vishaltechrexx&quot; &quot;Shubham82408211&quot; ## [53] &quot;Damnshashi&quot; &quot;ms_ayushi&quot; &quot;SankalpKr&quot; &quot;rchandra12&quot; ## [57] &quot;Vishalcr7999&quot; &quot;ub1112&quot; &quot;VishalRajShodhi&quot; &quot;prats_ag&quot; ## [61] &quot;Priyank64671386&quot; &quot;RudreshScharma&quot; &quot;naagaputra&quot; &quot;ipradyu&quot; ## [65] &quot;Prakash13119341&quot; &quot;anitajha2802&quot; &quot;Ankit_yadaavv&quot; &quot;sanatan_&quot; ## [69] &quot;Raj_Sawant96&quot; &quot;sohil_oza&quot; &quot;NoAbsurdity&quot; &quot;PujariRaksha&quot; ## [73] &quot;ddhamaal_bai&quot; &quot;AyushmanDubey18&quot; &quot;NSK_Kochhar&quot; &quot;HadaniSuresh&quot; ## [77] &quot;binayamishra16&quot; &quot;realMeetu&quot; &quot;Sanmon96952110&quot; &quot;ItsRajatRai&quot; ## [81] &quot;ramkotipalli&quot; &quot;Omtripathi95&quot; # Tweet accounts in the Community 44 (the second biggest community) comm_1$`44` ## [1] &quot;TushitGarg&quot; &quot;PeacockSolar&quot; &quot;ianikk18&quot; &quot;rupeshwar_rao&quot; ## [5] &quot;imjyotidwivedi&quot; &quot;NandiniRajendr4&quot; &quot;5a13fec9874b479&quot; &quot;Moanish58105963&quot; ## [9] &quot;sonukau18053840&quot; &quot;snipervenom21&quot; &quot;RobinSingh1825&quot; &quot;Shalmalichakra3&quot; ## [13] &quot;RubalSh04483180&quot; &quot;arun_thevan03&quot; &quot;AshishK54595782&quot; &quot;BasantKKothari&quot; ## [17] &quot;SriAtul2&quot; &quot;roshan_munda&quot; &quot;KillerSrinivas2&quot; &quot;RitanshuChugh&quot; ## [21] &quot;KunalSh62765789&quot; &quot;Rampras17015630&quot; &quot;manvpandit&quot; &quot;27shikhar&quot; ## [25] &quot;anshulajin&quot; &quot;KumariMitya&quot; &quot;EmescoFdn&quot; &quot;AtishKu82064443&quot; ## [29] &quot;ReetuShukla12&quot; &quot;VIJAYAS36039459&quot; &quot;Shashwa38747031&quot; &quot;ManishV59713691&quot; ## [33] &quot;Sanjeev16620270&quot; &quot;Aashish04884619&quot; &quot;ArchitG36451766&quot; &quot;Vikrant64760265&quot; ## [37] &quot;bunny3298&quot; &quot;Joydeep85178197&quot; &quot;KishoreH14&quot; &quot;HarshTi80357066&quot; ## [41] &quot;Sivas_07&quot; &quot;AbhijitOjha7&quot; # Tweet accounts in the Community 16 (the second biggest community) comm_1$`16` ## [1] &quot;gardener_the&quot; &quot;HoratiosGarden&quot; &quot;BrownBurden&quot; &quot;VivienLloyd&quot; ## [5] &quot;LazarovMartin7&quot; &quot;FreeDealSteals&quot; &quot;theOGryankirk&quot; &quot;StellaYeahilike&quot; ## [9] &quot;vhhydroponics&quot; &quot;agritecture&quot; &quot;PlantGetEnough1&quot; &quot;GladerPhilip&quot; ## [13] &quot;Woroud&quot; &quot;tepanchinceva&quot; &quot;Smart_Reads&quot; &quot;birdwriter7&quot; ## [17] &quot;hiro_thoyou3864&quot; &quot;Watrasri&quot; &quot;AgtechOtori&quot; &quot;sthanleyc&quot; ## [21] &quot;richardelio_nyc&quot; &quot;Yeahilike&quot; &quot;azkamel1&quot; &quot;edohpa&quot; ## [25] &quot;romi_hime_black&quot; &quot;banan_thompson&quot; &quot;StellaSanLF&quot; &quot;PeLopez1&quot; ## [29] &quot;RTusuzuro_&quot; &quot;umada_ushijiro&quot; &quot;Sun_Flower119&quot; "],
["text-analysis-of-novels.html", "3 Text Analysis of Novels 3.1 Data 3.2 Text cleaning 3.3 Awakening 3.4 Commonality cloud and Comparison cloud 3.5 Sentiment timeline 3.6 Topic Modelling: LDA 3.7 Main points of interpretation", " 3 Text Analysis of Novels Figure 3.1: Novels used in analysis Although I like reading books, unfortunately I don’t find much time to read. However, I found that NLP is giving an opportunity to learn more about novels without reading it. My idea here is to try to learn as much as possible about two novels, The Aspern Papers and Awakening. 3.1 Data Downloading Novels ‘The Awakening’ and ‘The Aspern Papers’ awakening &lt;- scan(&quot;http://www.gutenberg.org/files/160/160-0.txt&quot;, what=&quot;character&quot;, blank.lines.skip = TRUE, sep=&quot;\\n&quot;) aspern &lt;- scan(&quot;http://www.gutenberg.org/files/211/211-0.txt&quot;, what=&quot;character&quot;, blank.lines.skip = TRUE, sep=&quot;\\n&quot;) awakening&lt;- iconv(awakening, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) aspern&lt;- iconv(aspern, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) 3.2 Text cleaning Cleaning headers of the two novels. # Awakening: cleaning headers awakening.begin &lt;- which(awakening==&quot;THE AWAKENING&quot;)[2] awakening.end &lt;- which(awakening==&quot;*****&quot;) - 1 awakening.v&lt;- awakening[awakening.begin:awakening.end] # Aspern: cleaning headers aspern.begin &lt;- which(aspern==&quot;Macmillan and Co., 1888.&quot;)+1 aspern.end &lt;- which(aspern==&quot;End of the Project Gutenberg EBook of The Aspern Papers, by Henry James&quot;) - 1 aspern.v &lt;- aspern[aspern.begin:aspern.end] head(aspern.v) ## [1] &quot;I&quot; ## [2] &quot;I had taken Mrs. Prest into my confidence; in truth without her I&quot; ## [3] &quot;should have made but little advance, for the fruitful idea in the whole&quot; ## [4] &quot;business dropped from her friendly lips. It was she who invented the&quot; ## [5] &quot;short cut, who severed the Gordian knot. It is not supposed to be the&quot; ## [6] &quot;nature of women to rise as a general thing to the largest and most&quot; 3.3 Awakening Cutting in chapters and corpus creation: # Cutting the Awakening in chapters awakening.v &lt;- gsub(&quot;^I*(X|V)*I*$&quot;, &quot;@@@&quot;, awakening.v) awakening.string &lt;- paste(awakening.v, collapse = &quot; &quot;) awakening.chapters &lt;- strsplit(awakening.string, &quot;@@@ &quot;) # Cutting the Aspern in chapters aspern.v &lt;- gsub(&quot;^I*(X|V)*I*$&quot;, &quot;@@@&quot;, aspern.v) aspern.string &lt;- paste(aspern.v, collapse = &quot; &quot;) aspern.chapters &lt;- strsplit(aspern.string, &quot;@@@ &quot;) # Awakening corpus creation awakening.df &lt;- as.data.frame(awakening.chapters, stringsAsFactors = FALSE) awakening.df &lt;-awakening.df[2:38,1] awakening.df &lt;- as.data.frame(awakening.df) colnames(awakening.df) &lt;- &quot;chapters&quot; awakening.docs &lt;- Corpus(VectorSource(awakening.df$chapters)) # Aspern corpus creation aspern.df &lt;- as.data.frame(aspern.chapters, stringsAsFactors = FALSE) aspern.df &lt;-aspern.df[2:38,1] aspern.df &lt;- as.data.frame(aspern.df) colnames(aspern.df) &lt;- &quot;chapters&quot; aspern.docs &lt;- Corpus(VectorSource(aspern.df$chapters)) Text pre-processing: removing unnecessary simbols and signs, converting letters to lower case, removing numbers, punctuation and white space. # Awakening: Text pre-processing toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;/&quot;) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;@&quot;) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;\\\\|&quot;) awakening.docs &lt;- tm_map(awakening.docs, content_transformer(tolower)) awakening.docs &lt;- tm_map(awakening.docs, removeNumbers) awakening.docs &lt;- tm_map(awakening.docs, removeWords, stopwords(&quot;english&quot;)) awakening.docs &lt;- tm_map(awakening.docs, removePunctuation) awakening.docs &lt;- tm_map(awakening.docs, stripWhitespace) # Aspern: Text pre-processing toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;/&quot;) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;@&quot;) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;\\\\|&quot;) aspern.docs &lt;- tm_map(aspern.docs, content_transformer(tolower)) aspern.docs &lt;- tm_map(aspern.docs, removeNumbers) aspern.docs &lt;- tm_map(aspern.docs, removeWords, stopwords(&quot;english&quot;)) aspern.docs &lt;- tm_map(aspern.docs, removePunctuation) aspern.docs &lt;- tm_map(aspern.docs, stripWhitespace) Creation of a WordCloud: # Awakening: Wordcloud awakening.dtm &lt;- DocumentTermMatrix(awakening.docs, control=list(weighting=weightTf)) awakening.m &lt;- as.matrix(t(awakening.dtm)) awakening_v &lt;- sort(rowSums(awakening.m),decreasing=TRUE) awakening.d &lt;- data.frame(word = names(awakening_v),freq=awakening_v) set.seed(1234) wordcloud(words = awakening.d$word, freq = awakening.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) # Aspern: Wordcloud aspern.dtm &lt;- DocumentTermMatrix(aspern.docs, control=list(weighting=weightTf)) aspern.m &lt;- as.matrix(t(aspern.dtm)) aspern_v &lt;- sort(rowSums(aspern.m),decreasing=TRUE) aspern.d &lt;- data.frame(word = names(aspern_v),freq=aspern_v) set.seed(1234) par(mar = rep(0, 4)) wordcloud(words = aspern.d$word, freq = aspern.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) 3.4 Commonality cloud and Comparison cloud Creation of a corpus out of the both texts: cc.docs &lt;- Corpus(VectorSource(c(awakening.string,aspern.string))) Text pre-processing of the new corpus: toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;/&quot;) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;@&quot;) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;\\\\|&quot;) cc.docs &lt;- tm_map(cc.docs, content_transformer(tolower)) cc.docs &lt;- tm_map(cc.docs, removeNumbers) cc.docs &lt;- tm_map(cc.docs, removeWords, stopwords(&quot;english&quot;)) cc.docs &lt;- tm_map(cc.docs, removePunctuation) cc.docs &lt;- tm_map(cc.docs, stripWhitespace) Comparison and Commonality cloud: cc.dtm &lt;- DocumentTermMatrix(cc.docs) cc.m &lt;- as.matrix(t(cc.dtm)) colnames(cc.m)&lt;- c(&quot;Awakening&quot;,&quot;Aspern&quot;) #Comparison cloud par(mar = rep(0, 4)) comparison.cloud(cc.m,max.words = 100,min.frrandom.order=FALSE) # Commonality cloud par(mar = rep(0, 4)) commonality.cloud(cc.m,max.words = 100,colors = &quot;steelblue1&quot;,min.frrandom.order=FALSE) 3.5 Sentiment timeline Loading in relevant lexicons: library(tidytext) library(dplyr) library(tidyr) library(plotly) library(ggthemes) library(sentimentr) library(syuzhet) data(&quot;sentiments&quot;) afinn &lt;- get_sentiments(&quot;afinn&quot;) bing &lt;- get_sentiments(&quot;bing&quot;) loughran &lt;- get_sentiments(&quot;loughran&quot;) nrc &lt;- get_sentiment_dictionary(&#39;nrc&#39;, language = &quot;english&quot;) Polarity timeline of Awakening: sent.awakening &lt;- readLines(&quot;http://www.gutenberg.org/files/160/160-0.txt&quot;) sent.awakening &lt;-iconv(sent.awakening, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) awakening.corpus &lt;- VCorpus(VectorSource(sent.awakening)) awakening.corpus &lt;- tm_map(awakening.corpus, content_transformer(tolower)) awakening.corpus &lt;- tm_map(awakening.corpus, removeNumbers) awakening.corpus &lt;- tm_map(awakening.corpus, removeWords, stopwords(&quot;english&quot;)) awakening.corpus &lt;- tm_map(awakening.corpus, removePunctuation) awakening.corpus &lt;- tm_map(awakening.corpus, stripWhitespace) awakening.dtm &lt;- DocumentTermMatrix(awakening.corpus) awakening.tidy &lt;- tidy(awakening.dtm) awakening.tidy$count &lt;-as.numeric(awakening.tidy$count) colnames(awakening.tidy)[2]&lt;- &#39;word&#39; awakening.tidy$document &lt;- as.numeric(awakening.tidy$document) nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) joy.words &lt;- inner_join(awakening.tidy, nrc.joy) joy.words &lt;- count(joy.words, word) bing &lt;- get_sentiments(&quot;bing&quot;) awakening.sentiment &lt;- inner_join(awakening.tidy, bing) awakening.sentiment &lt;- count(awakening.sentiment, sentiment, index=document) awakening.sentiment &lt;- spread(awakening.sentiment, sentiment, n, fill=0) awakening.sentiment$polarity &lt;- awakening.sentiment$positive - awakening.sentiment$negative awakening.sentiment$pos &lt;- ifelse(awakening.sentiment$polarity &gt;=0, &quot;pos&quot;, &quot;neg&quot;) ggplot(awakening.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1) awakening.smooth &lt;- ggplot(awakening.sentiment, aes(index, polarity)) (p2&lt;-awakening.smooth + stat_smooth()) Polarity timeline of Aspern: sent.aspern &lt;- readLines(&quot;http://www.gutenberg.org/files/211/211-0.txt&quot;) aspern.corpus &lt;- VCorpus(VectorSource(sent.aspern)) aspern.corpus &lt;- tm_map(aspern.corpus, content_transformer(tolower)) aspern.corpus &lt;- tm_map(aspern.corpus, removeNumbers) aspern.corpus &lt;- tm_map(aspern.corpus, removeWords, stopwords(&quot;english&quot;)) aspern.corpus &lt;- tm_map(aspern.corpus, removePunctuation) aspern.corpus &lt;- tm_map(aspern.corpus, stripWhitespace) sent.aspern &lt;-iconv(sent.aspern, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) aspern.dtm &lt;- DocumentTermMatrix(aspern.corpus) aspern.tidy &lt;- tidy(aspern.dtm) aspern.tidy$count &lt;-as.numeric(aspern.tidy$count) colnames(aspern.tidy)[2]&lt;- &#39;word&#39; aspern.tidy$document &lt;- as.numeric(aspern.tidy$document) nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) joy.words &lt;- inner_join(aspern.tidy, nrc.joy) joy.words &lt;- count(joy.words, word) #bing &lt;- subset(sentiments, sentiments$lexicon==&#39;bing&#39;)[,-4] aspern.sentiment &lt;- inner_join(aspern.tidy, bing) aspern.sentiment &lt;- count(aspern.sentiment, sentiment, index=document) aspern.sentiment &lt;- spread(aspern.sentiment, sentiment, n, fill=0) aspern.sentiment$polarity &lt;- aspern.sentiment$positive - aspern.sentiment$negative aspern.sentiment$pos &lt;- ifelse(aspern.sentiment$polarity &gt;=0, &quot;pos&quot;, &quot;neg&quot;) ggplot(aspern.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1) aspern.smooth &lt;- ggplot(aspern.sentiment, aes(index, polarity)) (p3&lt;-aspern.smooth + stat_smooth()) g2 &lt;- ggplotGrob(p2) g3 &lt;- ggplotGrob(p3) g &lt;- rbind(g2, g3, size = &quot;first&quot;) g$widths &lt;- unit.pmax(g2$widths, g3$widths) grid.newpage() grid.draw(g) 3.6 Topic Modelling: LDA Pick up 5 topics and try to make sense of the topics giving a label to them. # Awakening top.mod.awakening.dtm &lt;- DocumentTermMatrix(awakening.corpus, control = list(weighting=weightTf)) burnin = 1000 iter = 1000 keep = 50 set.seed(510) top.mod.awakening.n &lt;- nrow(top.mod.awakening.dtm) top.mod.awakening.dtm &lt;- top.mod.awakening.dtm[row_sums(top.mod.awakening.dtm &gt; 0) &gt; 1,] top.mod.awakening.lda_basic.model&lt;- LDA(top.mod.awakening.dtm, k = 5L, method = &quot;Gibbs&quot;, control = list(burnin = burnin, iter = iter, keep = keep, alpha = 1) ) top.mod.awakening.lda.topics &lt;- as.matrix(topics(top.mod.awakening.lda_basic.model)) #top.mod.awakening.lda.topics top.mod.awakening.lda.terms &lt;- as.matrix(terms(top.mod.awakening.lda_basic.model, 10)) top.mod.awakening.lda.terms&lt;- iconv(top.mod.awakening.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #top.mod.awakening.lda.terms awakening.top10termsPerTopic &lt;- terms(top.mod.awakening.lda_basic.model, 10) awakening.top10termsPerTopic &lt;- iconv(top.mod.awakening.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #awakening.top10termsPerTopic colnames(awakening.top10termsPerTopic)&lt;-c(&quot;Edna in her house/room&quot;,&quot; Work, life and love - Leaving the papers after he/she died&quot;,&quot;Madame Edna - Mademoiselle Ratignolle - Robert&quot;,&quot;Emotions - Description of a face &quot;,&quot;Mrs.Pontellier - Edna - Robert&quot;) awakening.top10termsPerTopic ## Edna in her house/room ## [1,] &quot;one&quot; ## [2,] &quot;like&quot; ## [3,] &quot;never&quot; ## [4,] &quot;thought&quot; ## [5,] &quot;day&quot; ## [6,] &quot;might&quot; ## [7,] &quot;seemed&quot; ## [8,] &quot;children&quot; ## [9,] &quot;two&quot; ## [10,] &quot;felt&quot; ## Work, life and love - Leaving the papers after he/she died ## [1,] &quot;project&quot; ## [2,] &quot;work&quot; ## [3,] &quot;must&quot; ## [4,] &quot;gutenbergtm&quot; ## [5,] &quot;without&quot; ## [6,] &quot;new&quot; ## [7,] &quot;found&quot; ## [8,] &quot;many&quot; ## [9,] &quot;feeling&quot; ## [10,] &quot;full&quot; ## Madame Edna - Mademoiselle Ratignolle - Robert ## [1,] &quot;upon&quot; ## [2,] &quot;eyes&quot; ## [3,] &quot;face&quot; ## [4,] &quot;hand&quot; ## [5,] &quot;looked&quot; ## [6,] &quot;sat&quot; ## [7,] &quot;white&quot; ## [8,] &quot;night&quot; ## [9,] &quot;table&quot; ## [10,] &quot;took&quot; ## Emotions - Description of a face Mrs.Pontellier - Edna - Robert ## [1,] &quot;little&quot; &quot;pontellier&quot; ## [2,] &quot;madame&quot; &quot;mrs&quot; ## [3,] &quot;went&quot; &quot;said&quot; ## [4,] &quot;away&quot; &quot;edna&quot; ## [5,] &quot;old&quot; &quot;will&quot; ## [6,] &quot;edna&quot; &quot;know&quot; ## [7,] &quot;back&quot; &quot;time&quot; ## [8,] &quot;house&quot; &quot;come&quot; ## [9,] &quot;ratignolle&quot; &quot;good&quot; ## [10,] &quot;robert&quot; &quot;mademoiselle&quot; awakening.topicNames &lt;- apply(awakening.top10termsPerTopic, 2, paste, collapse=&quot; &quot;) #awakening.topicNames # Aspern top.mod.aspern.dtm &lt;- DocumentTermMatrix(aspern.corpus, control = list(weighting=weightTf)) burnin = 1000 iter = 1000 keep = 50 set.seed(510) top.mod.aspern.n &lt;- nrow(top.mod.aspern.dtm) top.mod.aspern.dtm &lt;- top.mod.aspern.dtm[row_sums(top.mod.aspern.dtm &gt; 0) &gt; 1,] top.mod.aspern.lda_basic.model&lt;- LDA(top.mod.aspern.dtm, k = 5L, method = &quot;Gibbs&quot;, control = list(burnin = burnin, iter = iter, keep = keep, alpha = 1) ) top.mod.aspern.lda.topics &lt;- as.matrix(topics(top.mod.aspern.lda_basic.model)) #top.mod.aspern.lda.topics top.mod.aspern.lda.terms &lt;- as.matrix(terms(top.mod.aspern.lda_basic.model, 10)) top.mod.aspern.lda.terms&lt;- iconv(top.mod.aspern.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #top.mod.aspern.lda.terms aspern.top10termsPerTopic &lt;- terms(top.mod.aspern.lda_basic.model, 10) aspern.top10termsPerTopic &lt;- iconv(top.mod.aspern.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #aspern.top10termsPerTopic colnames(aspern.top10termsPerTopic)&lt;-c(&quot;Home - Seeing somebody - description of eyes - leaving&quot;,&quot;Jeffrey&quot;,&quot;Tita&quot;,&quot;Aunt&quot;,&quot;Time - Woman - Venice&quot;) aspern.top10termsPerTopic ## Home - Seeing somebody - description of eyes - leaving Jeffrey ## [1,] &quot;went&quot; &quot;project&quot; ## [2,] &quot;came&quot; &quot;gutenbergtm&quot; ## [3,] &quot;house&quot; &quot;work&quot; ## [4,] &quot;back&quot; &quot;papers&quot; ## [5,] &quot;face&quot; &quot;may&quot; ## [6,] &quot;looked&quot; &quot;almost&quot; ## [7,] &quot;took&quot; &quot;works&quot; ## [8,] &quot;away&quot; &quot;electronic&quot; ## [9,] &quot;look&quot; &quot;jeffrey&quot; ## [10,] &quot;put&quot; &quot;aspern&quot; ## Tita Aunt Time - Woman - Venice ## [1,] &quot;&quot; &quot;might&quot; &quot;old&quot; ## [2,] &quot;miss&quot; &quot;come&quot; &quot;one&quot; ## [3,] &quot;&quot; &quot;little&quot; &quot;made&quot; ## [4,] &quot;tita&quot; &quot;way&quot; &quot;never&quot; ## [5,] &quot;said&quot; &quot;aunt&quot; &quot;time&quot; ## [6,] &quot;know&quot; &quot;even&quot; &quot;long&quot; ## [7,] &quot;dont&quot; &quot;take&quot; &quot;great&quot; ## [8,] &quot;think&quot; &quot;make&quot; &quot;still&quot; ## [9,] &quot;oh&quot; &quot;idea&quot; &quot;woman&quot; ## [10,] &quot;bordereau&quot; &quot;told&quot; &quot;upon&quot; #aspern.topicNames &lt;- apply(aspern.top10termsPerTopic, 2, paste, collapse=&quot; &quot;) #aspern.topicNames 3.7 Main points of interpretation The most common words are names of characters such as Edna, Tita and Mrs.Pontellier. According to the sentiment analysis, Awakening novel is slightly from the beginning to the end. On the other hand, Aspern has a slight drop in the negative sentiment, but the end is positive. Awakening is as twice as longer text than Aspern. Due to lemmatization could not address the time of speaking (wheater it is told in past or future) Words “said”,little“,”think“,”house“,”good“,”like“,”one“,”come“,”back“,”thought\" are the most common. "],
["covid-19-dashboard-for-balkan-countries.html", "4 COVID 19 Dashboard for Balkan countries* 4.1 Data 4.2 Curve of confirmed cases 4.3 Total cases per 1 million people 4.4 Deaths vs Recovered 4.5 Active cases vs New cases 4.6 Hospitalization", " 4 COVID 19 Dashboard for Balkan countries* Here you go directly to the dashboard 4.1 Data # Github covid19_confirmed_git &lt;-&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&quot; covid19_confirmed_git &lt;- read_csv(url(covid19_confirmed_git)) # Worldometer ## Covid codivid19_all &lt;- &quot;https://www.worldometers.info/coronavirus/&quot; main_table &lt;- codivid19_all%&gt;% xml2::read_html()%&gt;% html_nodes(xpath=&#39;//*[@id=&quot;main_table_countries_today&quot;]&#39;) %&gt;% html_table() main_table &lt;- as.data.frame(main_table) ## Filtering data for Balkan countries (plus Italy and Austria) balkan &lt;- filter(main_table,Country.Other == &quot;Bosnia and Herzegovina&quot; | Country.Other == &quot;Italy&quot; | Country.Other == &quot;Croatia&quot; | Country.Other == &quot;Serbia&quot; | Country.Other == &quot;Montenegro&quot; | Country.Other == &quot;Slovenia&quot; | Country.Other == &quot;Austria&quot; | Country.Other == &quot;North Macedonia&quot; | Country.Other == &quot;Greece&quot;) # Removing comma from numbers balkan[c(&quot;TotalRecovered&quot;,&quot;TotalDeaths&quot;,&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;ActiveCases&quot;,&quot;Tot.Cases.1M.pop&quot;)] &lt;- lapply(balkan[c(&quot;TotalRecovered&quot;,&quot;TotalDeaths&quot;,&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;ActiveCases&quot;,&quot;Tot.Cases.1M.pop&quot;)], function(x) gsub(&quot;,&quot;,&quot;&quot;,x)) # Turning columns to numeric balkan[c(&quot;TotalRecovered&quot;,&quot;TotalDeaths&quot;,&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;ActiveCases&quot;,&quot;Tot.Cases.1M.pop&quot;)] &lt;- lapply(balkan[c(&quot;TotalRecovered&quot;,&quot;TotalDeaths&quot;,&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;ActiveCases&quot;,&quot;Tot.Cases.1M.pop&quot;)], as.numeric) 4.2 Curve of confirmed cases The graph shows the number of confirmed cases by the last date shown.Updates daily at around 23:59 UTC. # Curve of confirmed cases---- columns &lt;- colnames(covid19_confirmed_git)[5:ncol(covid19_confirmed_git)] final &lt;-as.data.frame(pivot_longer(covid19_confirmed_git, cols = columns, names_to = &quot;Year&quot;, values_to = &quot;Confirmed&quot;)) final$Year &lt;- as.Date.character(final$Year,&quot;%m/%d/%y&quot;) colnames(final) &lt;- c(&quot;Province&quot;,&quot;Country&quot;,&quot;Lat&quot;,&quot;Long&quot;,&quot;Year&quot;,&quot;Confirmed&quot;) filter &lt;- filter(final, Country == &quot;Bosnia and Herzegovina&quot; | Country == &quot;Italy&quot; | Country == &quot;Croatia&quot; | Country == &quot;Serbia&quot; | Country == &quot;Slovenia&quot; | Country == &quot;Montenegro&quot; | Country == &quot;Austria&quot; | Country == &quot;North Macedonia&quot; | Country == &quot;Greece&quot;) p &lt;-ggplot(filter, aes(x = Year, y = Confirmed)) + geom_line(aes(color = Country), size = 1) + scale_color_brewer(palette=&quot;Set1&quot;)+ theme(legend.title = element_text(size = 6),legend.text = element_text(size = 6), # Remove panel background panel.background = element_blank(), # Add axis line axis.line = element_line(colour = &quot;grey&quot;))+ scale_y_log10(labels = comma)+ scale_x_date(date_labels = &quot;%b-%d&quot;, date_breaks = &quot;4 week&quot;)+ ylab(&quot;Confirmed cases&quot;)+ labs(caption=&quot;Data source: https://github.com/CSSEGISandData/COVID-19&quot;) ggplotly(p) 4.3 Total cases per 1 million people tot_cases_1m &lt;- melt(balkan[,c(&quot;Tot.Cases.1M.pop&quot;,&quot;Country.Other&quot;)]) head(tot_cases_1m) ## Country.Other variable value ## 1 Italy Tot.Cases.1M.pop 6854 ## 2 Austria Tot.Cases.1M.pop 7183 ## 3 Serbia Tot.Cases.1M.pop 4144 ## 4 Bosnia and Herzegovina Tot.Cases.1M.pop 10417 ## 5 Croatia Tot.Cases.1M.pop 6243 ## 6 Greece Tot.Cases.1M.pop 2438 p&lt;-ggplot(tot_cases_1m, aes(x=Country.Other,y=value,fill=Country.Other)) + geom_bar(stat = &quot;identity&quot;)+ scale_fill_manual(name=&quot;Country&quot;, values = c(&quot;#E41A1C&quot;, &quot;#377EB8&quot;, &quot;#4DAF4A&quot;, &quot;#984EA3&quot;, &quot;#FF7F00&quot;, &quot;#FFFF33&quot;, &quot;#A65628&quot;, &quot;#F781BF&quot;, &quot;#999999&quot;), labels=c(&quot;Austria&quot;, &quot;Bosnia and Herzegovina&quot;, &quot;Croatia&quot;, &quot;Greece&quot;, &quot;Italy&quot;, &quot;Montenegro&quot;, &quot;N.Macedonia&quot;, &quot;Serbia&quot;, &quot;Slovenia&quot;))+ labs(x=&quot;&quot;,y=&quot;Total Cases per 1M people&quot;, title = &quot;Total Cases per 1m people - Currently&quot;)+ theme(legend.title = element_text(size = 8), axis.text.x = element_blank(), legend.text = element_text(size = 8), panel.background = element_blank(), axis.line = element_line(colour = &quot;grey&quot;)) ggplotly(p) 4.4 Deaths vs Recovered The bar chart shows the total number of recovered people in comparison to the total number of death cases. Updates daily at around 23:59 UTC. options(scipen = 9999) p &lt;- ggplot(balkan) + geom_segment( aes(x=Country.Other, xend=Country.Other, y=TotalRecovered, yend=TotalDeaths), color=&quot;grey&quot;) + geom_point( aes(x=Country.Other, y=TotalRecovered), color=rgb(0.2,0.7,0.1,0.5), size=3 ) + geom_point( aes(x=Country.Other, y=TotalDeaths), color=rgb(0.7,0.2,0.1,0.5), size=3 ) + coord_flip()+ scale_y_log10()+ theme_minimal() + theme( legend.position = &quot;none&quot;, panel.background = element_blank(), panel.grid = element_blank(), axis.line = element_line(colour = &quot;grey&quot;)) + xlab(&quot;&quot;) + ylab(&quot;Number of cases&quot;)+ ggtitle(label = &quot;Death vs Recovered&quot;) ggplotly(p) 4.5 Active cases vs New cases The bar chart shows the number of active cases in comparison to the number of new cases being constantly reported. # Active cases vs New cases active_and_new &lt;- melt(balkan[,c(&quot;ActiveCases&quot;,&quot;NewCases&quot;,&quot;Country.Other&quot;)]) p &lt;- ggplot(active_and_new, aes(x=Country.Other, y=value, fill=variable)) + geom_bar(stat=&#39;identity&#39;, position=&#39;dodge&#39;, color=&quot;black&quot; ,aes(text=paste(&quot;Country: &quot;,Country.Other, &quot;\\n&quot;, variable,&quot;:&quot;,value, sep=&quot;&quot;))) + scale_fill_brewer(palette = &quot;Paired&quot;)+ scale_y_continuous(labels=comma, trans = &quot;log10&quot;) + ylab(&quot;Number of cases&quot;)+ xlab(&quot;&quot;)+ theme_minimal()+ labs(title = &quot;&quot;,fill=&quot;&quot;)+ coord_flip() ggplotly(p, tooltip = &quot;text&quot;) 4.6 Hospitalization 4.6.1 Croatia 4.6.2 Slovenia 4.6.3 Austria "],
["churn-prediction-with-random-forest.html", "5 Churn prediction with Random Forest 5.1 Data 5.2 Evaluating Models", " 5 Churn prediction with Random Forest In this project I decided to use Telco dataset, a dataset from the IBM Watson Analytics community, and apply Random Forest learning method to make a prediction whether a customer will churn or not. library(curl) library(readr) library(tidyr) library(dplyr) library(randomForest) library(caret) options(stringsAsFactor=TRUE) 5.1 Data The data set includes information about: Customers who left within the last month – the column is called Churn. Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies. Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges Demographic info about customers – gender, age range, and if they have partners and dependents Variable Topic Gender Whether the customer is a male or a female SeniorCitizen Whether the customer is a senior citizen or not (1, 0) Partner Whether the customer has a partner or not (Yes, No) Dependents Whether the customer has dependents or not (Yes, No) Tenure Number of months the customer has stayed with the company PhoneService Whether the customer has a phone service or not (Yes, No) MultipleLines Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService Customer’s internet service provider (DSL, Fiber optic, No) OnlineSecurity Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection Whether the customer has device protection or not (Yes, No, No internet service) TechSupport Whether the customer has tech support or not (Yes, No, No internet service) StreamingTV Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovie Whether the customer has streaming movies or not (Yes, No, No internet service) Contract The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling Whether the customer has paperless billing or not (Yes, No) PaymentMethod The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges The amount charged to the customer monthly TotalCharges The total amount charged to the customer Churn Whether the customer churned or not (Yes or No) Note: Information collected from https://www.kaggle.com/blastchar/telco-customer-churn Here are dimensions of our data set and first 6 rows: First, we need to remove rows which have at least one NA as a value: # Drop Nas dim(x) ## [1] 7043 21 x &lt;- x %&gt;% drop_na() dim(x) ## [1] 7032 21 11 are removed from the data set as they held at least one NA as a value. Next, we need to prepare our data set in terms of data types. In order to use Random Forest for churn prediction, we need to make sure that our categorical are represented in numeric manner. Therefore, all categorical variables will be converted to factors. # Categorical variables to factors x[c(&quot;Partner&quot;,&quot;Dependents&quot;,&quot;PhoneService&quot;,&quot;gender&quot;,&quot;MultipleLines&quot;,&quot;InternetService&quot;,&quot;OnlineSecurity&quot;,&quot;OnlineBackup&quot;,&quot;DeviceProtection&quot;,&quot;TechSupport&quot;,&quot;StreamingTV&quot;,&quot;Contract&quot;,&quot;StreamingMovies&quot;,&quot;PaperlessBilling&quot;,&quot;PaymentMethod&quot;,&quot;Churn&quot;)]&lt;-lapply(x[c(&quot;Partner&quot;,&quot;Dependents&quot;,&quot;PhoneService&quot;,&quot;gender&quot;,&quot;MultipleLines&quot;,&quot;InternetService&quot;,&quot;OnlineSecurity&quot;,&quot;OnlineBackup&quot;,&quot;DeviceProtection&quot;,&quot;TechSupport&quot;,&quot;StreamingTV&quot;,&quot;Contract&quot;,&quot;StreamingMovies&quot;,&quot;PaperlessBilling&quot;,&quot;PaymentMethod&quot;,&quot;Churn&quot;)], as.factor) str(x) ## &#39;data.frame&#39;: 7032 obs. of 21 variables: ## $ customerID : chr &quot;7590-VHVEG&quot; &quot;5575-GNVDE&quot; &quot;3668-QPYBK&quot; &quot;7795-CFOCW&quot; ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 1 1 2 1 1 2 ... ## $ SeniorCitizen : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Partner : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 1 1 1 1 1 2 1 ... ## $ Dependents : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 2 1 1 2 ... ## $ tenure : int 1 34 2 45 2 8 22 10 28 62 ... ## $ PhoneService : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 2 1 2 2 2 1 2 2 ... ## $ MultipleLines : Factor w/ 3 levels &quot;No&quot;,&quot;No phone service&quot;,..: 2 1 1 2 1 3 3 2 3 1 ... ## $ InternetService : Factor w/ 3 levels &quot;DSL&quot;,&quot;Fiber optic&quot;,..: 1 1 1 1 2 2 2 1 2 1 ... ## $ OnlineSecurity : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 3 3 3 1 1 1 3 1 3 ... ## $ OnlineBackup : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 3 1 3 1 1 1 3 1 1 3 ... ## $ DeviceProtection: Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 3 1 3 1 3 1 1 3 1 ... ## $ TechSupport : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 3 1 1 1 1 3 1 ... ## $ StreamingTV : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 1 1 3 3 1 3 1 ... ## $ StreamingMovies : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 1 1 3 1 1 3 1 ... ## $ Contract : Factor w/ 3 levels &quot;Month-to-month&quot;,..: 1 2 1 2 1 1 1 1 1 2 ... ## $ PaperlessBilling: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 2 1 2 2 2 1 2 1 ... ## $ PaymentMethod : Factor w/ 4 levels &quot;Bank transfer (automatic)&quot;,..: 3 4 4 1 3 3 2 4 3 1 ... ## $ MonthlyCharges : num 29.9 57 53.9 42.3 70.7 ... ## $ TotalCharges : num 29.9 1889.5 108.2 1840.8 151.7 ... ## $ Churn : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 2 1 2 2 1 1 2 1 ... We aim to convert our data set into matrix with only 0s and 1s. Thus, all binomial variables can be turned to 0s and 1s in the following way: # Binomial categorical variables to 0 and 1 x$PhoneService &lt;- as.numeric(x$PhoneService)-1 x$Partner &lt;-as.numeric(x$Partner)-1 x$Dependents &lt;- as.numeric(x$Dependents)-1 x$Churn &lt;- as.numeric(x$Churn)-1 x$gender &lt;- as.numeric(x$gender)-1 Now it is left to convert multi-class variables 0s and 1s. Before we do it, let’s inspect our data a bit: ## Categorical variables with more than 2 categories par(mfrow=c(5,2)) # Multiple lines ggplot(x, aes(MultipleLines,fill=MultipleLines)) + geom_bar() + labs(title=&quot;Multiple lines&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # Internet services ggplot(x, aes(InternetService,fill=InternetService)) + geom_bar() + labs(title=&quot;Internet service&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # OnlineSecurity ggplot(x, aes(OnlineSecurity,fill=OnlineSecurity)) + geom_bar() + labs(title=&quot;OnlineSecurity&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # OnlineBackup ggplot(x, aes(OnlineBackup,fill=OnlineBackup)) + geom_bar() + labs(title=&quot;OnlineBackup&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # DeviceProtection ggplot(x, aes(DeviceProtection,fill=DeviceProtection)) + geom_bar() + labs(title=&quot;DeviceProtection&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # TechSupport ggplot(x, aes(TechSupport,fill=TechSupport)) + geom_bar() + labs(title=&quot;TechSupport&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # StreamingTV ggplot(x, aes(StreamingTV,fill=StreamingTV)) + geom_bar() + labs(title=&quot;StreamingTV&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # StreamingMovies ggplot(x, aes(StreamingMovies,fill=StreamingMovies)) + geom_bar() + labs(title=&quot;StreamingMovies&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # Contract ggplot(x, aes(Contract,fill=Contract)) + geom_bar() + labs(title=&quot;Contract&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # PaymentMethod ggplot(x, aes(PaymentMethod,fill=PaymentMethod)) + geom_bar() + labs(title=&quot;PaymentMethod&quot;,x=&quot;&quot;,y=&quot;Count&quot;) We can apply one hot encoding to our data set by using R’s base function model.matrix. In the code below, ~.+0 leads to encoding of all categorical variables without producing an intercept. # One-hot encoding x.mat&lt;- model.matrix(~MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+Contract+StreamingMovies+PaperlessBilling+PaymentMethod+0,data = x) Now is our data set pre-processed: # Creation of the final data frame with 0s and 1s final.df&lt;-as.data.frame(x.mat) final.df&lt;- cbind(x.mat,x$tenure,x$TotalCharges,x$MonthlyCharges,x$PhoneService,x$Partner,x$Dependents,x$gender,x$Churn) colnames(final.df)[24:31]&lt;-c(&quot;tenure&quot;,&quot;TotalCharges&quot;,&quot;MonthlyCharges&quot;,&quot;PhoneService&quot;,&quot;Partner&quot;,&quot;Dependents&quot;,&quot;gender&quot;,&quot;Churn&quot;) head(final.df) ## MultipleLinesNo MultipleLinesNo phone service MultipleLinesYes ## 1 0 1 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 0 1 0 ## 5 1 0 0 ## 6 0 0 1 ## InternetServiceFiber optic InternetServiceNo ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 1 0 ## 6 1 0 ## OnlineSecurityNo internet service OnlineSecurityYes ## 1 0 0 ## 2 0 1 ## 3 0 1 ## 4 0 1 ## 5 0 0 ## 6 0 0 ## OnlineBackupNo internet service OnlineBackupYes ## 1 0 1 ## 2 0 0 ## 3 0 1 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## DeviceProtectionNo internet service DeviceProtectionYes ## 1 0 0 ## 2 0 1 ## 3 0 0 ## 4 0 1 ## 5 0 0 ## 6 0 1 ## TechSupportNo internet service TechSupportYes StreamingTVNo internet service ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 1 0 ## 5 0 0 0 ## 6 0 0 0 ## StreamingTVYes ContractOne year ContractTwo year ## 1 0 0 0 ## 2 0 1 0 ## 3 0 0 0 ## 4 0 1 0 ## 5 0 0 0 ## 6 1 0 0 ## StreamingMoviesNo internet service StreamingMoviesYes PaperlessBillingYes ## 1 0 0 1 ## 2 0 0 0 ## 3 0 0 1 ## 4 0 0 0 ## 5 0 0 1 ## 6 0 1 1 ## PaymentMethodCredit card (automatic) PaymentMethodElectronic check ## 1 0 1 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 1 ## 6 0 1 ## PaymentMethodMailed check tenure TotalCharges MonthlyCharges PhoneService ## 1 0 1 29.85 29.85 0 ## 2 1 34 1889.50 56.95 1 ## 3 1 2 108.15 53.85 1 ## 4 0 45 1840.75 42.30 0 ## 5 0 2 151.65 70.70 1 ## 6 0 8 820.50 99.65 1 ## Partner Dependents gender Churn ## 1 1 0 0 0 ## 2 0 0 1 0 ## 3 0 0 1 1 ## 4 0 0 1 0 ## 5 0 0 0 1 ## 6 0 0 0 1 Now we need to split our data into test and train data. The proportion is 70:30. # Creation of training and test data sets index &lt;- caret::createDataPartition(x$Churn, p = 0.7, list = F) train &lt;- final.df[index,] test &lt;- final.df[-index,] # Partitioning test data x_test &lt;- as.matrix(test[,-31]) y_test &lt;- as.matrix(test[,31]) # Partitioning train data x_train &lt;- as.matrix(train[,-31]) y_train &lt;- as.matrix(train[,31]) To train our random forest model we use function randomForest(). At this point we will not aim to fine tune our model, so we will define just two parameters,ntree and maxnodes: ntree defines the number of trees to build in the forest. maxnodes defines the maximum number of terminal nodes each tree in the forest can have. # Random Forest # Training library(randomForest) rfModel &lt;- randomForest(x=x_train, y=factor(y_train), ntree=500, maxnodes=24) Since predictions are made based on features our model was trained on, it is possible to observe importance of each feature. The more important a feature, the greater influence it exerts on predictions: importance_features &lt;- randomForest::importance(rfModel) importance_features &lt;- as.data.frame(importance_features) importance_features$features &lt;- row.names(importance_features) importance_features &lt;- importance_features[order(importance_features$MeanDecreaseGini ,decreasing = TRUE),] library(plotly) p&lt;-ggplot(importance_features) + geom_point(aes(reorder(features,MeanDecreaseGini),MeanDecreaseGini),stat = &quot;identity&quot;)+ theme_minimal()+ coord_flip()+ labs(title=&quot;Important features&quot;,x=&quot;Features&quot;) ggplotly(p) As we can see from this output, the tenure feature seems to be the most important factor in making the final prediction. Factors such as InternetServiceFiber optic,TotalCharges and ContractTwo year come subsequently. 5.2 Evaluating Models In order to evaluate our model we will take a look at accuracy, precision and recall. # Evaluating Models prediction_insample &lt;- as.double(predict(rfModel, x_train)) - 1 prediction_outsample &lt;- as.double(predict(rfModel, x_test)) - 1 Accuracy is the percentage of correct predictions out of all predictions. # Accuracy accu_insample &lt;- mean(y_train == prediction_insample) accu_outsample &lt;- mean(y_test == prediction_outsample) print(sprintf(&#39;In-Sample Accuracy: %0.2f&#39;, accu_insample)) ## [1] &quot;In-Sample Accuracy: 0.80&quot; print(sprintf(&#39;Out-Sample Accuracy: %0.2f&#39;, accu_outsample)) ## [1] &quot;Out-Sample Accuracy: 0.78&quot; We managed to achieve pretty good out-sample accuracy even without thorough fine-tuning our parameters. Precision is the number of true positives divided by the total number of true positives and false positives. # Precision prec_insample &lt;- sum(prediction_insample &amp; y_train) / sum(prediction_insample) prec_outsample &lt;- sum(prediction_outsample &amp; y_test) / sum(prediction_outsample) print(sprintf(&#39;In-Sample Precision: %0.2f&#39;, prec_insample)) ## [1] &quot;In-Sample Precision: 0.70&quot; print(sprintf(&#39;Out-Sample Precision: %0.2f&#39;, prec_outsample)) ## [1] &quot;Out-Sample Precision: 0.67&quot; Recall is defined as the number of true positives divided by number of true positives plus false negatives. # Recall recall_insample &lt;- sum(prediction_insample &amp; y_train) / sum(y_train) recall_outsample &lt;- sum(prediction_outsample &amp; y_test) / sum(y_test) print(sprintf(&#39;In-Sample Recall: %0.4f&#39;, recall_insample)) ## [1] &quot;In-Sample Recall: 0.4496&quot; print(sprintf(&#39;Out-Sample Recall: %0.4f&#39;, recall_outsample)) ## [1] &quot;Out-Sample Recall: 0.3807&quot; Finally, in order to estimate how good our model is in comparison to the random prediction, we will inspect ROC curve and the Area Under the Curve. library(ROCR) pred_prob_insample &lt;- as.double(predict(rfModel, x_train, type=&#39;prob&#39;)[,2]) pred_prob_outsample &lt;- as.double(predict(rfModel, x_test, type=&#39;prob&#39;)[,2]) pred &lt;- prediction(pred_prob_outsample, y_test) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure=&#39;auc&#39;)@y.values[[1]] plot(perf,main=sprintf(&#39;Random Forest (AUC: %0.2f)&#39;, auc),col=&#39;darkblue&#39;,lwd=2) + grid() ## integer(0) abline(a = 0, b = 1, col=&#39;darkgray&#39;, lty=4, lwd=2) "]
]
