[
["churn-prediction-with-random-forest.html", "5 Churn prediction with Random Forest 5.1 Data 5.2 Evaluating Models", " 5 Churn prediction with Random Forest In this project I decided to use Telco dataset, a dataset from the IBM Watson Analytics community, and apply Random Forest learning method to make a prediction whether a customer will churn or not. library(curl) library(readr) library(tidyr) library(dplyr) library(randomForest) library(caret) options(stringsAsFactor=TRUE) 5.1 Data The data set includes information about: Customers who left within the last month – the column is called Churn. Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies. Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges Demographic info about customers – gender, age range, and if they have partners and dependents Variable Topic Gender Whether the customer is a male or a female SeniorCitizen Whether the customer is a senior citizen or not (1, 0) Partner Whether the customer has a partner or not (Yes, No) Dependents Whether the customer has dependents or not (Yes, No) Tenure Number of months the customer has stayed with the company PhoneService Whether the customer has a phone service or not (Yes, No) MultipleLines Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService Customer’s internet service provider (DSL, Fiber optic, No) OnlineSecurity Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection Whether the customer has device protection or not (Yes, No, No internet service) TechSupport Whether the customer has tech support or not (Yes, No, No internet service) StreamingTV Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovie Whether the customer has streaming movies or not (Yes, No, No internet service) Contract The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling Whether the customer has paperless billing or not (Yes, No) PaymentMethod The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges The amount charged to the customer monthly TotalCharges The total amount charged to the customer Churn Whether the customer churned or not (Yes or No) Note: Information collected from https://www.kaggle.com/blastchar/telco-customer-churn Here are dimensions of our data set and first 6 rows: First, we need to remove rows which have at least one NA as a value: # Drop Nas dim(x) ## [1] 7043 21 x &lt;- x %&gt;% drop_na() dim(x) ## [1] 7032 21 11 are removed from the data set as they held at least one NA as a value. Next, we need to prepare our data set in terms of data types. In order to use Random Forest for churn prediction, we need to make sure that our categorical are represented in numeric manner. Therefore, all categorical variables will be converted to factors. # Categorical variables to factors x[c(&quot;Partner&quot;,&quot;Dependents&quot;,&quot;PhoneService&quot;,&quot;gender&quot;,&quot;MultipleLines&quot;,&quot;InternetService&quot;,&quot;OnlineSecurity&quot;,&quot;OnlineBackup&quot;,&quot;DeviceProtection&quot;,&quot;TechSupport&quot;,&quot;StreamingTV&quot;,&quot;Contract&quot;,&quot;StreamingMovies&quot;,&quot;PaperlessBilling&quot;,&quot;PaymentMethod&quot;,&quot;Churn&quot;)]&lt;-lapply(x[c(&quot;Partner&quot;,&quot;Dependents&quot;,&quot;PhoneService&quot;,&quot;gender&quot;,&quot;MultipleLines&quot;,&quot;InternetService&quot;,&quot;OnlineSecurity&quot;,&quot;OnlineBackup&quot;,&quot;DeviceProtection&quot;,&quot;TechSupport&quot;,&quot;StreamingTV&quot;,&quot;Contract&quot;,&quot;StreamingMovies&quot;,&quot;PaperlessBilling&quot;,&quot;PaymentMethod&quot;,&quot;Churn&quot;)], as.factor) str(x) ## &#39;data.frame&#39;: 7032 obs. of 21 variables: ## $ customerID : chr &quot;7590-VHVEG&quot; &quot;5575-GNVDE&quot; &quot;3668-QPYBK&quot; &quot;7795-CFOCW&quot; ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 1 1 2 1 1 2 ... ## $ SeniorCitizen : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Partner : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 1 1 1 1 1 2 1 ... ## $ Dependents : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 2 1 1 2 ... ## $ tenure : int 1 34 2 45 2 8 22 10 28 62 ... ## $ PhoneService : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 2 1 2 2 2 1 2 2 ... ## $ MultipleLines : Factor w/ 3 levels &quot;No&quot;,&quot;No phone service&quot;,..: 2 1 1 2 1 3 3 2 3 1 ... ## $ InternetService : Factor w/ 3 levels &quot;DSL&quot;,&quot;Fiber optic&quot;,..: 1 1 1 1 2 2 2 1 2 1 ... ## $ OnlineSecurity : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 3 3 3 1 1 1 3 1 3 ... ## $ OnlineBackup : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 3 1 3 1 1 1 3 1 1 3 ... ## $ DeviceProtection: Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 3 1 3 1 3 1 1 3 1 ... ## $ TechSupport : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 3 1 1 1 1 3 1 ... ## $ StreamingTV : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 1 1 3 3 1 3 1 ... ## $ StreamingMovies : Factor w/ 3 levels &quot;No&quot;,&quot;No internet service&quot;,..: 1 1 1 1 1 3 1 1 3 1 ... ## $ Contract : Factor w/ 3 levels &quot;Month-to-month&quot;,..: 1 2 1 2 1 1 1 1 1 2 ... ## $ PaperlessBilling: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 2 1 2 2 2 1 2 1 ... ## $ PaymentMethod : Factor w/ 4 levels &quot;Bank transfer (automatic)&quot;,..: 3 4 4 1 3 3 2 4 3 1 ... ## $ MonthlyCharges : num 29.9 57 53.9 42.3 70.7 ... ## $ TotalCharges : num 29.9 1889.5 108.2 1840.8 151.7 ... ## $ Churn : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 2 1 2 2 1 1 2 1 ... We aim to convert our data set into matrix with only 0s and 1s. Thus, all binomial variables can be turned to 0s and 1s in the following way: # Binomial categorical variables to 0 and 1 x$PhoneService &lt;- as.numeric(x$PhoneService)-1 x$Partner &lt;-as.numeric(x$Partner)-1 x$Dependents &lt;- as.numeric(x$Dependents)-1 x$Churn &lt;- as.numeric(x$Churn)-1 x$gender &lt;- as.numeric(x$gender)-1 Now it is left to convert multi-class variables 0s and 1s. Before we do it, let’s inspect our data a bit: ## Categorical variables with more than 2 categories par(mfrow=c(5,2)) # Multiple lines ggplot(x, aes(MultipleLines,fill=MultipleLines)) + geom_bar() + labs(title=&quot;Multiple lines&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # Internet services ggplot(x, aes(InternetService,fill=InternetService)) + geom_bar() + labs(title=&quot;Internet service&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # OnlineSecurity ggplot(x, aes(OnlineSecurity,fill=OnlineSecurity)) + geom_bar() + labs(title=&quot;OnlineSecurity&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # OnlineBackup ggplot(x, aes(OnlineBackup,fill=OnlineBackup)) + geom_bar() + labs(title=&quot;OnlineBackup&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # DeviceProtection ggplot(x, aes(DeviceProtection,fill=DeviceProtection)) + geom_bar() + labs(title=&quot;DeviceProtection&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # TechSupport ggplot(x, aes(TechSupport,fill=TechSupport)) + geom_bar() + labs(title=&quot;TechSupport&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # StreamingTV ggplot(x, aes(StreamingTV,fill=StreamingTV)) + geom_bar() + labs(title=&quot;StreamingTV&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # StreamingMovies ggplot(x, aes(StreamingMovies,fill=StreamingMovies)) + geom_bar() + labs(title=&quot;StreamingMovies&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # Contract ggplot(x, aes(Contract,fill=Contract)) + geom_bar() + labs(title=&quot;Contract&quot;,x=&quot;&quot;,y=&quot;Count&quot;) # PaymentMethod ggplot(x, aes(PaymentMethod,fill=PaymentMethod)) + geom_bar() + labs(title=&quot;PaymentMethod&quot;,x=&quot;&quot;,y=&quot;Count&quot;) We can apply one hot encoding to our data set by using R’s base function model.matrix. In the code below, ~.+0 leads to encoding of all categorical variables without producing an intercept. # One-hot encoding x.mat&lt;- model.matrix(~MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+Contract+StreamingMovies+PaperlessBilling+PaymentMethod+0,data = x) Now is our data set pre-processed: # Creation of the final data frame with 0s and 1s final.df&lt;-as.data.frame(x.mat) final.df&lt;- cbind(x.mat,x$tenure,x$TotalCharges,x$MonthlyCharges,x$PhoneService,x$Partner,x$Dependents,x$gender,x$Churn) colnames(final.df)[24:31]&lt;-c(&quot;tenure&quot;,&quot;TotalCharges&quot;,&quot;MonthlyCharges&quot;,&quot;PhoneService&quot;,&quot;Partner&quot;,&quot;Dependents&quot;,&quot;gender&quot;,&quot;Churn&quot;) head(final.df) ## MultipleLinesNo MultipleLinesNo phone service MultipleLinesYes ## 1 0 1 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 0 1 0 ## 5 1 0 0 ## 6 0 0 1 ## InternetServiceFiber optic InternetServiceNo ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 1 0 ## 6 1 0 ## OnlineSecurityNo internet service OnlineSecurityYes ## 1 0 0 ## 2 0 1 ## 3 0 1 ## 4 0 1 ## 5 0 0 ## 6 0 0 ## OnlineBackupNo internet service OnlineBackupYes ## 1 0 1 ## 2 0 0 ## 3 0 1 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## DeviceProtectionNo internet service DeviceProtectionYes ## 1 0 0 ## 2 0 1 ## 3 0 0 ## 4 0 1 ## 5 0 0 ## 6 0 1 ## TechSupportNo internet service TechSupportYes StreamingTVNo internet service ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 1 0 ## 5 0 0 0 ## 6 0 0 0 ## StreamingTVYes ContractOne year ContractTwo year ## 1 0 0 0 ## 2 0 1 0 ## 3 0 0 0 ## 4 0 1 0 ## 5 0 0 0 ## 6 1 0 0 ## StreamingMoviesNo internet service StreamingMoviesYes PaperlessBillingYes ## 1 0 0 1 ## 2 0 0 0 ## 3 0 0 1 ## 4 0 0 0 ## 5 0 0 1 ## 6 0 1 1 ## PaymentMethodCredit card (automatic) PaymentMethodElectronic check ## 1 0 1 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 1 ## 6 0 1 ## PaymentMethodMailed check tenure TotalCharges MonthlyCharges PhoneService ## 1 0 1 29.85 29.85 0 ## 2 1 34 1889.50 56.95 1 ## 3 1 2 108.15 53.85 1 ## 4 0 45 1840.75 42.30 0 ## 5 0 2 151.65 70.70 1 ## 6 0 8 820.50 99.65 1 ## Partner Dependents gender Churn ## 1 1 0 0 0 ## 2 0 0 1 0 ## 3 0 0 1 1 ## 4 0 0 1 0 ## 5 0 0 0 1 ## 6 0 0 0 1 Now we need to split our data into test and train data. The proportion is 70:30. # Creation of training and test data sets index &lt;- caret::createDataPartition(x$Churn, p = 0.7, list = F) train &lt;- final.df[index,] test &lt;- final.df[-index,] # Partitioning test data x_test &lt;- as.matrix(test[,-31]) y_test &lt;- as.matrix(test[,31]) # Partitioning train data x_train &lt;- as.matrix(train[,-31]) y_train &lt;- as.matrix(train[,31]) To train our random forest model we use function randomForest(). At this point we will not aim to fine tune our model, so we will define just two parameters,ntree and maxnodes: ntree defines the number of trees to build in the forest. maxnodes defines the maximum number of terminal nodes each tree in the forest can have. # Random Forest # Training library(randomForest) rfModel &lt;- randomForest(x=x_train, y=factor(y_train), ntree=500, maxnodes=24) Since predictions are made based on features our model was trained on, it is possible to observe importance of each feature. The more important a feature, the greater influence it exerts on predictions: importance_features &lt;- randomForest::importance(rfModel) importance_features &lt;- as.data.frame(importance_features) importance_features$features &lt;- row.names(importance_features) importance_features &lt;- importance_features[order(importance_features$MeanDecreaseGini ,decreasing = TRUE),] library(plotly) p&lt;-ggplot(importance_features) + geom_point(aes(reorder(features,MeanDecreaseGini),MeanDecreaseGini),stat = &quot;identity&quot;)+ theme_minimal()+ coord_flip()+ labs(title=&quot;Important features&quot;,x=&quot;Features&quot;) ggplotly(p) As we can see from this output, the tenure feature seems to be the most important factor in making the final prediction. Factors such as InternetServiceFiber optic,TotalCharges and ContractTwo year come subsequently. 5.2 Evaluating Models In order to evaluate our model we will take a look at accuracy, precision and recall. # Evaluating Models prediction_insample &lt;- as.double(predict(rfModel, x_train)) - 1 prediction_outsample &lt;- as.double(predict(rfModel, x_test)) - 1 Accuracy is the percentage of correct predictions out of all predictions. # Accuracy accu_insample &lt;- mean(y_train == prediction_insample) accu_outsample &lt;- mean(y_test == prediction_outsample) print(sprintf(&#39;In-Sample Accuracy: %0.2f&#39;, accu_insample)) ## [1] &quot;In-Sample Accuracy: 0.80&quot; print(sprintf(&#39;Out-Sample Accuracy: %0.2f&#39;, accu_outsample)) ## [1] &quot;Out-Sample Accuracy: 0.78&quot; We managed to achieve pretty good out-sample accuracy even without thorough fine-tuning our parameters. Precision is the number of true positives divided by the total number of true positives and false positives. # Precision prec_insample &lt;- sum(prediction_insample &amp; y_train) / sum(prediction_insample) prec_outsample &lt;- sum(prediction_outsample &amp; y_test) / sum(prediction_outsample) print(sprintf(&#39;In-Sample Precision: %0.2f&#39;, prec_insample)) ## [1] &quot;In-Sample Precision: 0.70&quot; print(sprintf(&#39;Out-Sample Precision: %0.2f&#39;, prec_outsample)) ## [1] &quot;Out-Sample Precision: 0.72&quot; Recall is defined as the number of true positives divided by number of true positives plus false negatives. # Recall recall_insample &lt;- sum(prediction_insample &amp; y_train) / sum(y_train) recall_outsample &lt;- sum(prediction_outsample &amp; y_test) / sum(y_test) print(sprintf(&#39;In-Sample Recall: %0.4f&#39;, recall_insample)) ## [1] &quot;In-Sample Recall: 0.4055&quot; print(sprintf(&#39;Out-Sample Recall: %0.4f&#39;, recall_outsample)) ## [1] &quot;Out-Sample Recall: 0.3636&quot; Finally, in order to estimate how good our model is in comparison to the random prediction, we will inspect ROC curve and the Area Under the Curve. library(ROCR) pred_prob_insample &lt;- as.double(predict(rfModel, x_train, type=&#39;prob&#39;)[,2]) pred_prob_outsample &lt;- as.double(predict(rfModel, x_test, type=&#39;prob&#39;)[,2]) pred &lt;- prediction(pred_prob_outsample, y_test) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure=&#39;auc&#39;)@y.values[[1]] plot(perf,main=sprintf(&#39;Random Forest (AUC: %0.2f)&#39;, auc),col=&#39;darkblue&#39;,lwd=2) + grid() ## integer(0) abline(a = 0, b = 1, col=&#39;darkgray&#39;, lty=4, lwd=2) "]
]
