[
["index.html", "Mirza Mujanovic - Portfolio About me", " Mirza Mujanovic - Portfolio About me My name is Mirza Mujanović, I am an ever-curious, eager to learn and ready to adapt student in his third semester of Marketing master program at the Vienna University of Economics and Business. Originally, I come from Bosnia &amp; Herzegovina, however, I completed a bachelor’s program in business administration at the Vienna University of Economics and Business (WU). Currently, I hold a part-time position of teaching and eLearning assistant at the WU. Thinking of my future plans I see myself working in the field of data analytics or data science for marketing Random facts about me: Big basketball fan. Nowadays, fan of sports analytics. Interested in psychology-related topics. Used to collect old money together with my older sister. Favorite food: Bulgogi and pudding (but not together, of course!). "],
["emotions-in-online-customer-reviews.html", "1 Emotions In Online Customer Reviews What to expect in this article? 1.1 Dictionaries for NLP 1.2 Data set 1.3 Corpus cleaning 1.4 Visualisations of terms frequency 1.5 Sentiment analysis", " 1 Emotions In Online Customer Reviews Consumers usually seek quality and relevant information when buying new products. With the expansion and availability of the Internet, online consumer reviews have become a valuable resource to look at. Several studies tried to demystify relationship between product sales and online customer reviews. On the one hand, some of them, such as Senecal and Nantel (2004), suggest that participants who consulted product recommendations selected these products twice as often as those who did not consult recommendations. On the other hand, Zhang and Dellarocas (2006) find that online reviews and do not influence sales and serve solely as prediction. Between these two opinion fronts, one thing is certain: both sides aim to find out how consumers perceive and process word-of-mouth in a digital environment. In the academic paper The Role of Emotions for the Perceived Usefulness in Online Customer Reviews authors suggests that emotions impact the helpfulness ratings, i.e., the quality of online reviews as perceived by other customers. They found that, on average, the most prominent emotion dimensions that influence helpfulness ratings are trust, joy, and anticipation. Inspired by these findings, I decided to apply natural language processing techniques to analyze online customer reviews of a bestselling product on Amazon and try to detect those emotions using available lexicons. Final insights will show us whether trust, joy and anticipation can be identified in the reviews, thus improve helpfulness of reviews for potential customers. What to expect in this article? First, I will extract text via web-scrapping and form a corpus. Next, the text in the corpus will be pre-processed. Subsequently, from the pre-processed text will be stored in form of document-term-matrices or term-document matrices. Finally, an exploratory text analysis will be conducted and corresponding marketing implications pointed out. 1.1 Dictionaries for NLP For this exercise I will use 3 different lexicons available for R. One of them is AFINN, a lexicon of words rated for valence between minus five (indicating negative valence) and plus five (indicating positive valence). Next, I will use NRC Emotion Lexicon, which consists of English words and their labels for eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). # Dictionaries ---- afinn &lt;- get_sentiments(&quot;afinn&quot;) bing &lt;- get_sentiments(&quot;bing&quot;) loughran &lt;- get_sentiments(&quot;loughran&quot;) nrc &lt;- get_sentiment_dictionary(&#39;nrc&#39;, language = &quot;english&quot;) 1.2 Data set For our analysis, we will use text of 200 online customer reviews from Apple MacBook Pro (16-inch, 16GB RAM, 512GB Storage, 2.6GHz Intel Core i7) obtained in unpre-processed form: ## [1] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Great product. Replacement for my other macbook that lasted 7 years and was still going strong besides the battery. Great upgrade\\n\\n \\n&quot; ## [2] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Top notch until you don&#39;t use external monitor as well. Its common issue\\n\\n \\n&quot; ## [3] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n This is my first Mac and I love it. Apple is the BEST!\\n\\n \\n&quot; ## [4] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Basically, the laptop is a work of art. You know exactly what youre buying with an Apple product. I love that it comes with 16 GB of memory as default. The video card is very powerful. The touchpad is huge and works flawlessly. It is expensive, but worth every penny.\\n\\n \\n&quot; ## [5] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n A los dos dias de usarlo la pantalla se puso a rayas\\n\\n \\n \\n&quot; ## [6] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Product too heavy. Returned and have not received refund.\\n\\n \\n&quot; ## [7] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n I love this laptop - the screen resolution is beautiful, the light up touch bar is super nice and an innovative touch. I have recently converted form a PC user to MAC after all the years that I have enjoyed other apple products such as Ipad, and I doubt I&#39;d switch back.\\n\\n \\n&quot; ## [8] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n It is well made, Liked it had two positions when opened rather than just one. However, it would have been better if it had three reading positions when opened.\\n\\n \\n&quot; ## [9] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n Great laptop! I have been using it for months now and havent had any issues. Highly recommended\\n\\n \\n&quot; ## [10] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n I opened the box and was like, Hello Lover It&#39;s gorgeous.\\n\\n \\n&quot; 1.3 Corpus cleaning From the results above we could see that text contains unnecessary characters. Therefore, I will use some usual procedure to clean up the reviews’ text and make it more understandable. For the purpose of this exercise and for efficiency reasons, we will use the volatile corpus, that stores the collection of documents in RAM memory. To create a volatile corpus, I need to pass reviews’ text in such a form that each review text is interpretated as a document. # Creation of volatile corpus review.corpus &lt;- VCorpus(VectorSource(review$review_text)) We see that the volatile corpus contains as many documents as many online reviews we collected. To undertake a custom transformation, I will use tm package and content_transformer() function. It takes a custom function as input, which defines what transformation needs to be done: review.toSpace&lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;/&quot;) # remove &quot;/&quot; review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;@&quot;) # remove &quot;@&quot; review.corpus &lt;- tm_map(review.corpus, review.toSpace, &quot;\\\\|&quot;) # remove &quot;\\\\|&quot; review.corpus &lt;- tm_map(review.corpus, content_transformer(tolower)) # convert all capital letters to small review.corpus &lt;- tm_map(review.corpus, removeNumbers) # convert all capital letters to small review.corpus &lt;- tm_map(review.corpus, removeWords, stopwords(&quot;english&quot;)) # remove stop-words review.corpus &lt;- tm_map(review.corpus, removePunctuation) # remove punctuation review.corpus &lt;- tm_map(review.corpus, stripWhitespace) # strip extra whitespace from a document After cleaning the corpus, we can use document-term-matrix to store our cleaned corpus: review.dtm &lt;- DocumentTermMatrix(review.corpus) However, document-term-matrix is not the most suitable to work with, because it stores review texts in rows and terms frequencies in columns. We will transform it with tidy function: # Tidy up the document-term-matrix review.tidy &lt;- tidy(review.dtm) review.tidy$count &lt;-as.numeric(review.tidy$count) # Ensure correct class colnames(review.tidy)[2]&lt;- &#39;word&#39; # change name of the column from &quot;term&quot; to &quot;word&quot; review.tidy$document &lt;- as.numeric(review.tidy$document) # Ensure correct class Our tidy format has dimensions 6907 (the total number of terms) x 3 (document, term and count of the term in corresponding document): dim(review.tidy) # Dimensions ## [1] 6907 3 head(review.tidy)# Display first 6 rows ## # A tibble: 6 x 3 ## document word count ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 battery 1 ## 2 1 besides 1 ## 3 1 going 1 ## 4 1 great 2 ## 5 1 lasted 1 ## 6 1 macbook 1 1.4 Visualisations of terms frequency 1.4.1 Bar charts with the most frequent terms We would be interested in the most frequent words used in customer reviews. Sometimes just a glimpse of the most frequent words is sufficient to get some insights. Here we see that word “love” and “great” appears among most frequent terms. # Most frequent terms ---- review.tdm &lt;- TermDocumentMatrix(review.corpus) review.m &lt;- as.data.frame.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) word.names&lt;-names(review.v) df.review.v&lt;-data.frame(review.v,word.names) colnames(df.review.v)&lt;-c(&quot;n&quot;,&quot;word&quot;) p&lt;-ggplot(data=df.review.v[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;steelblue&quot;) + coord_flip() + ggtitle(&quot;20 most frequent words in customer reviews - MacBook Pro&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_test() ggplotly(p) 1.4.2 Wordcloud with the most frequent terms Similarly to the bar chart with the most frequent words, we could use wordcloud as well. It displays words from the corpus and signalizes their frequency by displaying more frequent words bigger relative to those that appear less frequently in the corpus. In the wordcloud below you can see 200 most frequent words, where the minimum frequency was set to 1. # Wordcloud review.tdm &lt;- TermDocumentMatrix(review.corpus) review.m &lt;- as.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) review.d &lt;- data.frame(word = names(review.v),freq=review.v) set.seed(1234) wordcloud(words = review.d$word, freq = review.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) 1.4.3 The most frequent terms indicating emotions When it comes to anticipation, words such as “good”,“time”,“happy” or “powerful” indicates that this emotion can be identified among customer reviews. On the other hand, there are some words that could be a signal both for good and bad experience: “finally”,“money” or “wait”. # Anticipation words---- nrc.anticipation &lt;- subset(nrc, nrc$sentiment==&quot;anticipation&quot;) review.anticipation.words &lt;- inner_join(review.tidy, nrc.anticipation) review.anticipation.words &lt;- count(review.anticipation.words, word) review.anticipation.words &lt;- review.anticipation.words[order(review.anticipation.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.anticipation.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;orange&quot;) + coord_flip() + ggtitle(&quot;20 most frequent anticipation words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) Similarly to anticipation, now we observe a list of top 20 words that indicate trust. It reveals new quite frequent term in the corpus: “recommend”. # Trust words---- nrc.trust &lt;- subset(nrc, nrc$sentiment==&quot;trust&quot;) review.trust.words &lt;- inner_join(review.tidy, nrc.trust) review.trust.words &lt;- count(review.trust.words, word) review.trust.words &lt;- review.trust.words[order(review.trust.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.trust.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;royalblue1&quot;) + coord_flip() + ggtitle(&quot;20 most frequent trust words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) Although at the bottom of the list, “The top 20 list” of joy words displays some additional words that we did not observe previously such as “beautiful”,“gorgeous”,“wonderful”,“improvement”,“excellent”. # Joy words ---- nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) review.joy.words &lt;- inner_join(review.tidy, nrc.joy) review.joy.words &lt;- count(review.joy.words, word) review.joy.words &lt;- review.joy.words[order(review.joy.words$n,decreasing = TRUE),] p&lt;-ggplot(data=review.joy.words[1:20,], aes(x=reorder(word,n), y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;darkorange1&quot;) + coord_flip() + ggtitle(&quot;20 most frequent trust words in customer reviews&quot;)+ xlab(&quot;Count&quot;)+ ylab(&quot;Word&quot;)+ theme_minimal() ggplotly(p) 1.5 Sentiment analysis 1.5.1 Polarity timeline One usual way to compare and quantify emotions in text is via polarity. We simply count number of unique words in each document (=review) labelled as negative and deduct from the count of unique positive words. For instance, the first review contains 2 unique positive words (“great” and “strong”) and none negative unique words. Therefore, its polarity score is 2. This polarity timeline suggests very important implication: the reviews’ sentiment is moving above the 0, bearly going even below +2, giving an indication that this product continuously meet customers’ expectations. That is a good signal to believe that customers are rather satisfied with the product. # Polarity timeline ---- review.sentiment &lt;- inner_join(review.tidy, bing) review.sentiment &lt;- count(review.sentiment, sentiment, index=document) review.sentiment &lt;- spread(review.sentiment, sentiment, n, fill=0) review.sentiment$polarity &lt;- review.sentiment$positive - review.sentiment$negative review.sentiment$pos &lt;- ifelse(review.sentiment$polarity &gt;=0, &quot;Positive&quot;, &quot;Negative&quot;) p&lt;-ggplot(review.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1)+theme_gdocs()+ggtitle(label=&quot;Polarity timeline&quot;) ggplotly(p) # Smooth curve review.smooth &lt;- ggplot(review.sentiment, aes(index, polarity)) p&lt;-review.smooth + stat_smooth() + theme_gdocs() + ggtitle(&quot;Polarity timeline - smooth&quot;) ggplotly(p) In the polarity graph at index 81 we identify a review with sentiment score of even 34! This seems to be a thrilled customer every brand loves! Let us take closer look: review.sentiment &lt;- inner_join(review.tidy, bing) doc_81&lt;-filter(review.sentiment, document==&quot;81&quot;) head(doc_81[order(doc_81$count,decreasing = T),]) ## # A tibble: 6 x 4 ## document word count sentiment ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 81 best 3 positive ## 2 81 better 3 positive ## 3 81 improved 3 positive ## 4 81 amazing 2 positive ## 5 81 breeze 2 positive ## 6 81 good 2 positive Finally, it certainly pays off to check the actual review: # Outlier in polarity score review$review_text[81] ## [1] &quot;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n \\n \\n If you&#39;ve been waiting for Apple to wake up and address the concerns raised by the Apple community -- your wait is now over. This is the MacBook Pro we&#39;ve all been wanting for years. This review is for the higher end model, stock.The good.- Keyboard. It&#39;s fantastic. The key travel has been extended to 1mm, which is about half of the original keyboard found on the 2015 and prior model years. It feels just as good to type on because apple improved the tactile feedback. The keys sort of spring back. The keyboard is quiet and very comfortable to type on. The keys are slightly smaller but do not make typing on them any more difficult. The directional arrows are now properly setup and the physical Esc key is back, making it a breeze to flip through open applications.- Screen. The difference in size is subtle but noticeable. It&#39;s technically even more dense, but just barely. You won&#39;t notice much difference from recent MacBook Pros but true to Apple, the display is absolutely gorgeous. The 0.6 inch difference in size retains the same logical resolution, so everything should look just slightly bigger which I welcome.- Processor. The base model has the same chipset as the 2019 15.4\\&quot; model. The performance, however, is about 10% better due to significantly improved airflow and larger heatsinks. The issue of heat related throttling has been largely addressed. On the upper model, the machine now comes with 8 cores and serious performance bump. There is literally nothing you can throw at this MacBook that it won&#39;t handle with breeze.- Graphics. The traditional setup is still here. You have the onboard Intel chipset, which operates when the demand for visual performance is low. You also have a discrete AMD chip which delivers significant improvement over the previous generation. In fact, the base model delivers performance in excess of the upper spec Vega chipset from last year. The leap is extraordinary. As before, the system will automatically choose which graphics card to use depending on demand.- Sound. In one world, amazing. Imagine Apple took a HomePod and flattened it to fit it in the housing of the MacBook Pro. That is essentially the experience. Bass is pronounced and crisp and treble is sharp. The sound is rich and room-filling. There are six speakers instead of two in the last generation.-Microphone. Major improvements with three mics instead of one in the previous generation. I use the MacBook to make calls using an iPhone and the sound on the other end is clear and free from background noice. I&#39;ve been told it sounds a lot better than before, but that is of course subjective. On paper, you&#39;re getting better noise reduction and improved sound fidelity.- Touch Bar. Controversial in the past, I think it may now be the \\&quot;happy medium\\&quot; between physical keys and the useful Touch Bar which adapts to the content on the screen. The Esc key is back and on the right hand side you&#39;ll find Touch ID and power button.- SSD. You&#39;ll love the fact that now base starting size of the SSD has been doubled on both the entry model and the upper model with 512GB and 1TB respectively.- Gaming. This has to be mentioned. The graphics card offers incredible leap in performance. Modern games that would get 14-16FPS on high setting now perform at 35-40FPS with ease. Same settings. Same games. Huge improvement. It&#39;s now possible to play AAA games on the MacBook Pro with reasonable performance and high visual settings.- Productivity. This machine is a beast. I use the full Affinity suite and do some limited video editing. In addition, I have multiple productivity programs open, over a dozen Safari tabs, two email clients, and dozens of other apps, such as CRM, task managers, notes, etc. Everything runs smoothly.- Value. Yes, value. This expensive laptop brings the best value in the lineup of the 15.\\&quot;4 (now 16\\&quot;) offering to date. If you carefully look at the costs of truly compatible Windows offerings, you&#39;ll find the MacBook Pro to be competitively priced.The Bad- Weight. The machine is slightly heavier but I welcome the added bulky. Finally Apple went with functionality over its obsession with thiner and lighter hardware, giving us a machine with proper thermal management, proper keyboard and more. But if you plan on taking it with you places, you&#39;ll feel the extra bulk.- BTO Updates. The cost of BTO options is still quite high, with the noted exception of the 8GB option on the GPU. You&#39;re still paying a significant premium for each incremental upgrade over the base model.To sum up -- This is the best MacBook Pro in many years. It&#39;s a well-rounded, powerful machine that brings about incredible performance and value. I would highly recommend it to any Pro.If you&#39;re upgrading from the 13\\&quot; model and wonder how much more real estate you can expect, see attached side-by-side comparison photo.Update 3/15/20 -- Absolutely a beast of a machine. I love it. It has been pretty much perfect and its performance continues to surprise me. I have the top-speced model and it smokes pretty much everything I&#39;ve used to date. In fact, it will perform on par with the new Mac Pro base configuration. Don&#39;t believe me? Google it. This is by far the best MacBook Pro to date and an amazing value. Well worth the price of admission if you need it.\\n\\n \\n&quot; It seems that our assumption was correct! The customer was definitely thrilled! This is a nice example how you can identify and take closer look at reviews that stand out based on its polarity score. 1.5.2 Analysis on sentence-level Text analysis provides freedom to choose level of observation. So far, we explored words and their frequencies, we explored customer reviews and quantified their sentiment in two dimensions (positive and negative). Next, we will approach the task of identifying the most negative and positive reviews by organizing text by sentences. By doing so, we will directly access those sentences whose average sentiment stand out. # Calculating the average sentiment review.highlighted&lt;-review$review_text%&gt;% get_sentences() %&gt;% sentiment_by() head(review.highlighted) ## element_id word_count sd ave_sentiment ## 1: 1 2 NA 0.35355339 ## 2: 2 16 NA 0.12500000 ## 3: 3 2 NA 0.35355339 ## 4: 4 10 NA 0.06324555 ## 5: 5 3 NA -0.57735027 ## 6: 6 9 NA 0.25000000 # Preparing data review.score &lt;- subset(review.highlighted, select = c(&quot;ave_sentiment&quot;,&quot;element_id&quot;)) review.worst &lt;- review.score[order(review.score$ave_sentiment,decreasing = FALSE),] review.worst&lt;-review.worst$element_id[1:10] review.best &lt;- review.score[order(review.score$ave_sentiment, decreasing = TRUE),] review.best &lt;- review.best$element_id[1:10] sentences&lt;-review$review_text %&gt;% get_sentences() sentences&lt;-as.matrix(sentences) And here we have “the worst 10 sentences” from customer reviews; # 10 worst sentences sentences[review.worst] ## [1] &quot;The screen is nice but has ridiculous ghosting.&quot; ## [2] &quot;Very disappointed.&quot; ## [3] &quot;Force cancelling woofers keep the annoying vibration at louder playback at bay.&quot; ## [4] &quot;I am very disappointed!&quot; ## [5] &quot;Having ports would have been nice (especially seeing how much space would be available for ports) but unfortunately, that is how Apple operates.&quot; ## [6] &quot;This is a Finder issue, as apps launch but don&#39;t leap to the top.&quot; ## [7] &quot;Im glad I didnt have important files to transfer or I would have been upset.&quot; ## [8] &quot;I guess I got a lemon, which is disappointing considering the high cost.&quot; ## [9] &quot;Small issue: fan comes up too often ( I do application development with it).&quot; ## [10] &quot;Very suspicious batch.&quot; Despite the fact that positive sentiment prevails, we see that there are certain problems associated with MacBook laptop. Issues with screen, problems with woofers, disappointment that there are no ports, unsatisfying value-price ratio. # 10 most positive sentences sentences[review.best] ## [1] &quot;Yes its considerably big and heavy compared to others, but Honestly that just feels like better quality to me&quot; ## [2] &quot;Quality is very good.&quot; ## [3] &quot;Extremely pleased with my NEW MacBook Pro!&quot; ## [4] &quot;This MacBook Pro 16\\&quot; is outstanding and much much better than the previous generation.&quot; ## [5] &quot;Overall, this new MacBook is rather pricey, but there&#39;s a reason it is called \\&quot;Pro\\&quot; because it is designed for professional.&quot; ## [6] &quot;Very fast!&quot; ## [7] &quot;Very fast, great screen, fantastic keyboard and wonderful speakers.&quot; ## [8] &quot;If you&#39;re upgrading from the 13\\&quot; model and wonder how much more real estate you can expect, see attached side-by-side comparison photo.&quot; ## [9] &quot;Well made computerBest specsNo much innovation from the 15 MacBook Pro that I Have.&quot; ## [10] &quot;I absolutely love this new Mac!&quot; If we take a look at “10 most positive sentences” from customer reviews, we would find a similar evidence as we obtained with polarity score. However, by reading those sentences a reader can have better feeling what the reviewer is actually satisfied or unsatisfied with. Here we see that some people admire the speed for instance. 1.5.3 What are the most emotional reviews? Package sentimentr provides nice function emotion() which uses a dictionary to find emotion words and then compute the rate per sentence. The final emotion score ranges between 0 (no emotion used) and 1 (all words used were emotional). # Extract emotions terms reviews.emotion &lt;- review$review_text %&gt;% get_sentences() %&gt;% emotion() # Top 50 sentences with the highest emotion score top_emotional_sentences &lt;- unique(reviews.emotion[order(reviews.emotion$emotion,decreasing = TRUE),]$element_id[1:50]) # The most emotional reviews sentences[top_emotional_sentences,] ## [1] &quot;Frustrated!&quot; &quot;PERFECT!!!!&quot; &quot;Finally!&quot; ## [4] &quot;Hot damn.&quot; &quot;Hot.&quot; &quot;Damn.&quot; ## [7] &quot;LOVE LOVE LOVE it!&quot; &quot;The Bad- Weight.&quot; &quot;Highly recommend.&quot; ## [10] &quot;Highly recommend.&quot; &quot;Outstanding laptop.&quot; &quot;Excellent computer&quot; ## [13] &quot;Loving it!&quot; &quot;Good computer.&quot; &quot;good product&quot; ## [16] &quot;Huge improvement.&quot; &quot;So disappointed.....&quot; &quot;Very disappointed.&quot; ## [19] &quot;so disappointed.&quot; &quot;Good keyboard.&quot; &quot;Ridiculously good.&quot; We can see that identified sentences very clearly reflect emotions that customers expressed. It seems that intensity of emotions is high in both positive and negative direction. Finally, we can plot detected emotions in order to get a bit more clear insight in emotional structure detected in the reviews: # Plot of emotion plot(reviews.emotion, transformation.function = syuzhet::get_dct_transform, drop.unused.emotions = TRUE, facet = TRUE) INTERPRETATION "],
["twitter-analysis-for-co2mustgo-initiative.html", "2 Twitter Analysis for CO2mustGo initiative 2.1 Tweets distribution 2.2 Word Frequency in tweets with #carbonfootprint or #co2 2.3 Word Network in tweets with #carbonfootprint or #co2 2.4 Wordcloud of words with #carbonfootprint #co2 2.5 Sentiment analysis: 2.6 Top retweeted Tweets 2.7 Network of retweets", " 2 Twitter Analysis for CO2mustGo initiative Tweets downloaded on 10.06.2020 carbon_tweets &lt;- read_twitter_csv(&quot;data/#carbonfootprintOR#greenhouse-tweets.csv&quot;, unflatten = T) carbon_tweets &lt;- carbon_tweets[, colSums(is.na(carbon_tweets)) != nrow(carbon_tweets)] # Removing any character that you don’t want to show in our text such as hyperlinks, @ mentions or punctuations carbon_tweets$stripped_text &lt;- gsub(&quot;https\\\\S*&quot;,&quot;&quot;, carbon_tweets$text) carbon_tweets$stripped_text &lt;- gsub(&quot;@\\\\S*&quot;,&quot;&quot;, carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;amp&quot;,&quot;&quot;,carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;[\\r\\n]&quot;,&quot;&quot;,carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;[[:punct:]]&quot;, &quot;&quot;,carbon_tweets$stripped_text) # Convert text to lowercase, punctuation is removed, occurrence/frequency of the each word will be added carbon_tweets_clean &lt;- carbon_tweets %&gt;% dplyr::select(stripped_text) %&gt;% unnest_tokens(word, stripped_text) # Remove stop words data(&quot;stop_words&quot;) carbon_tweets_words &lt;- carbon_tweets_clean %&gt;% anti_join(stop_words) 2.1 Tweets distribution # Distribution of tweets considered in the data. search_term &lt;- &#39;#carbonfootprint OR #co2&#39; by &lt;- &#39;hour&#39; p &lt;- ts_plot(carbon_tweets, by = by, trim = 2) + geom_point(col = &quot;#00acee&quot;) + theme_minimal() + labs(title = paste0(&quot;Tweets with &quot;,search_term,&quot; by &quot;,by),x = &#39;Date&#39;, y = &#39;Count&#39;, caption = &#39;Source: Twitter API&#39;) ggplotly(p) The peak of tweets has been registered on 6th of June. This happening has to be more closely analysed. 2.2 Word Frequency in tweets with #carbonfootprint or #co2 p &lt;- carbon_tweets_words %&gt;% dplyr::count(word, sort=T) %&gt;% top_n(10) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x = word, y = n)) + geom_col(fill=&quot;deepskyblue&quot;) + theme_minimal()+ xlab(NULL) + coord_flip() + labs(x = &quot;Count&quot;, y = &quot;Unique words&quot;, title = &quot;Count of unique words found in tweets&quot;) ggplotly(p) No major surprises regarding the most frequent words. 2.3 Word Network in tweets with #carbonfootprint or #co2 # Remove punctuation, convert to lowercase, add id for each tweet! carbon_tweets_paired_words &lt;- carbon_tweets %&gt;% dplyr::select(stripped_text) %&gt;% unnest_tokens(paired_words, stripped_text, token = &quot;ngrams&quot;, n = 2) #carbon_tweets_paired_words %&gt;% # dplyr::count(paired_words, sort = TRUE) #library(tidyr) carbon_tweets_separated_words &lt;- carbon_tweets_paired_words %&gt;% separate(paired_words, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) carbon_tweets_filtered &lt;- carbon_tweets_separated_words %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) carbon_words_counts &lt;- carbon_tweets_filtered %&gt;% dplyr::count(word1, word2, sort = TRUE) #library(igraph) #library(ggraph) # Plot carbon change word network p&lt;- carbon_words_counts %&gt;% filter(n &gt;=30) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = 0.5, edge_width = n)) + geom_node_point(color = &quot;darkslategray4&quot;, size = 3) + geom_node_text(aes(label = name), vjust = 1.8, size = 3) + labs(title = &quot;Word Network: Tweets using the hashtag #carbonfootprint or #co2&quot;, subtitle = &quot;Text mining twitter data &quot;, x = &quot;&quot;, y = &quot;&quot;) p The word network is made based on bi-grams. Basically, based on the number of times two words shows up together. Interesting observation is that the word network shows the word “justiceforvinayaki” appering together with “climatecrisis”. More specifically, “justiceforvinayaki” is actually a hashtag related to the story behind the pregnant elephant’s killing in Kerala’s Palakkad. More you can read here. 2.4 Wordcloud of words with #carbonfootprint #co2 # Text preparation carbon_tweets$stripped_text &lt;- iconv(carbon_tweets$stripped_text, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) review.docs &lt;- Corpus(VectorSource(carbon_tweets$stripped_text)) review.toSpace&lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;/&quot;) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;@&quot;) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;\\\\|&quot;) review.docs &lt;- tm_map(review.docs, content_transformer(tolower)) review.docs &lt;- tm_map(review.docs, removeNumbers) review.docs &lt;- tm_map(review.docs, removeWords, stopwords(&quot;english&quot;)) review.docs &lt;- tm_map(review.docs, content_transformer(tolower)) review.docs &lt;- tm_map(review.docs, removePunctuation) review.docs &lt;- tm_map(review.docs, stripWhitespace) # Text stemming #review.docs &lt;- tm_map(review.docs, stemDocument) review.tdm &lt;- TermDocumentMatrix(review.docs) review.m &lt;- as.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) review.d &lt;- data.frame(word = names(review.v),freq=review.v) set.seed(1234) wordcloud(words = review.d$word, freq = review.d$freq, max.words = 200, min.freq = 10, random.order=FALSE, rot.per=0.15, colors=brewer.pal(8, &quot;Dark2&quot;), scale=c(8,.3), vfont=c(&quot;sans serif&quot;,&quot;plain&quot;)) Minimum word frequency: 10 Number of words in the wordcloud: 200 library(wordcloud) library(reshape2) par(mar = rep(0, 4)) set.seed(1234) carbon_tweets_words%&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment,sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;darkred&quot;,&quot;darkgreen&quot;), max.words = 400, min.freq= 10, scale = c(4.0,0.25)) 2.5 Sentiment analysis: # Sentiment analysis sentiment &lt;- carbon_tweets[,3:5] %&gt;% unnest_tokens(output = &#39;word&#39;, input = &#39;text&#39;) #Add sentiment dataset sentiment_dataset &lt;- get_sentiments(&quot;afinn&quot;) sentiment_dataset &lt;- arrange(sentiment_dataset, -value) #Merge sentiment &lt;- merge(sentiment, sentiment_dataset, by = &#39;word&#39;) #Clean sentiment$word &lt;- NULL sentiment$screen_name &lt;- NULL #Time sentiment$hour &lt;- format(base::round.POSIXt(sentiment$created_at, units=&quot;hours&quot;), format=&quot;%H:%M&quot;) #Pivot pivot &lt;- sentiment %&gt;% group_by(hour) %&gt;% summarise(sentiment = mean(value)) #head(pivot) #plot p &lt;- ggplot(pivot[-1,], aes(x = hour, y = sentiment)) + geom_line(group = 1, color=&quot;deepskyblue&quot;) + geom_point() + theme_minimal() + labs(title = paste0(&#39;Average sentiment of tweetings mentioning &quot;&#39;,search_term,&#39;&quot;&#39;),x = &#39;Date&#39;, y = &#39;Sentiment&#39;, caption = &#39;Source: Twitter API&#39;) #p &lt;- ggplot(pivot[-1,], aes(x = hour, y = sentiment)) + geom_line(group = 1) + geom_point() + theme_minimal() + labs(title = paste0(&#39;Average sentiment of tweetings mentioning &quot;&#39;,search_term,&#39;&quot;&#39;),subtitle = paste0(pivot$hour[2],&#39; - &#39;,pivot$hour[nrow(pivot)],&#39; on &#39;,format(sentiment$created_at[1],&#39;%d %B %Y&#39;)),x = &#39;Date&#39;, y = &#39;Sentiment&#39;, caption = &#39;Source: Twitter API&#39;) ggplotly(p) ##Visualize the emotions # Get sentiments using the four different lexicons syuzhet &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;syuzhet&quot;) bing &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;bing&quot;) afinn &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;afinn&quot;) nrc &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;nrc&quot;) sentiments &lt;- data.frame(syuzhet, bing, afinn, nrc) # get the emotions using the NRC dictionary nrc.sentiment &lt;- get_nrc_sentiment(carbon_tweets$stripped_text) emo_bar = colSums(nrc.sentiment) emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar)) emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)]) # Visualize the emotions from NRC sentiments plot_ly(emo_sum, x=~emotion, y=~count, type=&quot;bar&quot;, color=~emotion) %&gt;% layout(xaxis=list(title=&quot;&quot;), showlegend=FALSE, title=&quot;Distribution of emotion categories&quot;) # Comparison word cloud all = c( paste(carbon_tweets$stripped_text[nrc.sentiment$anger &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$anticipation &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$disgust &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$fear &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$joy &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$sadness &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$surprise &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$trust &gt; 0], collapse=&quot; &quot;) ) all &lt;- removeWords(all, stopwords(&quot;english&quot;)) # create corpus corpus = Corpus(VectorSource(all)) # # create term-document matrix tdm = TermDocumentMatrix(corpus) # # convert as matrix tdm = as.matrix(tdm) tdm1 &lt;- tdm[nchar(rownames(tdm)) &lt; 11,] # # add column names colnames(tdm) = c(&#39;anger&#39;, &#39;anticipation&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;sadness&#39;, &#39;surprise&#39;, &#39;trust&#39;) colnames(tdm1) &lt;- colnames(tdm) comparison.cloud(tdm1, random.order=FALSE, colors = c(&quot;#00B2FF&quot;, &quot;red&quot;, &quot;#FF0099&quot;, &quot;#6600CC&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;blue&quot;, &quot;brown&quot;), title.size=1, max.words=250, scale=c(2.5, 0.4),rot.per=0.4) 2.6 Top retweeted Tweets 2.6.1 Top retweets (with equal or more than 60 mentions) # Select top retweeted tweets selected &lt;- which(carbon_tweets$retweet_count &gt;= 60) # Plot dates &lt;-as.POSIXct(strptime(carbon_tweets$created_at, format=&quot;%Y-%m-%d&quot;)) plot(x=dates, y=carbon_tweets$retweet_count, type=&quot;l&quot;, col=&quot;grey&quot;, xlab=&quot;Date&quot;, ylab=&quot;Times retweeted&quot;) colors &lt;- rainbow(10)[1:length(selected)] points(dates[selected], carbon_tweets$retweet_count[selected], pch=19, col=colors) 2.6.2 Interactive graph with retweets’ text # Plotly carbon_tweets$created_at &lt;-as.POSIXct(strptime(carbon_tweets$created_at, format=&quot;%Y-%m-%d&quot;)) p&lt;-ggplot(carbon_tweets, aes(x=created_at, y=retweet_count, col=retweet_count, size=retweet_count, retweet_text=retweet_text, created_at=created_at, retweet_name=retweet_name))+geom_point() +xlab(label=&quot;Date&quot;)+ylab(label=&quot;Retweet count&quot;)+ggtitle(label=&quot;Top retweeted tweets(hover over points to see text)&quot;) ggplotly(p,tooltip = c(&quot;retweet_text&quot;,&quot;retweet_name&quot;)) 2.7 Network of retweets # Create data frame for the network rt_df &lt;- carbon_tweets[, c(&quot;screen_name&quot; , &quot;retweet_screen_name&quot; )] # Remove rows with missing values rt_df_new &lt;- rt_df[complete.cases(rt_df), ] # Convert to matrix matrx &lt;- as.matrix(rt_df_new) # Create the retweet network nw_rtweet &lt;- graph_from_edgelist(el = matrx, directed = TRUE) # View the retweet network print.igraph(nw_rtweet) ## IGRAPH bece821 DN-- 1100 936 -- ## + attr: name (v/c) ## + edges from bece821 (vertex names): ## [1] Amrapali_c -&gt;cathrinejahnsen hanopcan -&gt;GreenTech_SWest ## [3] MehmetO33440789-&gt;UKHaulier m_carmody -&gt;crowdfarmingco ## [5] amrendrakumar02-&gt;SUPERGASind abhishekk85 -&gt;theswitchfix ## [7] MarkCNorwich -&gt;52WeeksForEarth RPiUptime -&gt;TrafficlyApp ## [9] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [11] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [13] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [15] RPiUptime -&gt;TrafficlyApp madebyhyphae -&gt;GreenTech_SWest ## + ... omitted several edges 2.7.1 Follower count of network users followers &lt;-carbon_tweets[, c(&quot;screen_name&quot; , &quot;followers_count&quot; )] # Remove rows with missing values followers &lt;- followers[complete.cases(rt_df), ] followers &lt;-unique(followers) # Categorize high and low follower count dim(followers) ## [1] 828 2 followers$follow &lt;- ifelse(followers$followers_count &gt; 500, &quot;1&quot;, &quot;0&quot;) # Assign external network attributes to retweet network V(nw_rtweet)$followers &lt;- followers$follow 2.7.2 Putting twitter data on the map (use plotly zoom in locations!) # Extract geolocation data and append new columns library(rtweet) library(sf) library(rnaturalearth) library(rnaturalearthdata) library(rgeos) pol_coord &lt;- lat_lng(carbon_tweets) pol_geo &lt;- na.omit(pol_coord[, c(&quot;lat&quot;, &quot;lng&quot;,&quot;location&quot;,&quot;retweet_count&quot;)]) world &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) p&lt;-ggplot(data = world) + geom_sf() + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + geom_point(data= pol_geo,aes(x=lng, y=lat,loc=location,retweet_count=retweet_count),col = &quot;#00acee&quot;)+ theme(panel.grid.major = element_line(color = gray(.25), linetype =&quot;dashed&quot;, size = 0.15),panel.background = element_rect(fill = &quot;aliceblue&quot;))+ ggtitle(&quot;World map with tweets location and retweet count&quot;, subtitle = paste0(&quot;(&quot;, length(unique(pol_geo$location)), &quot; countries)&quot;)) ggplotly(p,tooltip = c(&quot;location&quot;,&quot;retweet_count&quot;)) 2.7.3 Users who retweet the most # Calculate the out-degree scores out_degree &lt;- degree(nw_rtweet, mode = c(&quot;out&quot;)) # Sort the users in descending order of out-degree scores out_degree_sort &lt;- sort(out_degree, decreasing = TRUE) head(out_degree_sort,10) ## imagine_garden RPiUptime taxa_monocot Eco1stArt greenhousedave ## 9 8 7 5 5 ## pepparsteve LazarovMartin7 PetrovichBilly greentechdon researchmrx ## 4 4 4 4 3 # INTERPRETATION: Users who retweeted the most. #Hubs: Tweeter accounts with a lot of outgoing edges. hs &lt;- hub_score(nw_rtweet, weights=NA)$vector sort(hs, decreasing = TRUE)[1:20] ## SaachinPatel virendrathor007 ModheraAjay YuvraajsO mahisawOfficial ## 1 1 1 1 1 ## MayankS08111059 Maveric94280289 ujjain_live PayaswiniShett1 MaheshK70846514 ## 1 1 1 1 1 ## er_gaurav_singh mohanbhadri rushessensedood ShivanshikaF GMahindroo ## 1 1 1 1 1 ## avng47 berojgaradami SandeepKumawat_ _AdityaRaje kirtischandel ## 1 1 1 1 1 # #Ex-kurs: An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices. # They are likely to retweet. 2.7.4 Users who are the most retweeted # Calculate the in-degree scores in_degree &lt;- degree(nw_rtweet, mode = c(&quot;in&quot;)) # Sort the users in descending order of in-degree scores in_degree_sort &lt;- sort(in_degree, decreasing = TRUE) head(in_degree_sort,10) ## ReSanskrit PeacockSolar rockerblonde StellaYeahilike WaterlooEnergy ## 81 41 25 19 19 ## gulf_intel DanAlluf BPCLimited ByronTweetsData TrafficlyApp ## 17 17 15 14 13 # INTERPRETATION: Users whose posts were retweeted most. #Authorities: Tweeter accounts with a lot of incoming edges. as &lt;- authority_score(nw_rtweet, weights=NA)$vector sort(as, decreasing = TRUE)[1:20] ## ReSanskrit TrafficlyApp PeacockSolar OrchidOfTheDay rockerblonde ## 1.000000e+00 1.677825e-14 3.459879e-15 1.982355e-15 1.238491e-15 ## WaterlooEnergy DanAlluf BPCLimited ByronTweetsData IntlPeaceBureau ## 8.468956e-16 8.319809e-16 8.051224e-16 7.793320e-16 7.423804e-16 ## starindia L_FudgerGalvez ESA_EO gulf_intel StellaYeahilike ## 7.423804e-16 6.518274e-16 5.949006e-16 5.739893e-16 5.672778e-16 ## crowdfarmingco katyalston gikiearth CrownPlatform AyiccZim ## 5.267684e-16 5.267684e-16 5.197328e-16 4.809261e-16 4.755414e-16 #Ex-kurs: An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices. # They are likely to be retweeted. 2.7.5 Users with important role in allowing information to pass through network Users with higher betweenness has more control over the network. # Calculate the betweenness scores of the network betwn_nw &lt;- betweenness(nw_rtweet, directed = TRUE) # Sort the users in descending order of betweenness scores betwn_nw_sort &lt;- betwn_nw %&gt;% sort(decreasing = TRUE) %&gt;% round() %&gt;% head(10) betwn_nw_sort ## gikiearth Privatecarfree ChinyeRumby DioxideMat LifexSoles ## 6 6 4 2 1 ## esuohneerg swift_iron NGO_NISWARTH BPCLBareilly Va3tsal ## 1 1 1 1 1 2.7.6 Clustering largest_cliques(nw_rtweet) #list only 20 vertices in that cluster ## [[1]] ## + 3/1100 vertices, named, from bece821: ## [1] BpclStateLPGUP BPCLimited BPCLBareilly ## ## [[2]] ## + 3/1100 vertices, named, from bece821: ## [1] iona_nyandoro LifexSoles LxS_Build ## ## [[3]] ## + 3/1100 vertices, named, from bece821: ## [1] MKY110987 BPCLimited BPCLBareilly ## ## [[4]] ## + 3/1100 vertices, named, from bece821: ## [1] DarlacGardening WaterWorxUK swift_iron 2.7.7 Community detection #https://rpubs.com/cosmopolitanvan/twitternetworks #Community detection based on edge betweenness (Newman-Girvan) comm &lt;- cluster_edge_betweenness(nw_rtweet) sort(sizes(comm), decreasing = T)[1:20] ## Community sizes ## 60 44 16 74 98 31 108 233 205 58 42 114 91 110 21 57 4 37 48 99 ## 82 42 31 25 22 20 20 18 15 14 13 13 12 12 11 11 10 10 10 10 comm_1 &lt;- communities(comm) # Tweet accounts in the Community 60 (the biggest community) comm_1$`60` ## [1] &quot;SaachinPatel&quot; &quot;ReSanskrit&quot; &quot;virendrathor007&quot; &quot;ModheraAjay&quot; ## [5] &quot;YuvraajsO&quot; &quot;mahisawOfficial&quot; &quot;MayankS08111059&quot; &quot;Maveric94280289&quot; ## [9] &quot;ujjain_live&quot; &quot;PayaswiniShett1&quot; &quot;MaheshK70846514&quot; &quot;er_gaurav_singh&quot; ## [13] &quot;mohanbhadri&quot; &quot;rushessensedood&quot; &quot;ShivanshikaF&quot; &quot;GMahindroo&quot; ## [17] &quot;avng47&quot; &quot;berojgaradami&quot; &quot;SandeepKumawat_&quot; &quot;_AdityaRaje&quot; ## [21] &quot;kirtischandel&quot; &quot;Sai_3196&quot; &quot;Abhi_Gosavi17&quot; &quot;g_one01&quot; ## [25] &quot;amritanshu20712&quot; &quot;Shrirang_4u&quot; &quot;ChandanSahaDas1&quot; &quot;HGhumnar&quot; ## [29] &quot;im_zala&quot; &quot;the_bhaveshp&quot; &quot;SolankiDhavalJ&quot; &quot;AshokDh10070811&quot; ## [33] &quot;VGilankar&quot; &quot;free_sridhar&quot; &quot;looookeeee&quot; &quot;_dactar_babu&quot; ## [37] &quot;QnalH&quot; &quot;Bhuhan_Raut12&quot; &quot;thakurmanish25&quot; &quot;Dvipalgoswami&quot; ## [41] &quot;SSSPrashantS&quot; &quot;pahariparul&quot; &quot;shwetarathi0301&quot; &quot;Pournimakothap1&quot; ## [45] &quot;SKMaisuriya&quot; &quot;gandhecha_deep&quot; &quot;ravi_patel21&quot; &quot;SarveshdKumar&quot; ## [49] &quot;jenilsaurabh&quot; &quot;Byomkesh5&quot; &quot;vishaltechrexx&quot; &quot;Shubham82408211&quot; ## [53] &quot;Damnshashi&quot; &quot;ms_ayushi&quot; &quot;SankalpKr&quot; &quot;rchandra12&quot; ## [57] &quot;Vishalcr7999&quot; &quot;ub1112&quot; &quot;VishalRajShodhi&quot; &quot;prats_ag&quot; ## [61] &quot;Priyank64671386&quot; &quot;RudreshScharma&quot; &quot;naagaputra&quot; &quot;ipradyu&quot; ## [65] &quot;Prakash13119341&quot; &quot;anitajha2802&quot; &quot;Ankit_yadaavv&quot; &quot;sanatan_&quot; ## [69] &quot;Raj_Sawant96&quot; &quot;sohil_oza&quot; &quot;NoAbsurdity&quot; &quot;PujariRaksha&quot; ## [73] &quot;ddhamaal_bai&quot; &quot;AyushmanDubey18&quot; &quot;NSK_Kochhar&quot; &quot;HadaniSuresh&quot; ## [77] &quot;binayamishra16&quot; &quot;realMeetu&quot; &quot;Sanmon96952110&quot; &quot;ItsRajatRai&quot; ## [81] &quot;ramkotipalli&quot; &quot;Omtripathi95&quot; # Tweet accounts in the Community 44 (the second biggest community) comm_1$`44` ## [1] &quot;TushitGarg&quot; &quot;PeacockSolar&quot; &quot;ianikk18&quot; &quot;rupeshwar_rao&quot; ## [5] &quot;imjyotidwivedi&quot; &quot;NandiniRajendr4&quot; &quot;5a13fec9874b479&quot; &quot;Moanish58105963&quot; ## [9] &quot;sonukau18053840&quot; &quot;snipervenom21&quot; &quot;RobinSingh1825&quot; &quot;Shalmalichakra3&quot; ## [13] &quot;RubalSh04483180&quot; &quot;arun_thevan03&quot; &quot;AshishK54595782&quot; &quot;BasantKKothari&quot; ## [17] &quot;SriAtul2&quot; &quot;roshan_munda&quot; &quot;KillerSrinivas2&quot; &quot;RitanshuChugh&quot; ## [21] &quot;KunalSh62765789&quot; &quot;Rampras17015630&quot; &quot;manvpandit&quot; &quot;27shikhar&quot; ## [25] &quot;anshulajin&quot; &quot;KumariMitya&quot; &quot;EmescoFdn&quot; &quot;AtishKu82064443&quot; ## [29] &quot;ReetuShukla12&quot; &quot;VIJAYAS36039459&quot; &quot;Shashwa38747031&quot; &quot;ManishV59713691&quot; ## [33] &quot;Sanjeev16620270&quot; &quot;Aashish04884619&quot; &quot;ArchitG36451766&quot; &quot;Vikrant64760265&quot; ## [37] &quot;bunny3298&quot; &quot;Joydeep85178197&quot; &quot;KishoreH14&quot; &quot;HarshTi80357066&quot; ## [41] &quot;Sivas_07&quot; &quot;AbhijitOjha7&quot; # Tweet accounts in the Community 16 (the second biggest community) comm_1$`16` ## [1] &quot;gardener_the&quot; &quot;HoratiosGarden&quot; &quot;BrownBurden&quot; &quot;VivienLloyd&quot; ## [5] &quot;LazarovMartin7&quot; &quot;FreeDealSteals&quot; &quot;theOGryankirk&quot; &quot;StellaYeahilike&quot; ## [9] &quot;vhhydroponics&quot; &quot;agritecture&quot; &quot;PlantGetEnough1&quot; &quot;GladerPhilip&quot; ## [13] &quot;Woroud&quot; &quot;tepanchinceva&quot; &quot;Smart_Reads&quot; &quot;birdwriter7&quot; ## [17] &quot;hiro_thoyou3864&quot; &quot;Watrasri&quot; &quot;AgtechOtori&quot; &quot;sthanleyc&quot; ## [21] &quot;richardelio_nyc&quot; &quot;Yeahilike&quot; &quot;azkamel1&quot; &quot;edohpa&quot; ## [25] &quot;romi_hime_black&quot; &quot;banan_thompson&quot; &quot;StellaSanLF&quot; &quot;PeLopez1&quot; ## [29] &quot;RTusuzuro_&quot; &quot;umada_ushijiro&quot; &quot;Sun_Flower119&quot; "],
["text-analysis-of-novels.html", "3 Text Analysis of Novels", " 3 Text Analysis of Novels Downloading Novels ‘The Awakening’ and ‘The Aspern Papers’ awakening &lt;- scan(&quot;http://www.gutenberg.org/files/160/160-0.txt&quot;, what=&quot;character&quot;, blank.lines.skip = TRUE, sep=&quot;\\n&quot;) aspern &lt;- scan(&quot;http://www.gutenberg.org/files/211/211-0.txt&quot;, what=&quot;character&quot;, blank.lines.skip = TRUE, sep=&quot;\\n&quot;) awakening&lt;- iconv(awakening, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) aspern&lt;- iconv(aspern, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) Cleaning the novels awakening.begin &lt;- which(awakening==&quot;THE AWAKENING&quot;)[2] awakening.end &lt;- which(awakening==&quot;*****&quot;) - 1 awakening.v&lt;- awakening[awakening.begin:awakening.end] head(awakening.v) ## [1] &quot;THE AWAKENING&quot; ## [2] &quot;I&quot; ## [3] &quot;A green and yellow parrot, which hung in a cage outside the door, kept&quot; ## [4] &quot;repeating over and over:&quot; ## [5] &quot;Allez vous-en! Allez vous-en! Sapristi! That&#39;s all right!&quot; ## [6] &quot;He could speak a little Spanish, and also a language which nobody&quot; tail(awakening.v) ## [1] &quot;She looked into the distance, and the old terror flamed up for an&quot; ## [2] &quot;instant, then sank again. Edna heard her father&#39;s voice and her sister&quot; ## [3] &quot;Margaret&#39;s. She heard the barking of an old dog that was chained to the&quot; ## [4] &quot;sycamore tree. The spurs of the cavalry officer clanged as he walked&quot; ## [5] &quot;across the porch. There was the hum of bees, and the musky odor of pinks&quot; ## [6] &quot;filled the air.&quot; aspern.begin &lt;- which(aspern==&quot;Macmillan and Co., 1888.&quot;)+1 aspern.end &lt;- which(aspern==&quot;End of the Project Gutenberg EBook of The Aspern Papers, by Henry James&quot;) - 1 aspern.v &lt;- aspern[aspern.begin:aspern.end] head(aspern.v) ## [1] &quot;I&quot; ## [2] &quot;I had taken Mrs. Prest into my confidence; in truth without her I&quot; ## [3] &quot;should have made but little advance, for the fruitful idea in the whole&quot; ## [4] &quot;business dropped from her friendly lips. It was she who invented the&quot; ## [5] &quot;short cut, who severed the Gordian knot. It is not supposed to be the&quot; ## [6] &quot;nature of women to rise as a general thing to the largest and most&quot; tail(aspern.v) ## [1] &quot;of Jeffrey Aspern a larger sum of money than I had hoped to be able to&quot; ## [2] &quot;gather for her, writing to her that I had sold the picture, she kept it&quot; ## [3] &quot;with thanks; she never sent it back. I wrote to her that I had sold the&quot; ## [4] &quot;picture, but I admitted to Mrs. Prest, at the time (I met her in London,&quot; ## [5] &quot;in the autumn), that it hangs above my writing table. When I look at it&quot; ## [6] &quot;my chagrin at the loss of the letters becomes almost intolerable.&quot; Awakening: Cutting in chapters and corpus creation awakening.v &lt;- gsub(&quot;^I*(X|V)*I*$&quot;, &quot;@@@&quot;, awakening.v) awakening.string &lt;- paste(awakening.v, collapse = &quot; &quot;) awakening.chapters &lt;- strsplit(awakening.string, &quot;@@@ &quot;) awakening.df &lt;- as.data.frame(awakening.chapters, stringsAsFactors = FALSE) awakening.df &lt;-awakening.df[2:38,1] awakening.df &lt;- as.data.frame(awakening.df) colnames(awakening.df) &lt;- &quot;chapters&quot; awakening.docs &lt;- Corpus(VectorSource(awakening.df$chapters)) Text preprocessing toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;/&quot;) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;@&quot;) awakening.docs &lt;- tm_map(awakening.docs, toSpace, &quot;\\\\|&quot;) awakening.docs &lt;- tm_map(awakening.docs, content_transformer(tolower)) awakening.docs &lt;- tm_map(awakening.docs, removeNumbers) awakening.docs &lt;- tm_map(awakening.docs, removeWords, stopwords(&quot;english&quot;)) awakening.docs &lt;- tm_map(awakening.docs, removePunctuation) awakening.docs &lt;- tm_map(awakening.docs, stripWhitespace) WordCloud preparation and wordcloud graph awakening.dtm &lt;- DocumentTermMatrix(awakening.docs, control=list(weighting=weightTf)) awakening.m &lt;- as.matrix(t(awakening.dtm)) awakening_v &lt;- sort(rowSums(awakening.m),decreasing=TRUE) awakening.d &lt;- data.frame(word = names(awakening_v),freq=awakening_v) head(awakening.d, 10) ## word freq ## edna edna 287 ## one one 194 ## upon upon 190 ## pontellier pontellier 180 ## little little 148 ## robert robert 148 ## mrs mrs 145 ## said said 143 ## madame madame 126 ## like like 124 set.seed(1234) wordcloud(words = awakening.d$word, freq = awakening.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) Aspern Cutting in chapters and corpus creation aspern.v &lt;- gsub(&quot;^I*(X|V)*I*$&quot;, &quot;@@@&quot;, aspern.v) aspern.string &lt;- paste(aspern.v, collapse = &quot; &quot;) aspern.chapters &lt;- strsplit(aspern.string, &quot;@@@ &quot;) aspern.df &lt;- as.data.frame(aspern.chapters, stringsAsFactors = FALSE) aspern.df &lt;-aspern.df[2:38,1] aspern.df &lt;- as.data.frame(aspern.df) colnames(aspern.df) &lt;- &quot;chapters&quot; aspern.docs &lt;- Corpus(VectorSource(aspern.df$chapters)) Text preprocessing toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;/&quot;) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;@&quot;) aspern.docs &lt;- tm_map(aspern.docs, toSpace, &quot;\\\\|&quot;) aspern.docs &lt;- tm_map(aspern.docs, content_transformer(tolower)) aspern.docs &lt;- tm_map(aspern.docs, removeNumbers) aspern.docs &lt;- tm_map(aspern.docs, removeWords, stopwords(&quot;english&quot;)) aspern.docs &lt;- tm_map(aspern.docs, removePunctuation) aspern.docs &lt;- tm_map(aspern.docs, stripWhitespace) WordCloud preparation and wordcloud graph aspern.dtm &lt;- DocumentTermMatrix(aspern.docs, control=list(weighting=weightTf)) aspern.m &lt;- as.matrix(t(aspern.dtm)) aspern_v &lt;- sort(rowSums(aspern.m),decreasing=TRUE) aspern.d &lt;- data.frame(word = names(aspern_v),freq=aspern_v) head(aspern.d, 10) ## word freq ## miss miss 264 ## tita tita 173 ## said said 149 ## know know 111 ## old old 105 ## dont dont 96 ## made made 85 ## little little 84 ## see see 82 ## one one 80 set.seed(1234) wordcloud(words = aspern.d$word, freq = aspern.d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) Commonality cloud and Comparison cloud Creation of a corpus out of the both texts cc.docs &lt;- Corpus(VectorSource(c(awakening.string,aspern.string))) Text preprocessing toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;/&quot;) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;@&quot;) cc.docs &lt;- tm_map(cc.docs, toSpace, &quot;\\\\|&quot;) cc.docs &lt;- tm_map(cc.docs, content_transformer(tolower)) cc.docs &lt;- tm_map(cc.docs, removeNumbers) cc.docs &lt;- tm_map(cc.docs, removeWords, stopwords(&quot;english&quot;)) cc.docs &lt;- tm_map(cc.docs, removePunctuation) cc.docs &lt;- tm_map(cc.docs, stripWhitespace) 3.Comparison and Commonality cloud cc.dtm &lt;- DocumentTermMatrix(cc.docs) cc.m &lt;- as.matrix(t(cc.dtm)) colnames(cc.m)&lt;- c(&quot;Awakening&quot;,&quot;Aspern&quot;) comparison.cloud(cc.m,max.words = 100,min.frrandom.order=FALSE) commonality.cloud(cc.m,max.words = 100,colors = &quot;steelblue1&quot;,min.frrandom.order=FALSE) Sentiment timeline 1.Awakening sent.awakening &lt;- readLines(&quot;http://www.gutenberg.org/files/160/160-0.txt&quot;) sent.awakening &lt;-iconv(sent.awakening, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) awakening.corpus &lt;- VCorpus(VectorSource(sent.awakening)) awakening.corpus &lt;- tm_map(awakening.corpus, content_transformer(tolower)) awakening.corpus &lt;- tm_map(awakening.corpus, removeNumbers) awakening.corpus &lt;- tm_map(awakening.corpus, removeWords, stopwords(&quot;english&quot;)) awakening.corpus &lt;- tm_map(awakening.corpus, removePunctuation) awakening.corpus &lt;- tm_map(awakening.corpus, stripWhitespace) awakening.dtm &lt;- DocumentTermMatrix(awakening.corpus) awakening.tidy &lt;- tidy(awakening.dtm) awakening.tidy$count &lt;-as.numeric(awakening.tidy$count) colnames(awakening.tidy)[2]&lt;- &#39;word&#39; awakening.tidy$document &lt;- as.numeric(awakening.tidy$document) nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) joy.words &lt;- inner_join(awakening.tidy, nrc.joy) joy.words &lt;- count(joy.words, word) bing &lt;- get_sentiments(&quot;bing&quot;) awakening.sentiment &lt;- inner_join(awakening.tidy, bing) awakening.sentiment &lt;- count(awakening.sentiment, sentiment, index=document) awakening.sentiment &lt;- spread(awakening.sentiment, sentiment, n, fill=0) awakening.sentiment$polarity &lt;- awakening.sentiment$positive - awakening.sentiment$negative awakening.sentiment$pos &lt;- ifelse(awakening.sentiment$polarity &gt;=0, &quot;pos&quot;, &quot;neg&quot;) ggplot(awakening.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1) awakening.smooth &lt;- ggplot(awakening.sentiment, aes(index, polarity)) (p2&lt;-awakening.smooth + stat_smooth()) 2.Aspern sent.aspern &lt;- readLines(&quot;http://www.gutenberg.org/files/211/211-0.txt&quot;) aspern.corpus &lt;- VCorpus(VectorSource(sent.aspern)) aspern.corpus &lt;- tm_map(aspern.corpus, content_transformer(tolower)) aspern.corpus &lt;- tm_map(aspern.corpus, removeNumbers) aspern.corpus &lt;- tm_map(aspern.corpus, removeWords, stopwords(&quot;english&quot;)) aspern.corpus &lt;- tm_map(aspern.corpus, removePunctuation) aspern.corpus &lt;- tm_map(aspern.corpus, stripWhitespace) sent.aspern &lt;-iconv(sent.aspern, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) aspern.dtm &lt;- DocumentTermMatrix(aspern.corpus) aspern.tidy &lt;- tidy(aspern.dtm) aspern.tidy$count &lt;-as.numeric(aspern.tidy$count) colnames(aspern.tidy)[2]&lt;- &#39;word&#39; aspern.tidy$document &lt;- as.numeric(aspern.tidy$document) nrc.joy &lt;- subset(nrc, nrc$sentiment==&quot;joy&quot;) joy.words &lt;- inner_join(aspern.tidy, nrc.joy) joy.words &lt;- count(joy.words, word) #bing &lt;- subset(sentiments, sentiments$lexicon==&#39;bing&#39;)[,-4] aspern.sentiment &lt;- inner_join(aspern.tidy, bing) aspern.sentiment &lt;- count(aspern.sentiment, sentiment, index=document) aspern.sentiment &lt;- spread(aspern.sentiment, sentiment, n, fill=0) aspern.sentiment$polarity &lt;- aspern.sentiment$positive - aspern.sentiment$negative aspern.sentiment$pos &lt;- ifelse(aspern.sentiment$polarity &gt;=0, &quot;pos&quot;, &quot;neg&quot;) ggplot(aspern.sentiment, aes(x=index, y=polarity, fill=pos))+geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, width=1) aspern.smooth &lt;- ggplot(aspern.sentiment, aes(index, polarity)) (p3&lt;-aspern.smooth + stat_smooth()) Merging Aspern and Awakening g2 &lt;- ggplotGrob(p2) g3 &lt;- ggplotGrob(p3) g &lt;- rbind(g2, g3, size = &quot;first&quot;) g$widths &lt;- unit.pmax(g2$widths, g3$widths) grid.newpage() grid.draw(g) Topic Modelling - Pick up 5 topics and try to make sense of the topics giving a label to them 1.Awakening top.mod.awakening.dtm &lt;- DocumentTermMatrix(awakening.corpus, control = list(weighting=weightTf)) burnin = 1000 iter = 1000 keep = 50 set.seed(510) top.mod.awakening.n &lt;- nrow(top.mod.awakening.dtm) top.mod.awakening.dtm &lt;- top.mod.awakening.dtm[row_sums(top.mod.awakening.dtm &gt; 0) &gt; 1,] top.mod.awakening.lda_basic.model&lt;- LDA(top.mod.awakening.dtm, k = 5L, method = &quot;Gibbs&quot;, control = list(burnin = burnin, iter = iter, keep = keep, alpha = 1) ) top.mod.awakening.lda.topics &lt;- as.matrix(topics(top.mod.awakening.lda_basic.model)) #top.mod.awakening.lda.topics top.mod.awakening.lda.terms &lt;- as.matrix(terms(top.mod.awakening.lda_basic.model, 10)) top.mod.awakening.lda.terms&lt;- iconv(top.mod.awakening.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #top.mod.awakening.lda.terms awakening.top10termsPerTopic &lt;- terms(top.mod.awakening.lda_basic.model, 10) awakening.top10termsPerTopic &lt;- iconv(top.mod.awakening.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #awakening.top10termsPerTopic colnames(awakening.top10termsPerTopic)&lt;-c(&quot;Edna in her house/room&quot;,&quot; Work, life and love - Leaving the papers after he/she died&quot;,&quot;Madame Edna - Mademoiselle Ratignolle - Robert&quot;,&quot;Emotions - Description of a face &quot;,&quot;Mrs.Pontellier - Edna - Robert&quot;) awakening.top10termsPerTopic ## Edna in her house/room ## [1,] &quot;one&quot; ## [2,] &quot;like&quot; ## [3,] &quot;never&quot; ## [4,] &quot;thought&quot; ## [5,] &quot;day&quot; ## [6,] &quot;might&quot; ## [7,] &quot;seemed&quot; ## [8,] &quot;children&quot; ## [9,] &quot;two&quot; ## [10,] &quot;felt&quot; ## Work, life and love - Leaving the papers after he/she died ## [1,] &quot;project&quot; ## [2,] &quot;work&quot; ## [3,] &quot;must&quot; ## [4,] &quot;gutenbergtm&quot; ## [5,] &quot;without&quot; ## [6,] &quot;new&quot; ## [7,] &quot;found&quot; ## [8,] &quot;many&quot; ## [9,] &quot;feeling&quot; ## [10,] &quot;full&quot; ## Madame Edna - Mademoiselle Ratignolle - Robert ## [1,] &quot;upon&quot; ## [2,] &quot;eyes&quot; ## [3,] &quot;face&quot; ## [4,] &quot;hand&quot; ## [5,] &quot;looked&quot; ## [6,] &quot;sat&quot; ## [7,] &quot;white&quot; ## [8,] &quot;night&quot; ## [9,] &quot;table&quot; ## [10,] &quot;took&quot; ## Emotions - Description of a face Mrs.Pontellier - Edna - Robert ## [1,] &quot;little&quot; &quot;pontellier&quot; ## [2,] &quot;madame&quot; &quot;mrs&quot; ## [3,] &quot;went&quot; &quot;said&quot; ## [4,] &quot;away&quot; &quot;edna&quot; ## [5,] &quot;old&quot; &quot;will&quot; ## [6,] &quot;edna&quot; &quot;know&quot; ## [7,] &quot;back&quot; &quot;time&quot; ## [8,] &quot;house&quot; &quot;come&quot; ## [9,] &quot;ratignolle&quot; &quot;good&quot; ## [10,] &quot;robert&quot; &quot;mademoiselle&quot; awakening.topicNames &lt;- apply(awakening.top10termsPerTopic, 2, paste, collapse=&quot; &quot;) #awakening.topicNames Aspern top.mod.aspern.dtm &lt;- DocumentTermMatrix(aspern.corpus, control = list(weighting=weightTf)) burnin = 1000 iter = 1000 keep = 50 set.seed(510) top.mod.aspern.n &lt;- nrow(top.mod.aspern.dtm) top.mod.aspern.dtm &lt;- top.mod.aspern.dtm[row_sums(top.mod.aspern.dtm &gt; 0) &gt; 1,] top.mod.aspern.lda_basic.model&lt;- LDA(top.mod.aspern.dtm, k = 5L, method = &quot;Gibbs&quot;, control = list(burnin = burnin, iter = iter, keep = keep, alpha = 1) ) top.mod.aspern.lda.topics &lt;- as.matrix(topics(top.mod.aspern.lda_basic.model)) #top.mod.aspern.lda.topics top.mod.aspern.lda.terms &lt;- as.matrix(terms(top.mod.aspern.lda_basic.model, 10)) top.mod.aspern.lda.terms&lt;- iconv(top.mod.aspern.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #top.mod.aspern.lda.terms aspern.top10termsPerTopic &lt;- terms(top.mod.aspern.lda_basic.model, 10) aspern.top10termsPerTopic &lt;- iconv(top.mod.aspern.lda.terms, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) #aspern.top10termsPerTopic colnames(aspern.top10termsPerTopic)&lt;-c(&quot;Home - Seeing somebody - description of eyes - leaving&quot;,&quot;Jeffrey&quot;,&quot;Tita&quot;,&quot;Aunt&quot;,&quot;Time - Woman - Venice&quot;) aspern.top10termsPerTopic ## Home - Seeing somebody - description of eyes - leaving Jeffrey ## [1,] &quot;went&quot; &quot;project&quot; ## [2,] &quot;came&quot; &quot;gutenbergtm&quot; ## [3,] &quot;house&quot; &quot;work&quot; ## [4,] &quot;back&quot; &quot;papers&quot; ## [5,] &quot;face&quot; &quot;may&quot; ## [6,] &quot;looked&quot; &quot;almost&quot; ## [7,] &quot;took&quot; &quot;works&quot; ## [8,] &quot;away&quot; &quot;electronic&quot; ## [9,] &quot;look&quot; &quot;jeffrey&quot; ## [10,] &quot;put&quot; &quot;aspern&quot; ## Tita Aunt Time - Woman - Venice ## [1,] &quot;&quot; &quot;might&quot; &quot;old&quot; ## [2,] &quot;miss&quot; &quot;come&quot; &quot;one&quot; ## [3,] &quot;&quot; &quot;little&quot; &quot;made&quot; ## [4,] &quot;tita&quot; &quot;way&quot; &quot;never&quot; ## [5,] &quot;said&quot; &quot;aunt&quot; &quot;time&quot; ## [6,] &quot;know&quot; &quot;even&quot; &quot;long&quot; ## [7,] &quot;dont&quot; &quot;take&quot; &quot;great&quot; ## [8,] &quot;think&quot; &quot;make&quot; &quot;still&quot; ## [9,] &quot;oh&quot; &quot;idea&quot; &quot;woman&quot; ## [10,] &quot;bordereau&quot; &quot;told&quot; &quot;upon&quot; #aspern.topicNames &lt;- apply(aspern.top10termsPerTopic, 2, paste, collapse=&quot; &quot;) #aspern.topicNames Main points of interpretation: The most common words are names of characters (Edna, Tita…) According to the sentiment analysis, Awakening novel is slightly from the beginning to the end. On the other hand, Aspern has a slight drop in the negative sentiment, but the end is positive. 3.Awakening is a longer text than Aspern 4.Due to lemmatization could not address the time of speaking (wheater it is told in past or future) Words “said”,little“,”think“,”house“,”good“,”like“,”one“,”come“,”back“,”thought\" are the most common. "],
["covid-19-dashboard-for-balkan-countries.html", "4 COVID 19 Dashboard for Balkan countries* 4.1 Data 4.2 Curve of confirmed cases 4.3 Deaths vs Recovered 4.4 Total cases vs new cases 4.5 Hospitalization", " 4 COVID 19 Dashboard for Balkan countries* 4.1 Data # Github covid19_confirmed_git &lt;-&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&quot; covid19_confirmed_git &lt;- read_csv(url(covid19_confirmed_git)) # Worldometer codivid19_all &lt;- &quot;https://www.worldometers.info/coronavirus/&quot; main_table &lt;- codivid19_all%&gt;% xml2::read_html()%&gt;% html_nodes(xpath=&#39;//*[@id=&quot;main_table_countries_today&quot;]&#39;) %&gt;% html_table() main_table &lt;- as.data.frame(main_table) ## Filtering data for Balkan countries (plus Italy and Austria) balkan &lt;- filter(main_table,Country.Other == &quot;Bosnia and Herzegovina&quot; | Country.Other == &quot;Italy&quot; | Country.Other == &quot;Croatia&quot; | Country.Other == &quot;Serbia&quot; | Country.Other == &quot;Montenegro&quot; | Country.Other == &quot;Slovenia&quot; | Country.Other == &quot;Austria&quot; | Country.Other == &quot;North Macedonia&quot; | Country.Other == &quot;Greece&quot;) balkan$TotalCases&lt;-gsub(&quot;,&quot;, &quot;&quot;, balkan$TotalCases) balkan$NewCases&lt;-gsub(&quot;,&quot;, &quot;&quot;, balkan$NewCases) balkan$TotalRecovered&lt;-gsub(&quot;,&quot;, &quot;&quot;, balkan$TotalRecovered) balkan$TotalDeaths&lt;-gsub(&quot;,&quot;, &quot;&quot;, balkan$TotalDeaths) balkan$TotalRecovered &lt;- as.numeric(balkan$TotalRecovered) balkan$TotalDeaths &lt;- as.numeric(balkan$TotalDeaths) balkan$TotalCases&lt;-as.numeric(balkan$TotalCases) balkan$NewCases &lt;- as.numeric(balkan$NewCases) 4.2 Curve of confirmed cases # Curve of confirmed cases---- columns &lt;- colnames(covid19_confirmed_git)[5:ncol(covid19_confirmed_git)] final &lt;-as.data.frame(pivot_longer(covid19_confirmed_git, cols = columns, names_to = &quot;Year&quot;, values_to = &quot;Confirmed&quot;)) final$Year &lt;- as.Date.character(final$Year,&quot;%m/%d/%y&quot;) colnames(final) &lt;- c(&quot;Province&quot;,&quot;Country&quot;,&quot;Lat&quot;,&quot;Long&quot;,&quot;Year&quot;,&quot;Confirmed&quot;) filter &lt;- filter(final, Country == &quot;Bosnia and Herzegovina&quot; | Country == &quot;Italy&quot; | Country == &quot;Croatia&quot; | Country == &quot;Serbia&quot; | Country == &quot;Slovenia&quot; | Country == &quot;Montenegro&quot; | Country == &quot;Austria&quot; | Country == &quot;North Macedonia&quot; | Country == &quot;Greece&quot;) p &lt;-ggplot(filter, aes(x = Year, y = Confirmed)) + geom_line(aes(color = Country), size = 1) + scale_color_brewer(palette=&quot;Set1&quot;)+ theme(legend.title = element_text(size = 6),legend.text = element_text(size = 6), # Remove panel background panel.background = element_blank(), # Add axis line axis.line = element_line(colour = &quot;grey&quot;))+ scale_y_log10(labels = comma)+ scale_x_date(date_labels = &quot;%b-%d&quot;, date_breaks = &quot;4 week&quot;)+ ylab(&quot;Confirmed cases&quot;)+ labs(caption=&quot;Data source: https://github.com/CSSEGISandData/COVID-19&quot;) ggplotly(p) 4.3 Deaths vs Recovered options(scipen = 9999) p &lt;- ggplot(balkan) + geom_segment( aes(x=Country.Other, xend=Country.Other, y=TotalRecovered, yend=TotalDeaths), color=&quot;grey&quot;) + geom_point( aes(x=Country.Other, y=TotalRecovered), color=rgb(0.2,0.7,0.1,0.5), size=3 ) + geom_point( aes(x=Country.Other, y=TotalDeaths), color=rgb(0.7,0.2,0.1,0.5), size=3 ) + coord_flip()+ scale_y_log10()+ theme_minimal() + theme( legend.position = &quot;none&quot;, panel.background = element_blank(), panel.grid = element_blank(), axis.line = element_line(colour = &quot;grey&quot;)) + xlab(&quot;&quot;) + ylab(&quot;Number of cases&quot;)+ ggtitle(label = &quot;Death vs Recovered&quot;) ggplotly(p) 4.4 Total cases vs new cases # Total cases vs new cases total_and_new &lt;- melt(balkan[,c(&quot;TotalCases&quot;,&quot;NewCases&quot;,&quot;Country.Other&quot;)]) p &lt;- ggplot(total_and_new, aes(x=Country.Other, y=value, fill=variable)) + geom_bar(stat=&#39;identity&#39;, position=&#39;dodge&#39;, color=&quot;black&quot; ,aes(text=paste(&quot;Country: &quot;,Country.Other, &quot;\\n&quot;, variable,&quot;:&quot;,value, sep=&quot;&quot;))) + scale_fill_brewer(palette = &quot;Paired&quot;)+ scale_y_continuous(labels=comma, trans = &quot;log10&quot;) + ylab(&quot;Number of cases&quot;)+ xlab(&quot;&quot;)+ ggtitle(label = &quot;Total cases vs New cases&quot;) + theme(panel.grid = element_blank(), panel.background = element_blank())+ labs(title = &quot;&quot;,fill=&quot;&quot; )+ coord_flip() ggplotly(p, tooltip = &quot;text&quot;) 4.5 Hospitalization 4.5.1 Croatia ## ## Abruzzo ## 196 ## Alabama ## 290 ## Alaska ## 267 ## Andalucia ## 225 ## Aragon ## 414 ## Arizona ## 367 ## Arkansas ## 350 ## Asturias ## 172 ## Austria ## 224 ## Baden-Wurttemberg ## 259 ## Balearic Islands ## 196 ## Basilicata ## 159 ## Basque County ## 240 ## Bavaria ## 300 ## Belgium ## 271 ## Berlin ## 251 ## Brandenburg ## 160 ## Bremen ## 175 ## Bulgaria ## 326 ## Calabria ## 195 ## California ## 339 ## Campania ## 202 ## Canary Islands ## 201 ## Cantabria ## 184 ## Castile and Leon ## 205 ## Castilla-La Mancha ## 193 ## Catalonia ## 262 ## Ceuta ## 209 ## Colorado ## 250 ## Community of Madrid ## 229 ## Connecticut ## 480 ## Croatia ## 264 ## Cyprus ## 268 ## Czechia ## 226 ## Delaware ## 202 ## Denmark ## 299 ## District of Columbia ## 230 ## Emilia-Romagna ## 287 ## Estonia ## 341 ## Extremadura ## 206 ## Finland ## 316 ## Florida ## 438 ## France ## 290 ## Friuli-Venezia Giulia ## 227 ## Galicia ## 227 ## Georgia ## 435 ## Germany ## 410 ## Greece ## 361 ## Hamburg ## 307 ## Hawaii ## 200 ## Hesse ## 368 ## Hungary ## 384 ## Idaho ## 201 ## Illinois ## 243 ## Indiana ## 257 ## Iowa ## 352 ## Ireland ## 272 ## Italy ## 421 ## Kansas ## 364 ## Kentucky ## 438 ## King and Snohomish Counties (excluding Life Care Center), WA ## 256 ## La Rioja ## 194 ## Latvia ## 261 ## Lazio ## 234 ## Life Care Center, Kirkland, WA ## 111 ## Liguria ## 243 ## Lithuania ## 315 ## Lombardia ## 299 ## Louisiana ## 244 ## Lower Saxony ## 293 ## Luxembourg ## 239 ## Maine ## 279 ## Malta ## 239 ## Marche ## 251 ## Maryland ## 316 ## Massachusetts ## 436 ## Mecklenburg-Vorpommern ## 177 ## Melilla ## 205 ## Michigan ## 241 ## Minnesota ## 354 ## Mississippi ## 284 ## Missouri ## 373 ## Molise ## 173 ## Montana ## 255 ## Murcia ## 147 ## Navarre ## 173 ## Nebraska ## 328 ## Netherlands ## 346 ## Nevada ## 293 ## New Hampshire ## 270 ## New Jersey ## 325 ## New Mexico ## 332 ## New York ## 302 ## North Carolina ## 260 ## North Dakota ## 397 ## North Rhine-Westphalia ## 263 ## Norway ## 395 ## Ohio ## 219 ## Oklahoma ## 366 ## Oregon ## 381 ## Other Counties, WA ## 321 ## Pennsylvania ## 287 ## Piemonte ## 255 ## Poland ## 385 ## Portugal ## 215 ## Provincia autonoma di Bolzano ## 244 ## Provincia autonoma di Trento ## 181 ## Puglia ## 340 ## Rhineland-Palatinate ## 276 ## Rhode Island ## 412 ## Romania ## 261 ## Saarland ## 314 ## Sardegna ## 195 ## Saxony ## 273 ## Saxony-Anhalt ## 198 ## Schleswig-Holstein ## 273 ## Sicilia ## 219 ## Slovakia ## 315 ## Slovenia ## 266 ## South Carolina ## 375 ## South Dakota ## 333 ## Spain ## 465 ## Sweden ## 472 ## Tennessee ## 305 ## Texas ## 399 ## Thuringia ## 334 ## Toscana ## 179 ## Umbria ## 153 ## United Kingdom ## 494 ## United States of America ## 632 ## Utah ## 358 ## Valencian Community ## 244 ## Valle d&#39;Aosta ## 192 ## Veneto ## 335 ## Vermont ## 229 ## Virginia ## 386 ## Washington ## 344 ## West Virginia ## 171 ## Wisconsin ## 275 ## Wyoming ## 249 4.5.2 Slovenia 4.5.3 Austria "]
]
