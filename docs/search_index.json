[
["twitter-analysis-for-co2mustgo-initiative.html", "2 Twitter Analysis for CO2mustGo initiative What is CO2mustGO? 2.1 Sankey diagram: Energy production capacity Twitter data analysis 2.2 Text cleaning and preprocessing 2.3 Tweets distribution 2.4 Word Frequency in Tweets 2.5 Word Network in Tweets Wordcloud 2.6 Sentiment analysis 2.7 Visualize the emotions 2.8 Top retweeted Tweets 2.9 Network of retweets", " 2 Twitter Analysis for CO2mustGo initiative What is CO2mustGO? Figure 2.1: Foto von Andrea Piacquadio von Pexels At the beginning of 2020 I heared that my working colleagues from another institute at the WU have launched an initiative called CO2mustGO initiative. The initiative aims to gather multinational group of students, researchers and teachers from the Vienna University of Economics and Business and other universities around Europe, to unite around the single issue of carbon price. This project started as a university course, with the vision of uniting students and scientists in an international movement supporting every serious carbon price initiative globally. Since my hometown, Tuzla, heavily suffers from the air pollution, I felt that I should give my contribution to this initiative and joined them in April 2020. I decided to support it with my R programming skills. Consequently, I ended up analyzing Twitter data, as it is one of the hot-spots for this topic. My role was to spark interest of stakeholders regarding this issue by using data available. Therefore, my tasks were related to using R to create understandable visualizations and to make use of Twitter data available. 2.1 Sankey diagram: Energy production capacity # Creation of source table$typ_11 &lt;- factor(table$typ_11,levels = c(&quot;c1&quot;,&quot;c2&quot;,&quot;g1&quot;,&quot;g2&quot;,&quot;g3&quot;,&quot;g4&quot;,&quot;n&quot;,&quot;o1&quot;,&quot;o2&quot;,&quot;o3&quot;,&quot;o4&quot;,&quot;r1&quot;,&quot;res&quot;),labels = c(&quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;,&quot;11&quot;,&quot;12&quot;)) source1 &lt;- as.vector.factor(table$typ_11) source1 &lt;- as.numeric(source1) #Country as mediator table2 &lt;- table table2$country&lt;-as.character(table2$country) table2$country[table$country==&quot;AT&quot;] &lt;- 14 table2$country[table$country==&quot;BE&quot;] &lt;- 15 table2$country[table$country==&quot;BG&quot;] &lt;- 16 table2$country[table$country==&quot;CH&quot;] &lt;- 17 table2$country[table$country==&quot;CY&quot;] &lt;- 18 table2$country[table$country==&quot;CZ&quot;] &lt;- 19 table2$country[table$country==&quot;DE&quot;] &lt;- 20 table2$country[table$country==&quot;DK&quot;] &lt;- 21 table2$country[table$country==&quot;EE&quot;] &lt;- 22 table2$country[table$country==&quot;ES&quot;] &lt;- 23 table2$country[table$country==&quot;FI&quot;] &lt;- 24 table2$country[table$country==&quot;FR&quot;] &lt;- 25 table2$country[table$country==&quot;GR&quot;] &lt;- 26 table2$country[table$country==&quot;HR&quot;] &lt;- 27 table2$country[table$country==&quot;HU&quot;] &lt;- 28 table2$country[table$country==&quot;IE&quot;] &lt;- 29 table2$country[table$country==&quot;IT&quot;] &lt;- 30 table2$country[table$country==&quot;LT&quot;] &lt;- 31 table2$country[table$country==&quot;LU&quot;] &lt;- 32 table2$country[table$country==&quot;LV&quot;] &lt;- 33 table2$country[table$country==&quot;NL&quot;] &lt;- 34 table2$country[table$country==&quot;NO&quot;] &lt;- 35 table2$country[table$country==&quot;PL&quot;] &lt;- 36 table2$country[table$country==&quot;PT&quot;] &lt;- 37 table2$country[table$country==&quot;RO&quot;] &lt;- 38 table2$country[table$country==&quot;SE&quot;] &lt;- 39 table2$country[table$country==&quot;SI&quot;] &lt;- 40 table2$country[table$country==&quot;SK&quot;] &lt;- 41 table2$country[table$country==&quot;UK&quot;] &lt;- 42 target1&lt;- table2$country #Creation of mediator(country),source and value value &lt;- table2 %&gt;% group_by(country,typ_11) %&gt;% count(mw_2018) #1st part of Target target1 &lt;- value$country target1&lt;-as.numeric(target1) #1st part of Value value1&lt;-value$mw_2018 #Creation of the final target value.df &lt;-table2 %&gt;% group_by(country)%&gt;% summarise(sum(mw_2018)) value.df$country&lt;-as.numeric(value.df$country) #2nd part of Source(from countries to total capacity) target1.2 &lt;-value.df$country #2nd part of Values (total of countries to the grand total) value1.2&lt;-value.df$`sum(mw_2018)` #Final target ft&lt;-rep(13,29) final_target &lt;- c(target1,ft) #final source final_soruce &lt;-c(source1,target1.2) #final value final_value &lt;-c(value1,value1.2) fig &lt;- plot_ly( type = &quot;sankey&quot;, orientation = &quot;h&quot;, valuesuffix = &quot;MW&quot;, arrangement = &quot;snap&quot;, node = list( label = c( &quot;Coal-lignite&quot;, # Node 0 &quot;Coal-hard&quot;, # Node 1 &quot;G1-gas&quot;, # Node 2 &quot;G2-gas&quot;, # Node 3 &quot;G3-gas&quot;, # Node 4 &quot;G4-gas&quot;, # Node 5 &quot;Nunclear&quot;, # Node 6 &quot;O1-oil&quot;, # Node 7 &quot;O2-oil&quot;, # Node 8 &quot;O3-oil&quot;, # Node 9 &quot;O4-oil&quot;, # Node 10 &quot;Renewables&quot;,# Node 11 &quot;Hydro&quot;, # Node 12 &quot;Total capacity&quot;, # Node 13 &quot;AT&quot;,#Node 14 &quot;BE&quot;,#Node 15 &quot;BG&quot;,#Node 16 &quot;CH&quot;,#Node 17 &quot;CY&quot;,#Node 18 &quot;CZ&quot;,#Node 19 &quot;DE&quot;,#Node 20 &quot;DK&quot;,#Node 21 &quot;EE&quot;,#Node 22 &quot;ES&quot;,#Node 23 &quot;FI&quot;,#Node 24 &quot;FR&quot;,#Node 25 &quot;GR&quot;,#Node 26 &quot;HR&quot;,#Node 27 &quot;HU&quot;,#Node 28 &quot;IE&quot;,#Node 29 &quot;IT&quot;,#Node 30 &quot;LT&quot;,#Node 31 &quot;LU&quot;,#Node 32 &quot;LV&quot;,#Node 33 &quot;NL&quot;,#Node 34 &quot;NO&quot;,#Node 35 &quot;PL&quot;,#Node 36 &quot;PT&quot;,#Node 37 &quot;RO&quot;,#Node 38 &quot;SE&quot;,#Node 39 &quot;SI&quot;,#Node 40 &quot;SK&quot;,#Node 41 &quot;UK&quot;)),#Node 42 link = list( source = final_soruce, target = final_target, value = final_value)) total.capacity &lt;- fig %&gt;% layout(title = &quot;Energy Production Capacity in the EU&quot;) total.capacity Figure 2.2: Energy production capacities in the EU In order to depict the nature of energy production facilities across EU countries, I created this sankey diagram. On the left side, you can see types of energy sources across the EU countries (data is not updated). In the middle, you can see respective countries. Finally, on the right you see the total of energy production. Especially nice feature of the visualization is the interactive component, so that, for instance, using “Box select” option, you can merge certain number of components of the same type (e.g., select 3 sources of energy) and the visualization changes accordingly. Twitter data analysis Twitter is nowadays among platforms that host strong communities. Carbon footprint and its repercussions belongs to prominent climate issue. Consequently, I decided to analyze Tweets downloaded via Twitter API on on 10.06.2020. The aim was simply to explore data about tweets with hashtags #carbonfootprint or #co2 and try to draw conclusions on how to approach social media presence on Twitter. 2.1.1 Data Our data was stored in CSV format.rtweet package provides nice function to read in data conveniently. #library(rtweet) # Read in Tweets data carbon_tweets &lt;- read_twitter_csv(&quot;data/#carbonfootprintOR#greenhouse-tweets.csv&quot;, unflatten = T) # Observe the data head(carbon_tweets) ## # A tibble: 6 x 90 ## user_id status_id created_at screen_name text source display_text_wi~ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 450721~ 12707070~ 2020-06-1~ Amrapali_c &quot;Goo~ Twitt~ 140 ## 2 295279~ 12707045~ 2020-06-1~ DanLittlec~ &quot;Fin~ Twitt~ 219 ## 3 118683~ 12707020~ 2020-06-1~ FutureOrga &quot;FRA~ Twitt~ 257 ## 4 118683~ 12699783~ 2020-06-0~ FutureOrga &quot;FRA~ Twitt~ 196 ## 5 118683~ 12703410~ 2020-06-0~ FutureOrga &quot;FRA~ Twitt~ 195 ## 6 338761~ 12707016~ 2020-06-1~ hanopcan &quot;Big~ Twitt~ 140 ## # ... with 83 more variables: reply_to_status_id &lt;chr&gt;, reply_to_user_id &lt;chr&gt;, ## # reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, is_retweet &lt;lgl&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, quote_count &lt;lgl&gt;, ## # reply_count &lt;lgl&gt;, hashtags &lt;list&gt;, symbols &lt;list&gt;, urls_url &lt;list&gt;, ## # urls_t.co &lt;list&gt;, urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;, ## # media_t.co &lt;list&gt;, media_expanded_url &lt;list&gt;, media_type &lt;list&gt;, ## # ext_media_url &lt;list&gt;, ext_media_t.co &lt;list&gt;, ext_media_expanded_url &lt;list&gt;, ## # ext_media_type &lt;lgl&gt;, mentions_user_id &lt;list&gt;, mentions_screen_name &lt;list&gt;, ## # lang &lt;chr&gt;, quoted_status_id &lt;chr&gt;, quoted_text &lt;chr&gt;, ## # quoted_created_at &lt;chr&gt;, quoted_source &lt;chr&gt;, quoted_favorite_count &lt;int&gt;, ## # quoted_retweet_count &lt;int&gt;, quoted_user_id &lt;chr&gt;, quoted_screen_name &lt;chr&gt;, ## # quoted_name &lt;chr&gt;, quoted_followers_count &lt;int&gt;, ## # quoted_friends_count &lt;int&gt;, quoted_statuses_count &lt;int&gt;, ## # quoted_location &lt;chr&gt;, quoted_description &lt;chr&gt;, quoted_verified &lt;lgl&gt;, ## # retweet_status_id &lt;chr&gt;, retweet_text &lt;chr&gt;, retweet_created_at &lt;chr&gt;, ## # retweet_source &lt;chr&gt;, retweet_favorite_count &lt;int&gt;, ## # retweet_retweet_count &lt;int&gt;, retweet_user_id &lt;chr&gt;, ## # retweet_screen_name &lt;chr&gt;, retweet_name &lt;chr&gt;, ## # retweet_followers_count &lt;int&gt;, retweet_friends_count &lt;int&gt;, ## # retweet_statuses_count &lt;int&gt;, retweet_location &lt;chr&gt;, ## # retweet_description &lt;chr&gt;, retweet_verified &lt;lgl&gt;, place_url &lt;chr&gt;, ## # place_name &lt;chr&gt;, place_full_name &lt;chr&gt;, place_type &lt;chr&gt;, country &lt;chr&gt;, ## # country_code &lt;chr&gt;, geo_coords &lt;list&gt;, coords_coords &lt;list&gt;, ## # bbox_coords &lt;list&gt;, status_url &lt;chr&gt;, name &lt;chr&gt;, location &lt;chr&gt;, ## # description &lt;chr&gt;, url &lt;chr&gt;, protected &lt;lgl&gt;, followers_count &lt;int&gt;, ## # friends_count &lt;int&gt;, listed_count &lt;int&gt;, statuses_count &lt;int&gt;, ## # favourites_count &lt;int&gt;, account_created_at &lt;chr&gt;, verified &lt;lgl&gt;, ## # profile_url &lt;chr&gt;, profile_expanded_url &lt;chr&gt;, account_lang &lt;lgl&gt;, ## # profile_banner_url &lt;chr&gt;, profile_background_url &lt;chr&gt;, ## # profile_image_url &lt;chr&gt; # Delete empty columns carbon_tweets &lt;- carbon_tweets[, colSums(is.na(carbon_tweets)) != nrow(carbon_tweets)] # Head of the data set dim(carbon_tweets) ## [1] 1858 86 Our data set contains 1858 Tweets and 86 features. 2.2 Text cleaning and preprocessing Tweets text in our data set requires some preprocessing and cleaning as it contains elements that are not helpful for our analysis. # Text cleaning carbon_tweets$stripped_text &lt;- gsub(&quot;https\\\\S*&quot;,&quot;&quot;, carbon_tweets$text) carbon_tweets$stripped_text &lt;- gsub(&quot;@\\\\S*&quot;,&quot;&quot;, carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;amp&quot;,&quot;&quot;,carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;[\\r\\n]&quot;,&quot;&quot;,carbon_tweets$stripped_text) carbon_tweets$stripped_text &lt;- gsub(&quot;[[:punct:]]&quot;, &quot;&quot;,carbon_tweets$stripped_text) # Text to lowercase, punctuation removed, frequency of the each word added carbon_tweets_clean &lt;- carbon_tweets %&gt;% dplyr::select(stripped_text) %&gt;% unnest_tokens(word, stripped_text) # Remove stop words data(&quot;stop_words&quot;) carbon_tweets_words &lt;- carbon_tweets_clean %&gt;% anti_join(stop_words) 2.3 Tweets distribution Since our data set contains information about the time Tweets are posted, I was interested to see the distribution of Tweets in the given period. # Distribution of tweets considered in the data. search_term &lt;- &#39;#carbonfootprint OR #co2&#39; by &lt;- &#39;hour&#39; p &lt;- ts_plot(carbon_tweets, by = by, trim = 2) + geom_point(col = &quot;#00acee&quot;) + theme_minimal() + labs(title = paste0(&quot;Tweets with &quot;,search_term,&quot; by &quot;,by),x = &#39;Date&#39;, y = &#39;Count&#39;) ggplotly(p) The period captured in our data set is from 15:00 on 2nd of June to 11:00 A.M. on 10th of June. Interestingly enough, there was certain occasion on 6th of June important for our topic. We see from the number of Tweets with hashtags #carbonfootprint or #co2, which stood at 43! This happening has to be more closely analyzed. 2.4 Word Frequency in Tweets Next, I wanted to inspect the word frequency in the Tweets from the data set. p &lt;- carbon_tweets_words %&gt;% dplyr::count(word, sort=T) %&gt;% top_n(10) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x = word, y = n)) + geom_col(fill=&quot;deepskyblue&quot;) + theme_minimal()+ xlab(NULL) + coord_flip() + labs(x = &quot;Count&quot;, y = &quot;Unique words&quot;, title = &quot;Count of unique words found in tweets&quot;) ggplotly(p) Generally, no major surprises regarding the most frequent words. It can be noted that term “worldenvironmentday” appears frequently as the World Environment Day is on 5th of June. 2.5 Word Network in Tweets Knowing that Tweets are short messages, I decided to inspect word network in order to possibly observe some unusual word combinations. The word network is made based on bi-grams. Basically, based on the number of times two words shows up together. # Remove punctuation, convert to lowercase, add id for each tweet! carbon_tweets_paired_words &lt;- carbon_tweets %&gt;% dplyr::select(stripped_text) %&gt;% unnest_tokens(paired_words, stripped_text, token = &quot;ngrams&quot;, n = 2) #library(tidyr) carbon_tweets_separated_words &lt;- carbon_tweets_paired_words %&gt;% separate(paired_words, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) carbon_tweets_filtered &lt;- carbon_tweets_separated_words %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) carbon_words_counts &lt;- carbon_tweets_filtered %&gt;% dplyr::count(word1, word2, sort = TRUE) #library(igraph) #library(ggraph) # Plot carbon change word network p&lt;- carbon_words_counts %&gt;% filter(n &gt;=30) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = 0.5, edge_width = n)) + geom_node_point(color = &quot;darkslategray4&quot;, size = 3) + geom_node_text(aes(label = name), vjust = 1.8, size = 3) + labs(title = &quot;Word Network: Tweets using the hashtag #carbonfootprint or #co2&quot;, subtitle = &quot;Text mining twitter data &quot;, x = &quot;&quot;, y = &quot;&quot;) The plot doesn’t load? Check it out here. An interesting observation is that the word network shows the word “justiceforvinayaki” appering together with “climatecrisis”. More specifically, “justiceforvinayaki” is actually a hashtag related to the story behind the pregnant elephant’s killing in Kerala’s Palakkad. More you can read here. Wordcloud In order to get a bigger picture of frequent terms in Tweets, I created a wordcloud. # Text preparation carbon_tweets$stripped_text &lt;- iconv(carbon_tweets$stripped_text, &#39;utf-8&#39;, &#39;ascii&#39;, sub=&#39;&#39;) review.docs &lt;- Corpus(VectorSource(carbon_tweets$stripped_text)) review.toSpace&lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x)) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;/&quot;) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;@&quot;) review.docs &lt;- tm_map(review.docs, review.toSpace, &quot;\\\\|&quot;) review.docs &lt;- tm_map(review.docs, content_transformer(tolower)) review.docs &lt;- tm_map(review.docs, removeNumbers) review.docs &lt;- tm_map(review.docs, removeWords, stopwords(&quot;english&quot;)) review.docs &lt;- tm_map(review.docs, content_transformer(tolower)) review.docs &lt;- tm_map(review.docs, removePunctuation) review.docs &lt;- tm_map(review.docs, stripWhitespace) review.tdm &lt;- TermDocumentMatrix(review.docs) review.m &lt;- as.matrix(review.tdm) review.v &lt;- sort(rowSums(review.m),decreasing=TRUE) review.d &lt;- data.frame(word = names(review.v),freq=review.v) set.seed(1234) wordcloud(words = review.d$word, freq = review.d$freq, max.words = 200, min.freq = 10, random.order=FALSE, rot.per=0.15, colors=brewer.pal(8, &quot;Dark2&quot;), scale=c(8,.3), vfont=c(&quot;sans serif&quot;,&quot;plain&quot;)) The plot doesn’t load? Check it out here. Minimum word frequency is set to 10, and the wordcloud depicts 200 most frequent terms. As you probably already know, Twitter is a place where discussions are going on frequently. Therefore, it made sense to try to reorganize the wordcloud to indicate positive vs negative terms. library(wordcloud) library(reshape2) par(mar = rep(0, 4)) set.seed(1234) carbon_tweets_words%&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment,sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;darkred&quot;,&quot;darkgreen&quot;), max.words = 400, min.freq= 10, scale = c(4.0,0.25)) The plot doesn’t load? Check it out here. 2.6 Sentiment analysis Knowing the nature of the carbon footprint related topics and based on the previous assumption about vivid discussions on Twitter, an analysis of emotions in Tweets would help us in opinion mining. # Sentiment analysis sentiment &lt;- carbon_tweets[,3:5] %&gt;% unnest_tokens(output = &#39;word&#39;, input = &#39;text&#39;) #Add sentiment dataset sentiment_dataset &lt;- get_sentiments(&quot;afinn&quot;) sentiment_dataset &lt;- arrange(sentiment_dataset, -value) #Merge sentiment &lt;- merge(sentiment, sentiment_dataset, by = &#39;word&#39;) #Clean sentiment$word &lt;- NULL sentiment$screen_name &lt;- NULL #Time sentiment$hour &lt;- format(base::round.POSIXt(sentiment$created_at, units=&quot;hours&quot;), format=&quot;%H:%M&quot;) #Pivot pivot &lt;- sentiment %&gt;% group_by(hour) %&gt;% summarise(sentiment = mean(value)) #Plot p &lt;- ggplot(pivot[-1,], aes(x = hour, y = sentiment)) + geom_line(group = 1, color=&quot;deepskyblue&quot;) + geom_point() + theme_minimal() + labs(title = paste0(&#39;Average sentiment of tweetings mentioning &quot;&#39;,search_term,&#39;&quot;&#39;),x = &#39;Date&#39;, y = &#39;Sentiment&#39;, caption = &#39;Source: Twitter API&#39;) ggplotly(p) The visualisation above depicts the average sentiment score of Tweets during a day. It seems that at 10:00 and 15:00 Tweets tend to be less positive than at 17:00 for instance. This information helps for scheduling Tweets so that they don’t get caught in the “bad moment”. 2.7 Visualize the emotions In order to decide what sort of discussion was going on the period of observation, we can visualise the count of words indicating emotions. # Get sentiments using the four different lexicons syuzhet &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;syuzhet&quot;) bing &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;bing&quot;) afinn &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;afinn&quot;) nrc &lt;- get_sentiment(carbon_tweets$stripped_text, method=&quot;nrc&quot;) sentiments &lt;- data.frame(syuzhet, bing, afinn, nrc) # get the emotions using the NRC dictionary nrc.sentiment &lt;- get_nrc_sentiment(carbon_tweets$stripped_text) emo_bar = colSums(nrc.sentiment) emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar)) emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)]) # Visualize the emotions from NRC sentiments plot_ly(emo_sum, x=~emotion, y=~count, type=&quot;bar&quot;, color=~emotion) %&gt;% layout(xaxis=list(title=&quot;&quot;), showlegend=FALSE, title=&quot;Distribution of emotion categories&quot;) It seems that positivity, trust, anticipation and joy are far more present in Tweets than emotions usually associated with something negative. Again, we can organize wordcloud so that it does not show just positive or negative words, but rather words associated with corresponding emotion. # Comparison word cloud all = c( paste(carbon_tweets$stripped_text[nrc.sentiment$anger &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$anticipation &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$disgust &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$fear &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$joy &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$sadness &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$surprise &gt; 0], collapse=&quot; &quot;), paste(carbon_tweets$stripped_text[nrc.sentiment$trust &gt; 0], collapse=&quot; &quot;) ) all &lt;- removeWords(all, stopwords(&quot;english&quot;)) # create corpus corpus = Corpus(VectorSource(all)) # # create term-document matrix tdm = TermDocumentMatrix(corpus) # # convert as matrix tdm = as.matrix(tdm) tdm1 &lt;- tdm[nchar(rownames(tdm)) &lt; 11,] # # add column names colnames(tdm) = c(&#39;anger&#39;, &#39;anticipation&#39;, &#39;disgust&#39;, &#39;fear&#39;, &#39;joy&#39;, &#39;sadness&#39;, &#39;surprise&#39;, &#39;trust&#39;) colnames(tdm1) &lt;- colnames(tdm) comparison.cloud(tdm1, random.order=FALSE, colors = c(&quot;#00B2FF&quot;, &quot;red&quot;, &quot;#FF0099&quot;, &quot;#6600CC&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;blue&quot;, &quot;brown&quot;), title.size=1, max.words=250, scale=c(2.5, 0.4),rot.per=0.4) ::: {.infobox .graph data-latex=“{download}”} The plot doesn’t load? Check it out here. ::: 2.8 Top retweeted Tweets Top retweets (with equal or more than 60 mentions) Next task was to identify Tweets that stand out. The graph below helped me to identify days on which certain Tweets were re-tweeted substaintially more than usually. # Select top retweeted tweets selected &lt;- which(carbon_tweets$retweet_count &gt;= 60) # Plot dates &lt;-as.POSIXct(strptime(carbon_tweets$created_at, format=&quot;%Y-%m-%d&quot;)) plot(x=dates, y=carbon_tweets$retweet_count, type=&quot;l&quot;, col=&quot;grey&quot;, xlab=&quot;Date&quot;, ylab=&quot;Times retweeted&quot;) colors &lt;- rainbow(10)[1:length(selected)] points(dates[selected], carbon_tweets$retweet_count[selected], pch=19, col=colors) ::: {.infobox .graph data-latex=“{download}”} The plot doesn’t load? Check it out here. ::: Interactive graph with retweets’ text In this interactive graph I manage to identify Tweets with their text that were re-tweeted frequently. If you hover over big sky-blue points you will see the actual text of each Tweet. # Plotly carbon_tweets$created_at &lt;-as.POSIXct(strptime(carbon_tweets$created_at, format=&quot;%Y-%m-%d&quot;)) p&lt;-ggplot(carbon_tweets, aes(x=created_at, y=retweet_count, col=retweet_count, size=retweet_count, retweet_text=retweet_text, created_at=created_at, retweet_name=retweet_name))+geom_point() +xlab(label=&quot;Date&quot;)+ylab(label=&quot;Retweet count&quot;)+ggtitle(label=&quot;Top retweeted tweets&quot;) ggplotly(p,tooltip = c(&quot;retweet_text&quot;,&quot;retweet_name&quot;)) 2.9 Network of retweets # Create data frame for the network rt_df &lt;- carbon_tweets[, c(&quot;screen_name&quot; , &quot;retweet_screen_name&quot; )] # Remove rows with missing values rt_df_new &lt;- rt_df[complete.cases(rt_df), ] # Convert to matrix matrx &lt;- as.matrix(rt_df_new) # Create the retweet network nw_rtweet &lt;- graph_from_edgelist(el = matrx, directed = TRUE) # View the retweet network print.igraph(nw_rtweet) ## IGRAPH 0ba62fb DN-- 1100 936 -- ## + attr: name (v/c) ## + edges from 0ba62fb (vertex names): ## [1] Amrapali_c -&gt;cathrinejahnsen hanopcan -&gt;GreenTech_SWest ## [3] MehmetO33440789-&gt;UKHaulier m_carmody -&gt;crowdfarmingco ## [5] amrendrakumar02-&gt;SUPERGASind abhishekk85 -&gt;theswitchfix ## [7] MarkCNorwich -&gt;52WeeksForEarth RPiUptime -&gt;TrafficlyApp ## [9] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [11] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [13] RPiUptime -&gt;TrafficlyApp RPiUptime -&gt;TrafficlyApp ## [15] RPiUptime -&gt;TrafficlyApp madebyhyphae -&gt;GreenTech_SWest ## + ... omitted several edges 2.9.1 Follower count of network users followers &lt;-carbon_tweets[, c(&quot;screen_name&quot; , &quot;followers_count&quot; )] # Remove rows with missing values followers &lt;- followers[complete.cases(rt_df), ] followers &lt;-unique(followers) # Categorize high and low follower count dim(followers) ## [1] 828 2 followers$follow &lt;- ifelse(followers$followers_count &gt; 500, &quot;1&quot;, &quot;0&quot;) # Assign external network attributes to retweet network V(nw_rtweet)$followers &lt;- followers$follow 2.9.2 Putting twitter data on the map (use plotly zoom in locations!) # Extract geolocation data and append new columns library(rtweet) library(sf) library(rnaturalearth) library(rnaturalearthdata) library(rgeos) pol_coord &lt;- lat_lng(carbon_tweets) pol_geo &lt;- na.omit(pol_coord[, c(&quot;lat&quot;, &quot;lng&quot;,&quot;location&quot;,&quot;retweet_count&quot;)]) world &lt;- ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) p&lt;-ggplot(data = world) + geom_sf() + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + geom_point(data= pol_geo,aes(x=lng, y=lat,loc=location,retweet_count=retweet_count),col = &quot;#00acee&quot;)+ theme(panel.grid.major = element_line(color = gray(.25), linetype =&quot;dashed&quot;, size = 0.15),panel.background = element_rect(fill = &quot;aliceblue&quot;))+ ggtitle(&quot;World map with tweets location and retweet count&quot;, subtitle = paste0(&quot;(&quot;, length(unique(pol_geo$location)), &quot; countries)&quot;)) ggplotly(p,tooltip = c(&quot;location&quot;,&quot;retweet_count&quot;)) 2.9.3 Users who retweet the most # Calculate the out-degree scores out_degree &lt;- degree(nw_rtweet, mode = c(&quot;out&quot;)) # Sort the users in descending order of out-degree scores out_degree_sort &lt;- sort(out_degree, decreasing = TRUE) head(out_degree_sort,10) ## imagine_garden RPiUptime taxa_monocot Eco1stArt greenhousedave ## 9 8 7 5 5 ## pepparsteve LazarovMartin7 PetrovichBilly greentechdon researchmrx ## 4 4 4 4 3 # INTERPRETATION: Users who retweeted the most. #Hubs: Tweeter accounts with a lot of outgoing edges. hs &lt;- hub_score(nw_rtweet, weights=NA)$vector sort(hs, decreasing = TRUE)[1:20] ## SaachinPatel virendrathor007 ModheraAjay YuvraajsO mahisawOfficial ## 1 1 1 1 1 ## MayankS08111059 Maveric94280289 ujjain_live PayaswiniShett1 MaheshK70846514 ## 1 1 1 1 1 ## er_gaurav_singh mohanbhadri rushessensedood ShivanshikaF GMahindroo ## 1 1 1 1 1 ## avng47 berojgaradami SandeepKumawat_ _AdityaRaje kirtischandel ## 1 1 1 1 1 # #Ex-kurs: An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices. # They are likely to retweet. 2.9.4 Users who are the most retweeted # Calculate the in-degree scores in_degree &lt;- degree(nw_rtweet, mode = c(&quot;in&quot;)) # Sort the users in descending order of in-degree scores in_degree_sort &lt;- sort(in_degree, decreasing = TRUE) head(in_degree_sort,10) ## ReSanskrit PeacockSolar rockerblonde StellaYeahilike WaterlooEnergy ## 81 41 25 19 19 ## gulf_intel DanAlluf BPCLimited ByronTweetsData TrafficlyApp ## 17 17 15 14 13 # INTERPRETATION: Users whose posts were retweeted most. #Authorities: Tweeter accounts with a lot of incoming edges. as &lt;- authority_score(nw_rtweet, weights=NA)$vector sort(as, decreasing = TRUE)[1:20] ## ReSanskrit TrafficlyApp PeacockSolar OrchidOfTheDay rockerblonde ## 1.000000e+00 1.677825e-14 3.459879e-15 1.982355e-15 1.238491e-15 ## WaterlooEnergy DanAlluf BPCLimited ByronTweetsData IntlPeaceBureau ## 8.468956e-16 8.319809e-16 8.051224e-16 7.793320e-16 7.423804e-16 ## starindia L_FudgerGalvez ESA_EO gulf_intel StellaYeahilike ## 7.423804e-16 6.518274e-16 5.949006e-16 5.739893e-16 5.672778e-16 ## crowdfarmingco katyalston gikiearth CrownPlatform AyiccZim ## 5.267684e-16 5.267684e-16 5.197328e-16 4.809261e-16 4.755414e-16 #Ex-kurs: An edge (a set of two elements) is drawn as a line connecting two vertices, called endpoints or end vertices or end vertices. # They are likely to be retweeted. 2.9.5 Users with important role in allowing information to pass through network Users with higher betweenness has more control over the network. # Calculate the betweenness scores of the network betwn_nw &lt;- betweenness(nw_rtweet, directed = TRUE) # Sort the users in descending order of betweenness scores betwn_nw_sort &lt;- betwn_nw %&gt;% sort(decreasing = TRUE) %&gt;% round() %&gt;% head(10) betwn_nw_sort ## gikiearth Privatecarfree ChinyeRumby DioxideMat LifexSoles ## 6 6 4 2 1 ## esuohneerg swift_iron NGO_NISWARTH BPCLBareilly Va3tsal ## 1 1 1 1 1 2.9.6 Clustering largest_cliques(nw_rtweet) #list only 20 vertices in that cluster ## [[1]] ## + 3/1100 vertices, named, from 0ba62fb: ## [1] BpclStateLPGUP BPCLimited BPCLBareilly ## ## [[2]] ## + 3/1100 vertices, named, from 0ba62fb: ## [1] iona_nyandoro LifexSoles LxS_Build ## ## [[3]] ## + 3/1100 vertices, named, from 0ba62fb: ## [1] MKY110987 BPCLimited BPCLBareilly ## ## [[4]] ## + 3/1100 vertices, named, from 0ba62fb: ## [1] DarlacGardening WaterWorxUK swift_iron 2.9.7 Community detection #https://rpubs.com/cosmopolitanvan/twitternetworks #Community detection based on edge betweenness (Newman-Girvan) comm &lt;- cluster_edge_betweenness(nw_rtweet) sort(sizes(comm), decreasing = T)[1:20] ## Community sizes ## 60 44 16 74 98 31 108 233 205 58 42 114 91 110 21 57 4 37 48 99 ## 82 42 31 25 22 20 20 18 15 14 13 13 12 12 11 11 10 10 10 10 comm_1 &lt;- communities(comm) # Tweet accounts in the Community 60 (the biggest community) comm_1$`60` ## [1] &quot;SaachinPatel&quot; &quot;ReSanskrit&quot; &quot;virendrathor007&quot; &quot;ModheraAjay&quot; ## [5] &quot;YuvraajsO&quot; &quot;mahisawOfficial&quot; &quot;MayankS08111059&quot; &quot;Maveric94280289&quot; ## [9] &quot;ujjain_live&quot; &quot;PayaswiniShett1&quot; &quot;MaheshK70846514&quot; &quot;er_gaurav_singh&quot; ## [13] &quot;mohanbhadri&quot; &quot;rushessensedood&quot; &quot;ShivanshikaF&quot; &quot;GMahindroo&quot; ## [17] &quot;avng47&quot; &quot;berojgaradami&quot; &quot;SandeepKumawat_&quot; &quot;_AdityaRaje&quot; ## [21] &quot;kirtischandel&quot; &quot;Sai_3196&quot; &quot;Abhi_Gosavi17&quot; &quot;g_one01&quot; ## [25] &quot;amritanshu20712&quot; &quot;Shrirang_4u&quot; &quot;ChandanSahaDas1&quot; &quot;HGhumnar&quot; ## [29] &quot;im_zala&quot; &quot;the_bhaveshp&quot; &quot;SolankiDhavalJ&quot; &quot;AshokDh10070811&quot; ## [33] &quot;VGilankar&quot; &quot;free_sridhar&quot; &quot;looookeeee&quot; &quot;_dactar_babu&quot; ## [37] &quot;QnalH&quot; &quot;Bhuhan_Raut12&quot; &quot;thakurmanish25&quot; &quot;Dvipalgoswami&quot; ## [41] &quot;SSSPrashantS&quot; &quot;pahariparul&quot; &quot;shwetarathi0301&quot; &quot;Pournimakothap1&quot; ## [45] &quot;SKMaisuriya&quot; &quot;gandhecha_deep&quot; &quot;ravi_patel21&quot; &quot;SarveshdKumar&quot; ## [49] &quot;jenilsaurabh&quot; &quot;Byomkesh5&quot; &quot;vishaltechrexx&quot; &quot;Shubham82408211&quot; ## [53] &quot;Damnshashi&quot; &quot;ms_ayushi&quot; &quot;SankalpKr&quot; &quot;rchandra12&quot; ## [57] &quot;Vishalcr7999&quot; &quot;ub1112&quot; &quot;VishalRajShodhi&quot; &quot;prats_ag&quot; ## [61] &quot;Priyank64671386&quot; &quot;RudreshScharma&quot; &quot;naagaputra&quot; &quot;ipradyu&quot; ## [65] &quot;Prakash13119341&quot; &quot;anitajha2802&quot; &quot;Ankit_yadaavv&quot; &quot;sanatan_&quot; ## [69] &quot;Raj_Sawant96&quot; &quot;sohil_oza&quot; &quot;NoAbsurdity&quot; &quot;PujariRaksha&quot; ## [73] &quot;ddhamaal_bai&quot; &quot;AyushmanDubey18&quot; &quot;NSK_Kochhar&quot; &quot;HadaniSuresh&quot; ## [77] &quot;binayamishra16&quot; &quot;realMeetu&quot; &quot;Sanmon96952110&quot; &quot;ItsRajatRai&quot; ## [81] &quot;ramkotipalli&quot; &quot;Omtripathi95&quot; # Tweet accounts in the Community 44 (the second biggest community) comm_1$`44` ## [1] &quot;TushitGarg&quot; &quot;PeacockSolar&quot; &quot;ianikk18&quot; &quot;rupeshwar_rao&quot; ## [5] &quot;imjyotidwivedi&quot; &quot;NandiniRajendr4&quot; &quot;5a13fec9874b479&quot; &quot;Moanish58105963&quot; ## [9] &quot;sonukau18053840&quot; &quot;snipervenom21&quot; &quot;RobinSingh1825&quot; &quot;Shalmalichakra3&quot; ## [13] &quot;RubalSh04483180&quot; &quot;arun_thevan03&quot; &quot;AshishK54595782&quot; &quot;BasantKKothari&quot; ## [17] &quot;SriAtul2&quot; &quot;roshan_munda&quot; &quot;KillerSrinivas2&quot; &quot;RitanshuChugh&quot; ## [21] &quot;KunalSh62765789&quot; &quot;Rampras17015630&quot; &quot;manvpandit&quot; &quot;27shikhar&quot; ## [25] &quot;anshulajin&quot; &quot;KumariMitya&quot; &quot;EmescoFdn&quot; &quot;AtishKu82064443&quot; ## [29] &quot;ReetuShukla12&quot; &quot;VIJAYAS36039459&quot; &quot;Shashwa38747031&quot; &quot;ManishV59713691&quot; ## [33] &quot;Sanjeev16620270&quot; &quot;Aashish04884619&quot; &quot;ArchitG36451766&quot; &quot;Vikrant64760265&quot; ## [37] &quot;bunny3298&quot; &quot;Joydeep85178197&quot; &quot;KishoreH14&quot; &quot;HarshTi80357066&quot; ## [41] &quot;Sivas_07&quot; &quot;AbhijitOjha7&quot; # Tweet accounts in the Community 16 (the second biggest community) comm_1$`16` ## [1] &quot;gardener_the&quot; &quot;HoratiosGarden&quot; &quot;BrownBurden&quot; &quot;VivienLloyd&quot; ## [5] &quot;LazarovMartin7&quot; &quot;FreeDealSteals&quot; &quot;theOGryankirk&quot; &quot;StellaYeahilike&quot; ## [9] &quot;vhhydroponics&quot; &quot;agritecture&quot; &quot;PlantGetEnough1&quot; &quot;GladerPhilip&quot; ## [13] &quot;Woroud&quot; &quot;tepanchinceva&quot; &quot;Smart_Reads&quot; &quot;birdwriter7&quot; ## [17] &quot;hiro_thoyou3864&quot; &quot;Watrasri&quot; &quot;AgtechOtori&quot; &quot;sthanleyc&quot; ## [21] &quot;richardelio_nyc&quot; &quot;Yeahilike&quot; &quot;azkamel1&quot; &quot;edohpa&quot; ## [25] &quot;romi_hime_black&quot; &quot;banan_thompson&quot; &quot;StellaSanLF&quot; &quot;PeLopez1&quot; ## [29] &quot;RTusuzuro_&quot; &quot;umada_ushijiro&quot; &quot;Sun_Flower119&quot; "]
]
